Release "mirror-1" has been upgraded. Happy Helming!
NAME: mirror-1
LAST DEPLOYED: Thu Nov 17 03:02:42 2022
NAMESPACE: default
STATUS: deployed
REVISION: 5
USER-SUPPLIED VALUES:
{}

COMPUTED VALUES:
affinity: {}
autoscaling:
  enabled: false
  maxReplicas: 100
  minReplicas: 1
  targetCPUUtilizationPercentage: 80
fullnameOverride: ""
hedera-explorer:
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - podAffinityTerm:
          labelSelector:
            matchLabels:
              app.kubernetes.io/component: hedera-explorer
          topologyKey: kubernetes.io/hostname
        weight: 100
  annotations: {}
  autoscaling:
    behavior: {}
    enabled: false
    maxReplicas: 3
    metrics:
    - resource:
        name: cpu
        target:
          averageUtilization: 80
          type: Utilization
      type: Resource
    minReplicas: 1
  env:
    VUE_APP_LOCAL_MIRROR_NODE_URL: https://mainnet-public.mirrornode.hedera.com
  envFrom: []
  fullnameOverride: ""
  global:
    namespaceOverride: ""
  image:
    pullPolicy: Always
    pullSecrets: []
    registry: docker.io
    repository: cabob/hedera-explorer
    tag: latest
  ingress:
    annotations: {}
    className: ""
    enabled: false
    hosts:
    - host: chart-example.local
      paths:
      - path: /
        pathType: ImplementationSpecific
    tls: []
  labels: {}
  livenessProbe:
    httpGet:
      path: /
      port: http
    initialDelaySeconds: 25
    timeoutSeconds: 2
  nodeSelector: {}
  podAnnotations: {}
  podDisruptionBudget:
    enabled: false
    minAvailable: 50%
  podSecurityContext:
    fsGroup: 1000
    runAsGroup: 1000
    runAsNonRoot: true
    runAsUser: 1000
    seccompProfile:
      type: RuntimeDefault
  priorityClassName: ""
  readinessProbe:
    httpGet:
      path: /
      port: http
    initialDelaySeconds: 30
    timeoutSeconds: 2
  replicas: 1
  resources:
    limits:
      cpu: 500m
      memory: 250Mi
    requests:
      cpu: 200m
      memory: 100Mi
  revisionHistoryLimit: 3
  securityContext:
    allowPrivilegeEscalation: false
    capabilities:
      drop:
      - ALL
    readOnlyRootFilesystem: true
  service:
    annotations: {}
    port: 80
    type: LoadBalancer
  serviceAccount:
    create: true
  terminationGracePeriodSeconds: 30
  tolerations: []
  updateStrategy:
    rollingUpdate:
      maxSurge: 10%
      maxUnavailable: 25%
    type: RollingUpdate
  volumeMounts:
    cache:
      mountPath: /var/cache/nginx
  volumes:
    cache:
      emptyDir: {}
hedera-json-rpc-relay:
  affinity: {}
  autoscaling:
    enabled: false
    maxReplicas: 100
    minReplicas: 1
    targetCPUUtilizationPercentage: 80
  config:
    CHAIN_ID: "0x12a"
    DEFAULT_RATE_LIMIT: 200
    DEV_MODE: "FALSE"
    E2E_RELAY_HOST: http://127.0.0.1:7546
    ETH_GET_LOGS_BLOCK_RANGE_LIMIT: 2000
    FEE_HISTORY_MAX_RESULTS: 10
    HBAR_RATE_LIMIT_DURATION: 60000
    HBAR_RATE_LIMIT_TINYBAR: 8000000000
    LIMIT_DURATION: 60000
    LOCAL_NODE: true
    LOG_LEVEL: ""
    MIRROR_NODE_URL: http://mirror-node-rest:5551
    OPERATOR_ID_ETH_SENDRAWTRANSACTION: ""
    OPERATOR_ID_MAIN: 0.0.2
    OPERATOR_KEY_ETH_SENDRAWTRANSACTION: ""
    OPERATOR_KEY_MAIN: 302e020100300506032b65700422042091132178e72057a1d7528025956fe39b0b847f200ab59b2fdd367017f3087137
    RATE_LIMIT_DISABLED: true
    SERVER_PORT: 7546
    TIER_1_RATE_LIMIT: 100
    TIER_2_RATE_LIMIT: 800
    TIER_3_RATE_LIMIT: 1600
    hosted:
      HEDERA_NETWORK: testnet
    local:
      HEDERA_NETWORK: '{ "network-node:50211": "0.0.3" }'
  fullnameOverride: ""
  global: {}
  image:
    pullPolicy: Always
    repository: cabob/hedera-json-rpc-relay
    tag: latest
  imagePullSecrets: []
  ingress:
    annotations: {}
    className: ""
    enabled: false
    hosts:
    - host: chart-example.local
      paths:
      - path: /
        pathType: ImplementationSpecific
    tls: []
  nameOverride: ""
  nodeSelector: {}
  podAnnotations: {}
  podSecurityContext: {}
  ports:
    containerPort: 7546
    name: jsonrpcrelay
  replicaCount: 2
  resources: {}
  rolling_restart:
    enabled: false
    schedule: '@daily'
  securityContext: {}
  service:
    annotations: {}
    annotations_JSON: {}
    port: 7546
    type: LoadBalancer
  serviceAccount:
    annotations: {}
    create: true
    name: ""
  tolerations: []
hedera-mirror:
  alertmanager:
    inhibitRules:
      InhibitAllWhenPlatformNotActive:
        enabled: true
        matches:
        - equal:
          - namespace
          sourceMatch:
          - name: alertname
            value: MonitorPublishPlatformNotActive
          targetMatch:
          - name: application
            regex: true
            value: .*
      InhibitGrpcAndMonitorHighLatencyWhenImporterHighLatencyOrNoPods:
        enabled: true
        matches:
        - equal:
          - namespace
          sourceMatch:
          - name: alertname
            value: ImporterNoPodsReady
          targetMatch:
          - name: alertname
            regex: true
            value: (GrpcHighLatency|MonitorSubscribeLatency)
      InhibitGrpcAndMonitorHighLatencyWhenImporterRecordFileIssues:
        enabled: true
        matches:
        - equal:
          - namespace
          sourceMatch:
          - name: application
            value: hedera-mirror-importer
          - name: type
            value: RECORD
          targetMatch:
          - name: alertname
            regex: true
            value: (GrpcHighLatency|MonitorSubscribeLatency)
      InhibitMonitorNoTransactionsWhenImporterNoTransactionsOrNoPods:
        enabled: true
        matches:
        - equal:
          - namespace
          sourceMatch:
          - name: alertname
            regex: true
            value: (ImporterNoTransactions|ImporterNoPodsReady)
          targetMatch:
          - name: alertname
            value: MonitorSubscribeStopped
      enabled: false
  applicationResource:
    enabled: false
    partnerId: ""
    partnerName: ""
    solutionId: ""
  db:
    host: '{{ .Release.Name }}'
    name: mirror_node
    owner:
      password: ""
      username: mirror_node
    schema: public
  global:
    hostname: ""
    image: {}
    namespaceOverride: ""
    podAnnotations: {}
    useReleaseForNameLabel: false
  grpc:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/component: grpc
            topologyKey: kubernetes.io/hostname
          weight: 100
    alertmanager:
      inhibitRules:
        InhibitAllWhenPodIssues:
          enabled: true
          matches:
          - equal:
            - namespace
            - pod
            sourceMatch:
            - name: area
              value: resource
            targetMatch:
            - name: application
              value: hedera-mirror-grpc
        InhibitGrpcLogAlertWhenHighErrors:
          enabled: true
          matches:
          - equal:
            - namespace
            - pod
            sourceMatch:
            - name: alertname
              value: GrpcErrors
            targetMatch:
            - name: alertname
              value: GrpcLogErrors
        enabled: false
    annotations: {}
    config: {}
    db:
      password: ""
      username: mirror_grpc
    enabled: true
    env:
      HEDERA_MIRROR_GRPC_DB_HOST:
        valueFrom:
          secretKeyRef:
            key: HEDERA_MIRROR_IMPORTER_DB_HOST
            name: mirror-passwords
      HEDERA_MIRROR_GRPC_DB_NAME:
        valueFrom:
          secretKeyRef:
            key: HEDERA_MIRROR_IMPORTER_DB_NAME
            name: mirror-passwords
      HEDERA_MIRROR_GRPC_DB_PASSWORD:
        valueFrom:
          secretKeyRef:
            key: HEDERA_MIRROR_GRPC_DB_PASSWORD
            name: mirror-passwords
      HEDERA_MIRROR_GRPC_DB_USERNAME:
        valueFrom:
          secretKeyRef:
            key: HEDERA_MIRROR_GRPC_DB_USERNAME
            name: mirror-passwords
      SPRING_CLOUD_KUBERNETES_ENABLED: "true"
      SPRING_CONFIG_ADDITIONAL_LOCATION: file:/usr/etc/hedera/
      SPRING_REDIS_HOST:
        valueFrom:
          secretKeyRef:
            key: SPRING_REDIS_HOST
            name: mirror-redis
      SPRING_REDIS_PASSWORD:
        valueFrom:
          secretKeyRef:
            key: SPRING_REDIS_PASSWORD
            name: mirror-redis
    envFrom: []
    fullnameOverride: ""
    global:
      hostname: ""
      image: {}
      middleware: false
      namespaceOverride: ""
      podAnnotations: {}
      useReleaseForNameLabel: false
    hpa:
      behavior: {}
      enabled: false
      maxReplicas: 3
      metrics:
      - resource:
          name: cpu
          target:
            averageUtilization: 80
            type: Utilization
        type: Resource
      minReplicas: 1
    image:
      pullPolicy: IfNotPresent
      registry: gcr.io
      repository: mirrornode/hedera-mirror-grpc
      tag: ""
    imagePullSecrets: []
    ingress:
      annotations:
        traefik.ingress.kubernetes.io/router.middlewares: '{{ include "hedera-mirror-grpc.namespace"
          . }}-{{ include "hedera-mirror-grpc.fullname" . }}@kubernetescrd'
      enabled: true
      hosts:
      - host: ""
        paths:
        - /com.hedera.mirror.api.proto.ConsensusService
        - /com.hedera.mirror.api.proto.NetworkService
        - /grpc.reflection.v1alpha.ServerReflection
      tls:
        enabled: false
        secretName: ""
    labels: {}
    livenessProbe:
      httpGet:
        path: /actuator/health/liveness
        port: http
      initialDelaySeconds: 50
      periodSeconds: 10
      timeoutSeconds: 2
    middleware:
    - circuitBreaker:
        expression: NetworkErrorRatio() > 0.10 || ResponseCodeRatio(500, 600, 0, 600)
          > 0.25
    - rateLimit:
        average: 5
        sourceCriterion:
          requestHost: true
    - retry:
        attempts: 3
        initialInterval: 250ms
    nodeSelector: {}
    podAnnotations: {}
    podDisruptionBudget:
      enabled: false
      minAvailable: 50%
    podSecurityContext:
      fsGroup: 1000
      runAsGroup: 1000
      runAsNonRoot: true
      runAsUser: 1000
      seccompProfile:
        type: RuntimeDefault
    priorityClassName: ""
    prometheusRules:
      GrpcErrors:
        annotations:
          description: '{{ $value | humanizePercentage }} gRPC {{ $labels.statusCode
            }} error rate for {{ $labels.namespace }}/{{ $labels.pod }}'
          summary: Mirror gRPC API error rate exceeds 5%
        enabled: true
        expr: sum(rate(grpc_server_processing_duration_seconds_count{application="hedera-mirror-grpc",
          statusCode!~"CANCELLED|DEADLINE_EXCEEDED|INVALID_ARGUMENT|NOT_FOUND|OK"}[5m]))
          by (namespace, pod, statusCode) / sum(rate(grpc_server_processing_duration_seconds_count{application="hedera-mirror-grpc"}[5m]))
          by (namespace, pod, statusCode) > 0.05
        for: 2m
        labels:
          application: hedera-mirror-grpc
          severity: critical
      GrpcHighCPU:
        annotations:
          description: '{{ $labels.namespace }}/{{ $labels.pod }} CPU usage reached
            {{ $value | humanizePercentage }}'
          summary: Mirror gRPC API CPU usage exceeds 80%
        enabled: true
        expr: sum(process_cpu_usage{application="hedera-mirror-grpc"}) by (namespace,
          pod) / sum(system_cpu_count{application="hedera-mirror-grpc"}) by (namespace,
          pod) > 0.8
        for: 5m
        labels:
          application: hedera-mirror-grpc
          area: resource
          severity: critical
      GrpcHighDBConnections:
        annotations:
          description: '{{ $labels.namespace }}/{{ $labels.pod }} is using {{ $value
            | humanizePercentage }} of available database connections'
          summary: Mirror gRPC API database connection utilization exceeds 75%
        enabled: true
        expr: sum(hikaricp_connections_active{application="hedera-mirror-grpc"}) by
          (namespace, pod) / sum(hikaricp_connections_max{application="hedera-mirror-grpc"})
          by (namespace, pod) > 0.75
        for: 5m
        labels:
          application: hedera-mirror-grpc
          area: resource
          severity: critical
      GrpcHighFileDescriptors:
        annotations:
          description: '{{ $labels.namespace }}/{{ $labels.pod }} file descriptor
            usage reached {{ $value | humanizePercentage }}'
          summary: Mirror gRPC API file descriptor usage exceeds 80%
        enabled: true
        expr: sum(process_files_open_files{application="hedera-mirror-grpc"}) by (namespace,
          pod) / sum(process_files_max_files{application="hedera-mirror-grpc"}) by
          (namespace, pod) > 0.8
        for: 5m
        labels:
          application: hedera-mirror-grpc
          area: resource
          severity: critical
      GrpcHighLatency:
        annotations:
          description: High latency of {{ $value | humanizeDuration }} between the
            main nodes and {{ $labels.namespace }}/{{ $labels.pod }}
          summary: Mirror gRPC API consensus to delivery (C2MD) latency exceeds 15s
        enabled: true
        expr: sum(rate(hedera_mirror_publish_latency_seconds_sum{application="hedera-mirror-grpc"}[5m]))
          by (namespace, pod) / sum(rate(hedera_mirror_publish_latency_seconds_count{application="hedera-mirror-grpc"}[5m]))
          by (namespace, pod) > 15
        for: 1m
        labels:
          application: hedera-mirror-grpc
          severity: critical
      GrpcHighMemory:
        annotations:
          description: '{{ $labels.namespace }}/{{ $labels.pod }} memory usage reached
            {{ $value | humanizePercentage }}'
          summary: Mirror gRPC API memory usage exceeds 80%
        enabled: true
        expr: sum(jvm_memory_used_bytes{application="hedera-mirror-grpc"}) by (namespace,
          pod) / sum(jvm_memory_max_bytes{application="hedera-mirror-grpc"}) by (namespace,
          pod) > 0.8
        for: 5m
        labels:
          application: hedera-mirror-grpc
          area: resource
          severity: critical
      GrpcLogErrors:
        annotations:
          description: Logs for {{ $labels.namespace }}/{{ $labels.pod }} have reached
            {{ $value }} error messages/s in a 3m period
          summary: High rate of log errors
        enabled: true
        expr: sum(increase(log4j2_events_total{application="hedera-mirror-grpc", level="error"}[1m]))
          by (namespace, pod) >= 2
        for: 3m
        labels:
          application: hedera-mirror-grpc
          severity: critical
      GrpcNoPodsReady:
        annotations:
          description: No gRPC API instances are currently running in {{ $labels.namespace
            }}
          summary: No gRPC API instances running
        enabled: true
        expr: sum(kube_pod_status_ready{pod=~".*-grpc-.*",condition="true"}) by (namespace)
          < 1
        for: 2m
        labels:
          application: hedera-mirror-grpc
          area: resource
          severity: critical
      GrpcNoSubscribers:
        annotations:
          description: '{{ $labels.namespace }} has {{ $value }} subscribers for {{
            $labels.type }}'
          summary: Mirror gRPC API has no subscribers
        enabled: true
        expr: sum(hedera_mirror_subscribers{application="hedera-mirror-grpc"}) by
          (namespace, type) <= 0
        for: 5m
        labels:
          application: hedera-mirror-grpc
          severity: critical
      GrpcQueryLatency:
        annotations:
          description: High average database query latency of {{ $value | humanizeDuration
            }} for {{ $labels.namespace }}/{{ $labels.pod }}
          summary: Mirror gRPC API query latency exceeds 1s
        enabled: true
        expr: sum(rate(spring_data_repository_invocations_seconds_sum{application="hedera-mirror-grpc"}[5m]))
          by (namespace, pod) / sum(rate(spring_data_repository_invocations_seconds_count{application="hedera-mirror-grpc"}[5m]))
          by (namespace, pod) > 1
        for: 1m
        labels:
          application: hedera-mirror-grpc
          severity: warning
      enabled: false
    rbac:
      enabled: true
    readinessProbe:
      httpGet:
        path: /actuator/health/readiness
        port: http
      initialDelaySeconds: 40
      timeoutSeconds: 2
    resources:
      limits:
        cpu: 2
        memory: 2048Mi
      requests:
        cpu: 100m
        memory: 128Mi
    revisionHistoryLimit: 3
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true
    service:
      annotations:
        traefik.ingress.kubernetes.io/service.serversscheme: h2c
      port: 5600
      type: ClusterIP
    serviceAccount:
      create: true
    serviceMonitor:
      enabled: false
      interval: 30s
    terminationGracePeriodSeconds: 60
    tolerations: []
    updateStrategy:
      rollingUpdate:
        maxSurge: 10%
        maxUnavailable: 25%
      type: RollingUpdate
    volumeMounts:
      config:
        mountPath: /usr/etc/hedera
    volumes:
      config:
        secret:
          defaultMode: 420
          secretName: '{{ include "hedera-mirror-grpc.fullname" . }}'
  hedera-mirror-grpc:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/component: grpc
            topologyKey: kubernetes.io/hostname
          weight: 100
    alertmanager:
      inhibitRules:
        InhibitAllWhenPodIssues:
          enabled: true
          matches:
          - equal:
            - namespace
            - pod
            sourceMatch:
            - name: area
              value: resource
            targetMatch:
            - name: application
              value: hedera-mirror-grpc
        InhibitGrpcLogAlertWhenHighErrors:
          enabled: true
          matches:
          - equal:
            - namespace
            - pod
            sourceMatch:
            - name: alertname
              value: GrpcErrors
            targetMatch:
            - name: alertname
              value: GrpcLogErrors
        enabled: false
    annotations: {}
    config: {}
    env:
      SPRING_CLOUD_KUBERNETES_ENABLED: "true"
      SPRING_CONFIG_ADDITIONAL_LOCATION: file:/usr/etc/hedera/
    envFrom: []
    fullnameOverride: ""
    global:
      hostname: ""
      image: {}
      middleware: false
      namespaceOverride: ""
      podAnnotations: {}
      useReleaseForNameLabel: false
    hpa:
      behavior: {}
      enabled: false
      maxReplicas: 3
      metrics:
      - resource:
          name: cpu
          target:
            averageUtilization: 80
            type: Utilization
        type: Resource
      minReplicas: 1
    image:
      pullPolicy: IfNotPresent
      registry: gcr.io
      repository: mirrornode/hedera-mirror-grpc
      tag: ""
    imagePullSecrets: []
    ingress:
      annotations:
        traefik.ingress.kubernetes.io/router.middlewares: '{{ include "hedera-mirror-grpc.namespace"
          . }}-{{ include "hedera-mirror-grpc.fullname" . }}@kubernetescrd'
      enabled: true
      hosts:
      - host: ""
        paths:
        - /com.hedera.mirror.api.proto.ConsensusService
        - /com.hedera.mirror.api.proto.NetworkService
        - /grpc.reflection.v1alpha.ServerReflection
      tls:
        enabled: false
        secretName: ""
    labels: {}
    livenessProbe:
      httpGet:
        path: /actuator/health/liveness
        port: http
      initialDelaySeconds: 50
      periodSeconds: 10
      timeoutSeconds: 2
    middleware:
    - circuitBreaker:
        expression: NetworkErrorRatio() > 0.10 || ResponseCodeRatio(500, 600, 0, 600)
          > 0.25
    - rateLimit:
        average: 5
        sourceCriterion:
          requestHost: true
    - retry:
        attempts: 3
        initialInterval: 250ms
    nodeSelector: {}
    podAnnotations: {}
    podDisruptionBudget:
      enabled: false
      minAvailable: 50%
    podSecurityContext:
      fsGroup: 1000
      runAsGroup: 1000
      runAsNonRoot: true
      runAsUser: 1000
      seccompProfile:
        type: RuntimeDefault
    priorityClassName: ""
    prometheusRules:
      GrpcErrors:
        annotations:
          description: '{{ $value | humanizePercentage }} gRPC {{ $labels.statusCode
            }} error rate for {{ $labels.namespace }}/{{ $labels.pod }}'
          summary: Mirror gRPC API error rate exceeds 5%
        enabled: true
        expr: sum(rate(grpc_server_processing_duration_seconds_count{application="hedera-mirror-grpc",
          statusCode!~"CANCELLED|DEADLINE_EXCEEDED|INVALID_ARGUMENT|NOT_FOUND|OK"}[5m]))
          by (namespace, pod, statusCode) / sum(rate(grpc_server_processing_duration_seconds_count{application="hedera-mirror-grpc"}[5m]))
          by (namespace, pod, statusCode) > 0.05
        for: 2m
        labels:
          application: hedera-mirror-grpc
          severity: critical
      GrpcHighCPU:
        annotations:
          description: '{{ $labels.namespace }}/{{ $labels.pod }} CPU usage reached
            {{ $value | humanizePercentage }}'
          summary: Mirror gRPC API CPU usage exceeds 80%
        enabled: true
        expr: sum(process_cpu_usage{application="hedera-mirror-grpc"}) by (namespace,
          pod) / sum(system_cpu_count{application="hedera-mirror-grpc"}) by (namespace,
          pod) > 0.8
        for: 5m
        labels:
          application: hedera-mirror-grpc
          area: resource
          severity: critical
      GrpcHighDBConnections:
        annotations:
          description: '{{ $labels.namespace }}/{{ $labels.pod }} is using {{ $value
            | humanizePercentage }} of available database connections'
          summary: Mirror gRPC API database connection utilization exceeds 75%
        enabled: true
        expr: sum(hikaricp_connections_active{application="hedera-mirror-grpc"}) by
          (namespace, pod) / sum(hikaricp_connections_max{application="hedera-mirror-grpc"})
          by (namespace, pod) > 0.75
        for: 5m
        labels:
          application: hedera-mirror-grpc
          area: resource
          severity: critical
      GrpcHighFileDescriptors:
        annotations:
          description: '{{ $labels.namespace }}/{{ $labels.pod }} file descriptor
            usage reached {{ $value | humanizePercentage }}'
          summary: Mirror gRPC API file descriptor usage exceeds 80%
        enabled: true
        expr: sum(process_files_open_files{application="hedera-mirror-grpc"}) by (namespace,
          pod) / sum(process_files_max_files{application="hedera-mirror-grpc"}) by
          (namespace, pod) > 0.8
        for: 5m
        labels:
          application: hedera-mirror-grpc
          area: resource
          severity: critical
      GrpcHighLatency:
        annotations:
          description: High latency of {{ $value | humanizeDuration }} between the
            main nodes and {{ $labels.namespace }}/{{ $labels.pod }}
          summary: Mirror gRPC API consensus to delivery (C2MD) latency exceeds 15s
        enabled: true
        expr: sum(rate(hedera_mirror_publish_latency_seconds_sum{application="hedera-mirror-grpc"}[5m]))
          by (namespace, pod) / sum(rate(hedera_mirror_publish_latency_seconds_count{application="hedera-mirror-grpc"}[5m]))
          by (namespace, pod) > 15
        for: 1m
        labels:
          application: hedera-mirror-grpc
          severity: critical
      GrpcHighMemory:
        annotations:
          description: '{{ $labels.namespace }}/{{ $labels.pod }} memory usage reached
            {{ $value | humanizePercentage }}'
          summary: Mirror gRPC API memory usage exceeds 80%
        enabled: true
        expr: sum(jvm_memory_used_bytes{application="hedera-mirror-grpc"}) by (namespace,
          pod) / sum(jvm_memory_max_bytes{application="hedera-mirror-grpc"}) by (namespace,
          pod) > 0.8
        for: 5m
        labels:
          application: hedera-mirror-grpc
          area: resource
          severity: critical
      GrpcLogErrors:
        annotations:
          description: Logs for {{ $labels.namespace }}/{{ $labels.pod }} have reached
            {{ $value }} error messages/s in a 3m period
          summary: High rate of log errors
        enabled: true
        expr: sum(increase(log4j2_events_total{application="hedera-mirror-grpc", level="error"}[1m]))
          by (namespace, pod) >= 2
        for: 3m
        labels:
          application: hedera-mirror-grpc
          severity: critical
      GrpcNoPodsReady:
        annotations:
          description: No gRPC API instances are currently running in {{ $labels.namespace
            }}
          summary: No gRPC API instances running
        enabled: true
        expr: sum(kube_pod_status_ready{pod=~".*-grpc-.*",condition="true"}) by (namespace)
          < 1
        for: 2m
        labels:
          application: hedera-mirror-grpc
          area: resource
          severity: critical
      GrpcNoSubscribers:
        annotations:
          description: '{{ $labels.namespace }} has {{ $value }} subscribers for {{
            $labels.type }}'
          summary: Mirror gRPC API has no subscribers
        enabled: true
        expr: sum(hedera_mirror_subscribers{application="hedera-mirror-grpc"}) by
          (namespace, type) <= 0
        for: 5m
        labels:
          application: hedera-mirror-grpc
          severity: critical
      GrpcQueryLatency:
        annotations:
          description: High average database query latency of {{ $value | humanizeDuration
            }} for {{ $labels.namespace }}/{{ $labels.pod }}
          summary: Mirror gRPC API query latency exceeds 1s
        enabled: true
        expr: sum(rate(spring_data_repository_invocations_seconds_sum{application="hedera-mirror-grpc"}[5m]))
          by (namespace, pod) / sum(rate(spring_data_repository_invocations_seconds_count{application="hedera-mirror-grpc"}[5m]))
          by (namespace, pod) > 1
        for: 1m
        labels:
          application: hedera-mirror-grpc
          severity: warning
      enabled: false
    rbac:
      enabled: true
    readinessProbe:
      httpGet:
        path: /actuator/health/readiness
        port: http
      initialDelaySeconds: 40
      timeoutSeconds: 2
    resources:
      limits:
        cpu: 2
        memory: 2048Mi
      requests:
        cpu: 100m
        memory: 128Mi
    revisionHistoryLimit: 3
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true
    service:
      annotations:
        traefik.ingress.kubernetes.io/service.serversscheme: h2c
      port: 5600
      type: ClusterIP
    serviceAccount:
      create: true
    serviceMonitor:
      enabled: false
      interval: 30s
    terminationGracePeriodSeconds: 60
    tolerations: []
    updateStrategy:
      rollingUpdate:
        maxSurge: 10%
        maxUnavailable: 25%
      type: RollingUpdate
    volumeMounts:
      config:
        mountPath: /usr/etc/hedera
    volumes:
      config:
        secret:
          defaultMode: 420
          secretName: '{{ include "hedera-mirror-grpc.fullname" . }}'
  hedera-mirror-importer:
    addressBook: ""
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/component: importer
            topologyKey: kubernetes.io/hostname
          weight: 100
    alertmanager:
      inhibitRules:
        InhibitAllWhenPodIssues:
          enabled: true
          matches:
          - equal:
            - namespace
            - pod
            sourceMatch:
            - name: area
              value: resource
            targetMatch:
            - name: application
              value: hedera-mirror-importer
        InhibitCloudLatencyAlertsWhenCloudError:
          enabled: true
          matches:
          - equal:
            - type
            - namespace
            - pod
            sourceMatch:
            - name: alertname
              value: ImporterCloudStorageErrors
            targetMatch:
            - name: alertname
              value: ImporterCloudStorageLatency
        InhibitLogAlertsWhenErrors:
          enabled: true
          matches:
          - equal:
            - namespace
            - pod
            sourceMatch:
            - name: alertname
              regex: true
              value: Importer[a-zA-Z]+Errors
            targetMatch:
            - name: alertname
              value: ImporterLogErrors
        InhibitParserLatencyAlertsWhenParseOrVerificationErrors:
          enabled: true
          matches:
          - equal:
            - namespace
            - pod
            - type
            sourceMatch:
            - name: alertname
              regex: true
              value: (ImporterFileVerificationErrors|ImporterParseErrors)
            targetMatch:
            - name: area
              value: parser
        InhibitStreamAlertsWhenCloudErrors:
          enabled: true
          matches:
          - equal:
            - type
            - namespace
            - pod
            sourceMatch:
            - name: area
              value: cloud
            targetMatch:
            - name: area
              regex: true
              value: (parser|downloader)
        InhibitVerificationErrorsWhenNoConsensus:
          enabled: true
          matches:
          - equal:
            - type
            - namespace
            - pod
            sourceMatch:
            - name: alertname
              value: ImporterNoConsensus
            targetMatch:
            - name: alertname
              value: ImporterFileVerificationErrors
        enabled: false
    annotations: {}
    config:
      hedera:
        mirror:
          importer:
            db:
              loadBalance: false
    env:
      SPRING_CLOUD_KUBERNETES_ENABLED: "true"
      SPRING_CONFIG_ADDITIONAL_LOCATION: file:/usr/etc/hedera/
    envFrom: []
    fullnameOverride: ""
    global:
      hostname: ""
      image: {}
      namespaceOverride: ""
      podAnnotations: {}
      useReleaseForNameLabel: false
    image:
      pullPolicy: IfNotPresent
      registry: gcr.io
      repository: mirrornode/hedera-mirror-importer
      tag: ""
    imagePullSecrets: []
    labels: {}
    livenessProbe:
      httpGet:
        path: /actuator/health/liveness
        port: http
      initialDelaySeconds: 60
      periodSeconds: 30
      timeoutSeconds: 2
    nameOverride: ""
    networkPolicy:
      enabled: false
    nodeSelector: {}
    podAnnotations: {}
    podDisruptionBudget:
      enabled: false
      minAvailable: 1
    podMonitor:
      enabled: false
      interval: 30s
    podSecurityContext:
      fsGroup: 1000
      runAsGroup: 1000
      runAsNonRoot: true
      runAsUser: 1000
      seccompProfile:
        type: RuntimeDefault
    priorityClassName: ""
    prometheusRules:
      ImporterBalanceParseLatency:
        annotations:
          description: Averaging {{ $value | humanizeDuration }} trying to parse balance
            stream files for {{ $labels.namespace }}/{{ $labels.pod }}
          summary: Took longer than 2m to parse balance stream files
        enabled: true
        expr: sum(rate(hedera_mirror_parse_duration_seconds_sum{application="hedera-mirror-importer",type="BALANCE"}[15m]))
          by (namespace, pod) / sum(rate(hedera_mirror_parse_duration_seconds_count{application="hedera-mirror-importer",type="BALANCE"}[15m]))
          by (namespace, pod) > 120
        for: 2m
        labels:
          application: hedera-mirror-importer
          area: parser
          severity: critical
          type: BALANCE
      ImporterBalanceStreamFallenBehind:
        annotations:
          description: The difference between the file timestamp and when it was processed
            is {{ $value | humanizeDuration }} for {{ $labels.namespace }}/{{ $labels.pod
            }}
          summary: Mirror Importer balance stream processing has fallen behind
        enabled: true
        expr: sum(rate(hedera_mirror_parse_latency_seconds_sum{application="hedera-mirror-importer",type="BALANCE"}[15m]))
          by (namespace, pod) / sum(rate(hedera_mirror_parse_latency_seconds_count{application="hedera-mirror-importer",type="BALANCE"}[15m]))
          by (namespace, pod) > 960
        for: 3m
        labels:
          application: hedera-mirror-importer
          area: parser
          severity: critical
          type: BALANCE
      ImporterCloudStorageErrors:
        annotations:
          description: Averaging {{ $value | humanizePercentage }} error rate trying
            to {{ if ne $labels.action "list" }} retrieve{{ end }} {{ $labels.action
            }} {{ $labels.type }} files from cloud storage for {{ $labels.namespace
            }}/{{ $labels.pod }}
          summary: Cloud storage error rate exceeds 5%
        enabled: true
        expr: (sum(rate(hedera_mirror_download_request_seconds_count{application="hedera-mirror-importer",
          status!~"^2.*"}[2m])) by (namespace, pod, type, action) / sum(rate(hedera_mirror_download_request_seconds_count{application="hedera-mirror-importer"}[2m]))
          by (namespace, pod, type, action)) > 0.05
        for: 2m
        labels:
          application: hedera-mirror-importer
          area: cloud
          severity: critical
      ImporterCloudStorageLatency:
        annotations:
          description: Averaging {{ $value | humanizeDuration }} cloud storage latency
            trying to {{ if ne $labels.action "list" }} retrieve{{ end }} {{ $labels.action
            }} {{ $labels.type }} files from cloud storage for {{ $labels.namespace
            }}/{{ $labels.pod }}
          summary: Cloud storage latency exceeds 2s
        enabled: true
        expr: sum(rate(hedera_mirror_download_request_seconds_sum{application="hedera-mirror-importer",
          status=~"^2.*"}[2m])) by (namespace, pod, type, action) / sum(rate(hedera_mirror_download_request_seconds_count{application="hedera-mirror-importer",
          status=~"^2.*"}[2m])) by (namespace, pod, type, action) > 2
        for: 2m
        labels:
          application: hedera-mirror-importer
          area: cloud
          severity: critical
      ImporterFileVerificationErrors:
        annotations:
          description: Error rate of {{ $value | humanizePercentage }} trying to download
            and verify {{ $labels.type }} stream files for {{ $labels.namespace }}/{{
            $labels.pod }}
          summary: '{{ $labels.type }} file verification error rate exceeds 5%'
        enabled: true
        expr: sum(rate(hedera_mirror_download_stream_verification_seconds_count{application="hedera-mirror-importer",
          success="false"}[3m])) by (namespace, pod, type) / sum(rate(hedera_mirror_download_stream_verification_seconds_count{application="hedera-mirror-importer"}[3m]))
          by (namespace, pod, type) > 0.05
        for: 2m
        labels:
          application: hedera-mirror-importer
          area: downloader
          severity: critical
      ImporterHighCPU:
        annotations:
          description: '{{ $labels.namespace }}/{{ $labels.pod }} CPU usage reached
            {{ $value | humanizePercentage }}'
          summary: Mirror Importer CPU usage exceeds 80%
        enabled: true
        expr: sum(process_cpu_usage{application="hedera-mirror-importer"}) by (namespace,
          pod) / sum(system_cpu_count{application="hedera-mirror-importer"}) by (namespace,
          pod) > 0.8
        for: 5m
        labels:
          application: hedera-mirror-importer
          area: resource
          severity: critical
      ImporterHighDBConnections:
        annotations:
          description: '{{ $labels.namespace }}/{{ $labels.pod }} is using {{ $value
            | humanizePercentage }} of available database connections'
          summary: Mirror Importer database connection utilization exceeds 75%
        enabled: true
        expr: sum(hikaricp_connections_active{application="hedera-mirror-importer"})
          by (namespace, pod) / sum(hikaricp_connections_max{application="hedera-mirror-importer"})
          by (namespace, pod) > 0.75
        for: 5m
        labels:
          application: hedera-mirror-importer
          area: resource
          severity: critical
      ImporterHighFileDescriptors:
        annotations:
          description: '{{ $labels.namespace }}/{{ $labels.pod }} file descriptor
            usage reached {{ $value | humanizePercentage }}'
          summary: Mirror Importer file descriptor usage exceeds 80%
        enabled: true
        expr: sum(process_files_open_files{application="hedera-mirror-importer"})
          by (namespace, pod) / sum(process_files_max_files{application="hedera-mirror-importer"})
          by (namespace, pod) > 0.8
        for: 5m
        labels:
          application: hedera-mirror-importer
          area: resource
          severity: critical
      ImporterHighMemory:
        annotations:
          description: '{{ $labels.namespace }}/{{ $labels.pod }} memory usage reached
            {{ $value | humanizePercentage }}'
          summary: Mirror Importer memory usage exceeds 80%
        enabled: true
        expr: sum(jvm_memory_used_bytes{application="hedera-mirror-importer"}) by
          (namespace, pod) / sum(jvm_memory_max_bytes{application="hedera-mirror-importer"})
          by (namespace, pod) > 0.8
        for: 5m
        labels:
          application: hedera-mirror-importer
          area: resource
          severity: critical
      ImporterLogErrors:
        annotations:
          description: Logs for {{ $labels.namespace }}/{{ $labels.pod }} have reached
            {{ $value }} error messages/s in a 3m period
          summary: High rate of log errors
        enabled: true
        expr: sum(increase(log4j2_events_total{application="hedera-mirror-importer",
          level="error"}[2m])) by (namespace, pod) >= 2
        for: 3m
        labels:
          application: hedera-mirror-importer
          area: log
          severity: critical
      ImporterNoBalanceFile:
        annotations:
          description: Have not processed a balance stream file in {{ $labels.namespace
            }} for the last 15 min
          summary: Missing balance stream files
        enabled: true
        expr: sum(increase(hedera_mirror_parse_latency_seconds_count{application="hedera-mirror-importer",type="BALANCE"}[16m]))
          by (namespace) < 1
        for: 5m
        labels:
          application: hedera-mirror-importer
          area: parser
          severity: critical
          type: BALANCE
      ImporterNoConsensus:
        annotations:
          description: '{{ $labels.namespace }} only able to achieve {{ $value | humanizePercentage
            }} consensus during {{ $labels.type }} stream signature verification'
          summary: Unable to verify {{ $labels.type }} stream signatures
        enabled: true
        expr: sum(rate(hedera_mirror_download_signature_verification_total{application="hedera-mirror-importer",
          status="CONSENSUS_REACHED"}[2m])) by (namespace, pod, type) / sum(rate(hedera_mirror_download_signature_verification_total{application="hedera-mirror-importer"}[2m]))
          by (namespace, pod, type) < 0.33
        for: 2m
        labels:
          application: hedera-mirror-importer
          area: downloader
          severity: critical
      ImporterNoPodsReady:
        annotations:
          description: No importer instances are currently ready in {{ $labels.namespace
            }}
          summary: No importer instances are ready
        enabled: true
        expr: sum(kube_pod_status_ready{pod=~".*-importer-.*",condition="true"}) by
          (namespace) < 1
        for: 2m
        labels:
          application: hedera-mirror-importer
          area: resource
          severity: critical
      ImporterNoTransactions:
        annotations:
          description: Record stream TPS has dropped to {{ $value }} for {{ $labels.namespace
            }}. This may be because importer is down, can't connect to cloud storage,
            main nodes are not uploading, error parsing the streams, no traffic, etc.
          summary: No transactions seen for 2m
        enabled: true
        expr: sum(rate(hedera_mirror_transaction_latency_seconds_count{application="hedera-mirror-importer"}[5m]))
          by (namespace) <= 0
        for: 2m
        labels:
          application: hedera-mirror-importer
          area: parser
          severity: critical
          type: RECORD
      ImporterParseErrors:
        annotations:
          description: Encountered {{ $value | humanizePercentage }} errors trying
            to parse {{ $labels.type }} stream files for {{ $labels.namespace }}/{{
            $labels.pod }}
          summary: Error rate parsing {{ $labels.type }} exceeds 5%
        enabled: true
        expr: sum(rate(hedera_mirror_parse_duration_seconds_count{application="hedera-mirror-importer",
          success="false"}[3m])) by (namespace, pod, type) / sum(rate(hedera_mirror_parse_duration_seconds_count{application="hedera-mirror-importer"}[3m]))
          by (namespace, pod, type) > 0.05
        for: 2m
        labels:
          application: hedera-mirror-importer
          area: parser
          severity: critical
      ImporterPublishLatency:
        annotations:
          description: Took {{ $value | humanizeDuration }} to publish {{ $labels.entity
            }}s to {{ $labels.type }} for {{ $labels.namespace }}/{{ $labels.pod }}
          summary: Slow {{ $labels.type }} publishing
        enabled: true
        expr: sum(rate(hedera_mirror_importer_publish_duration_seconds_sum{application="hedera-mirror-importer"}[3m]))
          by (namespace, pod, type, entity) / sum(rate(hedera_mirror_importer_publish_duration_seconds_count{application="hedera-mirror-importer"}[3m]))
          by (namespace, pod, type, entity) > 1
        for: 1m
        labels:
          application: hedera-mirror-importer
          area: publisher
          severity: critical
      ImporterQueryLatency:
        annotations:
          description: High average database query latency of {{ $value | humanizeDuration
            }} for {{ $labels.namespace }}/{{ $labels.pod }}
          summary: Mirror Importer query latency exceeds 1s
        enabled: true
        expr: sum(rate(spring_data_repository_invocations_seconds_sum{application="hedera-mirror-importer"}[5m]))
          by (namespace, pod) / sum(rate(spring_data_repository_invocations_seconds_count{application="hedera-mirror-importer"}[5m]))
          by (namespace, pod) > 1
        for: 1m
        labels:
          application: hedera-mirror-importer
          severity: warning
      ImporterReconciliationFailed:
        annotations:
          description: Unable to reconcile balance information for {{ $labels.namespace
            }}/{{ $labels.pod }}
          summary: Mirror reconciliation job failed
        enabled: true
        expr: sum(hedera_mirror_reconciliation{application="hedera-mirror-importer"})
          by (namespace, pod) > 2
        for: 1m
        labels:
          application: hedera-mirror-importer
          severity: critical
      ImporterRecordParseLatency:
        annotations:
          description: Averaging {{ $value | humanizeDuration }} trying to parse record
            stream files for {{ $labels.namespace }}/{{ $labels.pod }}
          summary: Took longer than 2s to parse record stream files
        enabled: true
        expr: sum(rate(hedera_mirror_parse_duration_seconds_sum{application="hedera-mirror-importer",type="RECORD"}[3m]))
          by (namespace, pod) / sum(rate(hedera_mirror_parse_duration_seconds_count{application="hedera-mirror-importer",type="RECORD"}[3m]))
          by (namespace, pod) > 2
        for: 1m
        labels:
          application: hedera-mirror-importer
          area: parser
          severity: critical
          type: RECORD
      ImporterRecordStreamFallenBehind:
        annotations:
          description: The difference between the file timestamp and when it was processed
            is {{ $value | humanizeDuration }} for {{ $labels.namespace }}/{{ $labels.pod
            }}
          summary: Mirror Importer record stream processing has fallen behind
        enabled: true
        expr: sum(rate(hedera_mirror_parse_latency_seconds_sum{application="hedera-mirror-importer",type="RECORD"}[3m]))
          by (namespace, pod) / sum(rate(hedera_mirror_parse_latency_seconds_count{application="hedera-mirror-importer",type="RECORD"}[3m]))
          by (namespace, pod) > 20
        for: 3m
        labels:
          application: hedera-mirror-importer
          area: parser
          severity: critical
          type: RECORD
      ImporterStreamCloseInterval:
        annotations:
          description: '{{ $labels.namespace }}/{{ $labels.pod }} file stream should
            close every 2s but is actually {{ $value | humanizeDuration }}. This could
            just be due to the lack of traffic in the environment, but it could potentially
            be something more serious to look into.'
          summary: Record stream close interval exceeds 10s
        enabled: true
        expr: sum(rate(hedera_mirror_stream_close_latency_seconds_sum{application="hedera-mirror-importer",
          type="RECORD"}[5m])) by (namespace, pod) / sum(rate(hedera_mirror_stream_close_latency_seconds_count{application="hedera-mirror-importer",
          type="RECORD"}[5m])) by (namespace, pod) > 10
        for: 1m
        labels:
          application: hedera-mirror-importer
          area: downloader
          severity: warning
          type: RECORD
      enabled: false
    rbac:
      enabled: true
    readinessProbe:
      httpGet:
        path: /actuator/health/readiness
        port: http
      initialDelaySeconds: 60
      timeoutSeconds: 2
    replicas: 1
    resources:
      limits:
        cpu: 4
        memory: 4Gi
      requests:
        cpu: 200m
        memory: 512Mi
    revisionHistoryLimit: 3
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true
    serviceAccount:
      create: true
    startupProbe:
      failureThreshold: 8640
      httpGet:
        path: /actuator/health/startup
        port: http
      initialDelaySeconds: 60
      periodSeconds: 10
      timeoutSeconds: 2
    terminationGracePeriodSeconds: 30
    tolerations: []
    updateStrategy:
      type: Recreate
    volumeMounts:
      config:
        mountPath: /usr/etc/hedera
    volumes:
      config:
        secret:
          defaultMode: 420
          secretName: '{{ include "hedera-mirror-importer.fullname" . }}'
  hedera-mirror-monitor:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/component: monitor
            topologyKey: kubernetes.io/hostname
          weight: 100
    alertmanager:
      inhibitRules:
        InhibitAllWhenPodIssues:
          enabled: true
          matches:
          - equal:
            - namespace
            - pod
            sourceMatch:
            - name: area
              value: resource
            targetMatch:
            - name: application
              value: hedera-mirror-monitor
        InhibitHandleLatencyAlertWhenHighPublishLatency:
          enabled: true
          matches:
          - equal:
            - namespace
            - pod
            sourceMatch:
            - name: alertname
              value: MonitorPublishLatency
            targetMatch:
            - name: alertname
              value: MonitorPublishToHandleLatency
        InhibitPublishAlertsWhenPublishStopped:
          enabled: true
          matches:
          - equal:
            - namespace
            - pod
            sourceMatch:
            - name: alertname
              value: MonitorPublishStopped
            targetMatch:
            - name: mode
              value: publish
        enabled: false
    annotations: {}
    config: {}
    env:
      SPRING_CLOUD_KUBERNETES_ENABLED: "true"
      SPRING_CONFIG_ADDITIONAL_LOCATION: file:/usr/etc/hedera/
    envFrom: []
    fullnameOverride: ""
    global:
      hostname: ""
      image: {}
      namespaceOverride: ""
      podAnnotations: {}
      useReleaseForNameLabel: false
    image:
      pullPolicy: IfNotPresent
      registry: gcr.io
      repository: mirrornode/hedera-mirror-monitor
      tag: ""
    imagePullSecrets: []
    ingress:
      annotations: {}
      enabled: true
      hosts:
      - host: ""
        paths:
        - /actuator/health/cluster
      tls:
        enabled: false
        secretName: ""
    labels: {}
    livenessProbe:
      failureThreshold: 5
      httpGet:
        path: /actuator/health/liveness
        port: http
      initialDelaySeconds: 90
      periodSeconds: 10
      timeoutSeconds: 2
    nodeSelector: {}
    podAnnotations: {}
    podSecurityContext:
      fsGroup: 1000
      runAsGroup: 1000
      runAsNonRoot: true
      runAsUser: 1000
      seccompProfile:
        type: RuntimeDefault
    priorityClassName: ""
    prometheusRules:
      MonitorHighCPU:
        annotations:
          description: '{{ $labels.namespace }}/{{ $labels.pod }} CPU usage reached
            {{ $value | humanizePercentage }}'
          summary: Mirror Monitor CPU usage exceeds 80%
        enabled: true
        expr: sum(process_cpu_usage{application="hedera-mirror-monitor"}) by (namespace,
          pod) / sum(system_cpu_count{application="hedera-mirror-monitor"}) by (namespace,
          pod) > 0.8
        for: 5m
        labels:
          application: hedera-mirror-monitor
          area: resource
          severity: critical
      MonitorHighMemory:
        annotations:
          description: '{{ $labels.namespace }}/{{ $labels.pod }} memory usage reached
            {{ $value | humanizePercentage }}'
          summary: Mirror Monitor memory usage exceeds 80%
        enabled: true
        expr: sum(jvm_memory_used_bytes{application="hedera-mirror-monitor"}) by (namespace,
          pod) / sum(jvm_memory_max_bytes{application="hedera-mirror-monitor"}) by
          (namespace, pod) > 0.8
        for: 5m
        labels:
          application: hedera-mirror-monitor
          area: resource
          severity: critical
      MonitorLogErrors:
        annotations:
          description: Logs for {{ $labels.namespace }}/{{ $labels.pod }} have reached
            {{ $value }} error messages/s in a 3m period
          summary: High rate of log errors
        enabled: true
        expr: sum(increase(log4j2_events_total{application="hedera-mirror-monitor",
          level="error"}[2m])) by (namespace, pod) >= 2
        for: 3m
        labels:
          application: hedera-mirror-monitor
          severity: critical
      MonitorNoPodsReady:
        annotations:
          description: No monitor instances are currently running in {{ $labels.namespace
            }}
          summary: No monitor instances running
        enabled: true
        expr: sum(kube_pod_status_ready{pod=~".*-monitor-.*",condition="true"}) by
          (namespace) < 1
        for: 2m
        labels:
          application: hedera-mirror-monitor
          area: resource
          severity: critical
      MonitorPublishErrors:
        annotations:
          description: Averaging {{ $value | humanizePercentage }} error rate publishing
            '{{ $labels.scenario }}' scenario from {{ $labels.namespace }}/{{ $labels.pod
            }}
          summary: Publish error rate exceeds 50%
        enabled: true
        expr: sum(rate(hedera_mirror_monitor_publish_submit_seconds_count{application="hedera-mirror-monitor",status!="SUCCESS"}[2m]))
          by (namespace, pod, scenario) / sum(rate(hedera_mirror_monitor_publish_submit_seconds_count{application="hedera-mirror-monitor"}[2m]))
          by (namespace, pod, scenario) > 0.50
        for: 3m
        labels:
          application: hedera-mirror-monitor
          mode: publish
          severity: critical
      MonitorPublishLatency:
        annotations:
          description: Averaging {{ $value | humanizeDuration }} publish latency for
            '{{ $labels.scenario }}' scenario for {{ $labels.namespace }}/{{ $labels.pod
            }}
          summary: Publish latency exceeds 7s
        enabled: true
        expr: sum(rate(hedera_mirror_monitor_publish_submit_seconds_sum{application="hedera-mirror-monitor"}[2m]))
          by (namespace, pod, scenario) / sum(rate(hedera_mirror_monitor_publish_submit_seconds_count{application="hedera-mirror-monitor"}[2m]))
          by (namespace, pod, scenario) > 7
        for: 5m
        labels:
          application: hedera-mirror-monitor
          mode: publish
          severity: warning
      MonitorPublishPlatformNotActive:
        annotations:
          description: Averaging {{ $value | humanizePercentage }} PLATFORM_NOT_ACTIVE
            or UNAVAILABLE errors while attempting to publish in {{ $labels.namespace
            }}
          summary: Platform is not active
        enabled: true
        expr: sum(rate(hedera_mirror_monitor_publish_submit_seconds_count{application="hedera-mirror-monitor",status=~"(PLATFORM_NOT_ACTIVE|UNAVAILABLE)"}[2m]))
          by (namespace) / sum(rate(hedera_mirror_monitor_publish_submit_seconds_count{application="hedera-mirror-monitor"}[2m]))
          by (namespace) > 0.33
        for: 1m
        labels:
          application: hedera-mirror-monitor
          mode: publish
          severity: warning
      MonitorPublishStopped:
        annotations:
          description: Publish TPS dropped to {{ $value }} for '{{ $labels.scenario
            }}' scenario for {{ $labels.namespace }}/{{ $labels.pod }}
          summary: Publishing stopped
        enabled: true
        expr: (sum(rate(hedera_mirror_monitor_publish_submit_seconds_sum{application="hedera-mirror-monitor"}[2m]))
          by (namespace, pod, scenario) / sum(rate(hedera_mirror_monitor_publish_submit_seconds_count{application="hedera-mirror-monitor"}[2m]))
          by (namespace, pod, scenario) > 0 or on() vector(0)) <= 0
        for: 2m
        labels:
          application: hedera-mirror-monitor
          mode: publish
          severity: critical
      MonitorPublishToHandleLatency:
        annotations:
          description: Averaging {{ $value | humanizeDuration }} transaction latency
            for '{{ $labels.scenario }}' scenario for {{ $labels.namespace }}/{{ $labels.pod
            }}
          summary: Submit to transaction being handled latency exceeds 11s
        enabled: true
        expr: sum(rate(hedera_mirror_monitor_publish_handle_seconds_sum{application="hedera-mirror-monitor"}[5m]))
          by (namespace, pod, scenario) / sum(rate(hedera_mirror_monitor_publish_handle_seconds_count{application="hedera-mirror-monitor"}[5m]))
          by (namespace, pod, scenario) > 11
        for: 5m
        labels:
          application: hedera-mirror-monitor
          mode: publish
          severity: warning
      MonitorSubscribeLatency:
        annotations:
          description: 'Latency averaging {{ $value | humanizeDuration }} for ''{{
            $labels.scenario }}'' #{{ $labels.subscriber }} scenario for {{ $labels.namespace
            }}/{{ $labels.pod }}'
          summary: End to end latency exceeds 14s
        enabled: true
        expr: sum(rate(hedera_mirror_monitor_subscribe_e2e_seconds_sum{application="hedera-mirror-monitor"}[2m]))
          by (namespace, pod, scenario, subscriber) / sum(rate(hedera_mirror_monitor_subscribe_e2e_seconds_count{application="hedera-mirror-monitor"}[2m]))
          by (namespace, pod, scenario, subscriber) > 14
        for: 5m
        labels:
          application: hedera-mirror-monitor
          severity: critical
      MonitorSubscribeStopped:
        annotations:
          description: 'TPS dropped to {{ $value }} for ''{{ $labels.scenario }}''
            #{{ $labels.subscriber }} scenario for {{ $labels.namespace }}/{{ $labels.pod
            }}'
          summary: Subscription stopped
        enabled: true
        expr: (sum(rate(hedera_mirror_monitor_subscribe_e2e_seconds_sum{application="hedera-mirror-monitor"}[2m]))
          by (namespace, pod, subscriber, scenario) / sum(rate(hedera_mirror_monitor_subscribe_e2e_seconds_count{application="hedera-mirror-monitor"}[2m]))
          by (namespace, pod, subscriber, scenario) > 0 or on() vector(0)) <= 0
        for: 2m
        labels:
          application: hedera-mirror-monitor
          severity: critical
      enabled: false
    rbac:
      enabled: true
    readinessProbe:
      failureThreshold: 5
      httpGet:
        path: /actuator/health/readiness
        port: http
      initialDelaySeconds: 60
      timeoutSeconds: 2
    replicas: 1
    resources:
      limits:
        cpu: 500m
        memory: 768Mi
      requests:
        cpu: 100m
        memory: 256Mi
    revisionHistoryLimit: 3
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true
    service:
      annotations: {}
      port: 80
      type: ClusterIP
    serviceAccount:
      create: true
    serviceMonitor:
      enabled: false
      interval: 30s
    terminationGracePeriodSeconds: 60
    tolerations: []
    updateStrategy:
      rollingUpdate:
        maxSurge: 10%
        maxUnavailable: 25%
      type: RollingUpdate
    volumeMounts:
      config:
        mountPath: /usr/etc/hedera
    volumes:
      config:
        secret:
          defaultMode: 420
          secretName: '{{ include "hedera-mirror-monitor.fullname" . }}'
  hedera-mirror-rest:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/component: rest
            topologyKey: kubernetes.io/hostname
          weight: 100
    alertmanager:
      inhibitRules:
        InhibitAllWhenPodIssues:
          enabled: true
          matches:
          - equal:
            - namespace
            - pod
            sourceMatch:
            - name: area
              value: resource
            targetMatch:
            - name: application
              value: hedera-mirror-rest
        enabled: false
    annotations: {}
    config:
      hedera:
        mirror:
          rest:
            metrics:
              config:
                authentication: false
    env:
      CONFIG_PATH: /usr/etc/hedera/
    envFrom: []
    fullnameOverride: mirror-node-rest
    global:
      hostname: ""
      image: {}
      middleware: false
      namespaceOverride: ""
      podAnnotations: {}
      useReleaseForNameLabel: false
    hpa:
      behavior: {}
      enabled: true
      maxReplicas: 15
      metrics:
      - resource:
          name: cpu
          target:
            averageUtilization: 80
            type: Utilization
        type: Resource
      minReplicas: 1
    image:
      pullPolicy: IfNotPresent
      registry: docker.io/
      repository: cabob/hedera-mirror-rest
      tag: latest
    imagePullSecrets: []
    ingress:
      annotations:
        traefik.ingress.kubernetes.io/router.middlewares: '{{ include "hedera-mirror-rest.namespace"
          . }}-{{ include "hedera-mirror-rest.fullname" . }}@kubernetescrd'
      enabled: true
      hosts:
      - host: ""
        paths:
        - /api/v1
      tls:
        enabled: false
        secretName: ""
    labels: {}
    livenessProbe:
      httpGet:
        path: /health/liveness
        port: http
      initialDelaySeconds: 25
      timeoutSeconds: 2
    middleware:
    - circuitBreaker:
        expression: NetworkErrorRatio() > 0.25 || ResponseCodeRatio(500, 600, 0, 600)
          > 0.25
    - retry:
        attempts: 10
        initialInterval: 100ms
    nodeSelector: {}
    podAnnotations: {}
    podDisruptionBudget:
      enabled: false
      minAvailable: 50%
    podSecurityContext:
      fsGroup: 1000
      runAsGroup: 1000
      runAsNonRoot: true
      runAsUser: 1000
      seccompProfile:
        type: RuntimeDefault
    priorityClassName: ""
    prometheusRules:
      RestErrors:
        annotations:
          description: REST API 5xx error rate for {{ $labels.namespace }}/{{ $labels.pod
            }} is {{ $value | humanizePercentage }}
          summary: Mirror REST API error rate exceeds 5%
        enabled: true
        expr: sum(rate(api_request_total{container="rest",code=~"^5.."}[5m])) by (namespace,
          pod) / sum(rate(api_request_total{container="rest"}[5m])) by (namespace,
          pod) > 0.05
        for: 1m
        labels:
          application: hedera-mirror-rest
          severity: critical
      RestHighCPU:
        annotations:
          description: '{{ $labels.namespace }}/{{ $labels.pod }} CPU usage reached
            {{ $value | humanizePercentage }}'
          summary: Mirror REST API CPU usage exceeds 80%
        enabled: true
        expr: sum(nodejs_process_cpu_usage_percentage{container="rest"}) by (namespace,
          pod) > 80
        for: 5m
        labels:
          application: hedera-mirror-rest
          area: resource
          severity: critical
      RestNoPodsReady:
        annotations:
          description: No REST API instances are currently running in {{ $labels.namespace
            }}
          summary: No REST API instances running
        enabled: true
        expr: sum(kube_pod_status_ready{pod=~".*-rest-.*",condition="true"}) by (namespace)
          < 1
        for: 2m
        labels:
          application: hedera-mirror-rest
          area: resource
          severity: critical
      RestNoRequests:
        annotations:
          description: REST API has not seen any requests to {{ $labels.namespace
            }} for 5m
          summary: No Mirror REST API requests seen for awhile
        enabled: true
        expr: sum(rate(api_all_request_total{container="rest"}[3m])) by (namespace)
          <= 0
        for: 5m
        labels:
          application: hedera-mirror-rest
          severity: warning
      RestRequestLatency:
        annotations:
          description: '{{ $labels.namespace }}/{{ $labels.pod }} is taking {{ $value
            | humanizeDuration }} to generate a response'
          summary: Mirror REST API request latency exceeds 2s
        enabled: true
        expr: sum(rate(api_request_duration_milliseconds_sum{container="rest"}[5m]))
          by (namespace, pod) / sum(rate(api_request_duration_milliseconds_count{container="rest"}[5m]))
          by (namespace, pod) > 2000
        for: 1m
        labels:
          application: hedera-mirror-rest
          severity: warning
      enabled: false
    readinessProbe:
      httpGet:
        path: /health/readiness
        port: http
      initialDelaySeconds: 30
      timeoutSeconds: 2
    resources:
      limits:
        cpu: 750m
        memory: 350Mi
      requests:
        cpu: 300m
        memory: 64Mi
    revisionHistoryLimit: 3
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true
    service:
      annotations: {}
      port: 80
      type: ClusterIP
    serviceAccount:
      create: true
    serviceMonitor:
      enabled: false
      interval: 30s
    terminationGracePeriodSeconds: 60
    test:
      checks:
        accounts: true
        transactions: true
      enabled: true
      image:
        pullPolicy: IfNotPresent
        repository: bats/bats
        tag: 1.7.0
    tolerations: []
    updateStrategy:
      rollingUpdate:
        maxSurge: 10%
        maxUnavailable: 25%
      type: RollingUpdate
    volumeMounts:
      config:
        mountPath: /usr/etc/hedera
    volumes:
      config:
        secret:
          defaultMode: 420
          secretName: '{{ include "hedera-mirror-rest.fullname" . }}'
  hedera-mirror-rosetta:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/component: rosetta
            topologyKey: kubernetes.io/hostname
          weight: 100
    alertmanager:
      inhibitRules:
        RosettaInhibitAll:
          enabled: true
          matches:
          - equal:
            - namespace
            - pod
            sourceMatch:
            - name: area
              value: resource
            targetMatch:
            - name: application
              value: hedera-mirror-rosetta
        enabled: false
    annotations: {}
    config: {}
    env:
      CONFIG_PATH: /usr/etc/hedera/
    envFrom: []
    fullnameOverride: ""
    global:
      hostname: ""
      image: {}
      middleware: false
      namespaceOverride: ""
      podAnnotations: {}
      useReleaseForNameLabel: false
    hpa:
      behavior: {}
      enabled: true
      maxReplicas: 10
      metrics:
      - resource:
          name: cpu
          target:
            averageUtilization: 80
            type: Utilization
        type: Resource
      minReplicas: 1
    image:
      pullPolicy: IfNotPresent
      registry: gcr.io
      repository: mirrornode/hedera-mirror-rosetta
      tag: ""
    imagePullSecrets: []
    ingress:
      annotations:
        traefik.ingress.kubernetes.io/router.middlewares: '{{ include "hedera-mirror-rosetta.namespace"
          . }}-{{ include "hedera-mirror-rosetta.fullname" . }}@kubernetescrd'
      enabled: true
      hosts:
      - host: ""
        paths:
        - /rosetta/account
        - /rosetta/block
        - /rosetta/call
        - /rosetta/construction
        - /rosetta/events
        - /rosetta/mempool
        - /rosetta/network
        - /rosetta/search
      tls:
        enabled: false
        secretName: ""
    labels: {}
    livenessProbe:
      failureThreshold: 5
      httpGet:
        path: /health/liveness
        port: http
      initialDelaySeconds: 30
      periodSeconds: 10
      timeoutSeconds: 2
    middleware:
    - circuitBreaker:
        expression: NetworkErrorRatio() > 0.25 || ResponseCodeRatio(500, 600, 0, 600)
          > 0.25
    - inFlightReq:
        amount: 5
        sourceCriterion:
          ipStrategy:
            depth: 1
    - rateLimit:
        average: 10
        sourceCriterion:
          requestHost: true
    - retry:
        attempts: 3
        initialInterval: 100ms
    - stripPrefix:
        prefixes:
        - /rosetta
    nodeSelector: {}
    podAnnotations: {}
    podDisruptionBudget:
      enabled: false
      minAvailable: 50%
    podSecurityContext:
      fsGroup: 1000
      runAsGroup: 1000
      runAsNonRoot: true
      runAsUser: 1000
      seccompProfile:
        type: RuntimeDefault
    priorityClassName: ""
    prometheusRules:
      RosettaApiErrors:
        annotations:
          description: Rosetta API 5xx error rate for {{ $labels.namespace }}/{{ $labels.pod
            }} is {{ $value | humanizePercentage }}
          summary: Mirror Rosetta API error rate exceeds 5%
        enabled: true
        expr: sum(rate(hedera_mirror_rosetta_request_duration_count{application="hedera-mirror-rosetta",status_code=~"^5.."}[5m]))
          by (namespace, pod) / sum(rate(hedera_mirror_rosetta_request_duration_count{application="hedera-mirror-rosetta"}[5m]))
          by (namespace, pod) > 0.05
        for: 1m
        labels:
          application: hedera-mirror-rosetta
          severity: critical
      RosettaNoPodsReady:
        annotations:
          description: No Rosetta API instances are currently running in {{ $labels.namespace
            }}
          summary: No Rosetta API instances running
        enabled: true
        expr: sum(kube_pod_status_ready{pod=~".*-rosetta-.*",condition="true"}) by
          (namespace) < 1
        for: 2m
        labels:
          application: hedera-mirror-rosetta
          area: resource
          severity: critical
      RosettaNoRequests:
        annotations:
          description: Rosetta API has not seen any requests to {{ $labels.namespace
            }} for 5m
          summary: No Mirror Rosetta API requests seen for awhile
        enabled: false
        expr: sum(rate(hedera_mirror_rosetta_request_duration_count{application="hedera-mirror-rosetta"}[3m]))
          by (namespace) <= 0
        for: 5m
        labels:
          application: hedera-mirror-rosetta
          severity: warning
      RosettaSlowResponse:
        annotations:
          description: '{{ $labels.namespace }}/{{ $labels.pod }} is taking {{ $value
            | humanizeDuration }} to generate a response'
          summary: Mirror Rosetta API is taking too long to respond
        enabled: true
        expr: sum(rate(hedera_mirror_rosetta_request_duration_sum{application="hedera-mirror-rosetta"}[5m]))
          by (namespace, pod) / sum(rate(hedera_mirror_rosetta_request_duration_count{application="hedera-mirror-rosetta"}[5m]))
          by (namespace, pod) > 2000
        for: 1m
        labels:
          application: hedera-mirror-rosetta
          severity: warning
      enabled: false
    readinessProbe:
      failureThreshold: 5
      httpGet:
        path: /health/readiness
        port: http
      initialDelaySeconds: 30
      timeoutSeconds: 2
    resources:
      limits:
        cpu: 500m
        memory: 200Mi
      requests:
        cpu: 50m
        memory: 64Mi
    revisionHistoryLimit: 3
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true
    service:
      annotations: {}
      port: 80
      type: ClusterIP
    serviceAccount:
      create: true
    serviceMonitor:
      enabled: false
      interval: 30s
    terminationGracePeriodSeconds: 60
    test:
      enabled: true
      git:
        branch: ""
        repository: hashgraph/hedera-mirror-node
      image:
        pullPolicy: IfNotPresent
        repository: postman/newman
        tag: 5.3.1-alpine
    tolerations: []
    updateStrategy:
      rollingUpdate:
        maxSurge: 10%
        maxUnavailable: 25%
      type: RollingUpdate
    volumeMounts:
      config:
        mountPath: /usr/etc/hedera
    volumes:
      config:
        secret:
          defaultMode: 420
          secretName: '{{ include "hedera-mirror-rosetta.fullname" . }}'
  hedera-mirror-web3:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/component: web3
            topologyKey: kubernetes.io/hostname
          weight: 100
    alertmanager:
      inhibitRules:
        InhibitAllWhenPodIssues:
          enabled: true
          matches:
          - equal:
            - namespace
            - pod
            sourceMatch:
            - name: area
              value: resource
            targetMatch:
            - name: application
              value: hedera-mirror-web3
        enabled: false
    annotations: {}
    config: {}
    env:
      SPRING_CLOUD_KUBERNETES_ENABLED: "true"
      SPRING_CONFIG_ADDITIONAL_LOCATION: file:/usr/etc/hedera/
    envFrom: []
    fullnameOverride: ""
    global:
      hostname: ""
      image: {}
      middleware: false
      namespaceOverride: ""
      podAnnotations: {}
      useReleaseForNameLabel: false
    hpa:
      behavior: {}
      enabled: false
      maxReplicas: 8
      metrics:
      - resource:
          name: cpu
          target:
            averageUtilization: 80
            type: Utilization
        type: Resource
      minReplicas: 1
    image:
      pullPolicy: IfNotPresent
      registry: gcr.io
      repository: mirrornode/hedera-mirror-web3
      tag: ""
    imagePullSecrets: []
    ingress:
      annotations:
        traefik.ingress.kubernetes.io/router.middlewares: '{{ include "hedera-mirror-web3.namespace"
          . }}-{{ include "hedera-mirror-web3.fullname" . }}@kubernetescrd'
      enabled: true
      hosts:
      - host: ""
        paths:
        - /web3
      tls:
        enabled: false
        secretName: ""
    labels: {}
    livenessProbe:
      httpGet:
        path: /actuator/health/liveness
        port: http
      initialDelaySeconds: 50
      periodSeconds: 10
      timeoutSeconds: 2
    middleware:
    - circuitBreaker:
        expression: NetworkErrorRatio() > 0.10 || ResponseCodeRatio(500, 600, 0, 600)
          > 0.25
    - inFlightReq:
        amount: 5
        sourceCriterion:
          ipStrategy:
            depth: 1
    - rateLimit:
        average: 10
        sourceCriterion:
          requestHost: true
    - retry:
        attempts: 3
        initialInterval: 100ms
    nodeSelector: {}
    podAnnotations: {}
    podDisruptionBudget:
      enabled: false
      minAvailable: 50%
    podSecurityContext:
      fsGroup: 1000
      runAsGroup: 1000
      runAsNonRoot: true
      runAsUser: 1000
      seccompProfile:
        type: RuntimeDefault
    priorityClassName: ""
    prometheusRules:
      Web3Errors:
        annotations:
          description: '{{ $value | humanizePercentage }} Web3 server error rate for
            {{ $labels.namespace }}/{{ $labels.pod }}'
          summary: Mirror Web3 API error rate exceeds 5%
        enabled: true
        expr: sum(rate(http_server_requests_seconds_count{application="hedera-mirror-web3",
          status="SERVER_ERROR"}[5m])) by (namespace, pod) / sum(rate(http_server_requests_seconds_count{application="hedera-mirror-web3"}[5m]))
          by (namespace, pod) > 0.05
        for: 2m
        labels:
          application: hedera-mirror-web3
          severity: critical
      Web3HighCPU:
        annotations:
          description: '{{ $labels.namespace }}/{{ $labels.pod }} CPU usage reached
            {{ $value | humanizePercentage }}'
          summary: Mirror Web3 API CPU usage exceeds 80%
        enabled: true
        expr: sum(process_cpu_usage{application="hedera-mirror-web3"}) by (namespace,
          pod) / sum(system_cpu_count{application="hedera-mirror-web3"}) by (namespace,
          pod) > 0.8
        for: 5m
        labels:
          application: hedera-mirror-web3
          area: resource
          severity: critical
      Web3HighDBConnections:
        annotations:
          description: '{{ $labels.namespace }}/{{ $labels.pod }} is using {{ $value
            | humanizePercentage }} of available database connections'
          summary: Mirror Web3 API database connection utilization exceeds 75%
        enabled: true
        expr: sum(hikaricp_connections_active{application="hedera-mirror-web3"}) by
          (namespace, pod) / sum(hikaricp_connections_max{application="hedera-mirror-web3"})
          by (namespace, pod) > 0.75
        for: 5m
        labels:
          application: hedera-mirror-web3
          area: resource
          severity: critical
      Web3HighFileDescriptors:
        annotations:
          description: '{{ $labels.namespace }}/{{ $labels.pod }} file descriptor
            usage reached {{ $value | humanizePercentage }}'
          summary: Mirror Web3 API file descriptor usage exceeds 80%
        enabled: true
        expr: sum(process_files_open_files{application="hedera-mirror-web3"}) by (namespace,
          pod) / sum(process_files_max_files{application="hedera-mirror-web3"}) by
          (namespace, pod) > 0.8
        for: 5m
        labels:
          application: hedera-mirror-web3
          area: resource
          severity: critical
      Web3HighMemory:
        annotations:
          description: '{{ $labels.namespace }}/{{ $labels.pod }} memory usage reached
            {{ $value | humanizePercentage }}'
          summary: Mirror Web3 API memory usage exceeds 80%
        enabled: true
        expr: sum(jvm_memory_used_bytes{application="hedera-mirror-web3"}) by (namespace,
          pod) / sum(jvm_memory_max_bytes{application="hedera-mirror-web3"}) by (namespace,
          pod) > 0.8
        for: 5m
        labels:
          application: hedera-mirror-web3
          area: resource
          severity: critical
      Web3LogErrors:
        annotations:
          description: Logs for {{ $labels.namespace }}/{{ $labels.pod }} have reached
            {{ $value }} error messages/s in a 3m period
          summary: High rate of log errors
        enabled: true
        expr: sum(increase(logback_events_total{application="hedera-mirror-web3",
          level="error"}[1m])) by (namespace, pod) >= 2
        for: 3m
        labels:
          application: hedera-mirror-web3
          severity: critical
      Web3NoPodsReady:
        annotations:
          description: No Web3 API instances are currently running in {{ $labels.namespace
            }}
          summary: No Web3 API instances running
        enabled: true
        expr: sum(kube_pod_status_ready{pod=~".*-web3-.*",condition="true"}) by (namespace)
          < 1
        for: 2m
        labels:
          application: hedera-mirror-web3
          area: resource
          severity: critical
      Web3NoRequests:
        annotations:
          description: Web3 API has not seen any requests to {{ $labels.namespace
            }} for 5m
          summary: No Web3 API requests seen for awhile
        enabled: true
        expr: sum(rate(http_server_requests_seconds_count{application="hedera-mirror-web3"}[3m]))
          by (namespace) <= 0
        for: 5m
        labels:
          application: hedera-mirror-web3
          severity: warning
      Web3QueryLatency:
        annotations:
          description: High average database query latency of {{ $value | humanizeDuration
            }} for {{ $labels.namespace }}/{{ $labels.pod }}
          summary: Mirror Web3 API query latency exceeds 1s
        enabled: true
        expr: sum(rate(spring_data_repository_invocations_seconds_sum{application="hedera-mirror-web3"}[5m]))
          by (namespace, pod) / sum(rate(spring_data_repository_invocations_seconds_count{application="hedera-mirror-web3"}[5m]))
          by (namespace, pod) > 1
        for: 1m
        labels:
          application: hedera-mirror-web3
          severity: warning
      Web3RequestLatency:
        annotations:
          description: High average request latency of {{ $value | humanizeDuration
            }} for {{ $labels.namespace }}/{{ $labels.pod }}
          summary: Mirror Web3 API request latency exceeds 2s
        enabled: true
        expr: sum(rate(http_server_requests_seconds_sum{application="hedera-mirror-web3"}[5m]))
          by (namespace, pod) / sum(rate(http_server_requests_seconds_count{application="hedera-mirror-web3"}[5m]))
          by (namespace, pod) > 2
        for: 1m
        labels:
          application: hedera-mirror-web3
          severity: warning
      enabled: false
    rbac:
      enabled: true
    readinessProbe:
      httpGet:
        path: /actuator/health/readiness
        port: http
      initialDelaySeconds: 40
      timeoutSeconds: 2
    resources:
      limits:
        cpu: 2
        memory: 2048Mi
      requests:
        cpu: 100m
        memory: 128Mi
    revisionHistoryLimit: 3
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true
    service:
      annotations: {}
      port: 80
      type: ClusterIP
    serviceAccount:
      create: true
    serviceMonitor:
      enabled: false
      interval: 30s
    terminationGracePeriodSeconds: 60
    test:
      enabled: true
      image:
        pullPolicy: IfNotPresent
        repository: postman/newman
        tag: 5.3.1-alpine
    tolerations: []
    updateStrategy:
      rollingUpdate:
        maxSurge: 10%
        maxUnavailable: 25%
      type: RollingUpdate
    volumeMounts:
      config:
        mountPath: /usr/etc/hedera
    volumes:
      config:
        secret:
          defaultMode: 420
          secretName: '{{ include "hedera-mirror-web3.fullname" . }}'
  importer:
    addressBook: CvQGCgoxNzIuMjcuMC4zGgUwLjAuMyLMBjMwODIwMWEyMzAwZDA2MDkyYTg2NDg4NmY3MGQwMTAxMDEwNTAwMDM4MjAxOGYwMDMwODIwMThhMDI4MjAxODEwMGExMjAwNmYyNTI3MjQyY2Q3ZjUzYjljNGZkNWRlODY5YzU2MmZmNGVkOGE0YWIzYTYyOTYzZjNmODE4OGJlNzhmMjU4ZWFmNDJiOWMzZTNlOGY1ODY5NzE2MDQxM2JiZjc3YWFkZmFmMGQ5ZmZkODY5ODRiM2JjNGZiNWYwMmUxMWU4YWNiNmVjNjdlNDI2NGFkZGNhMTNmZmRlODM2NzU3OGMyNzkyZmM2YWUzMjZlN2IzNDg1NmQyM2YwMjU3NGY1YTI3NDgxNTYwMDMyZDczNzQ5ZGFmOWQzMzJhNzg4MzUxMGU2MGVhMjIyODg5YTNiZmMzYmVkZjM1MGYzYTM2Y2FkYTM5Yjk0NTM5Yzc0ZmI4MjY1Njc4YmQ1YzE0OGZiYzkxNjlhZGNhY2Y2NzE5YjkyZWU4ZDQyYTlkMmY4OTIzY2M0ZmUzYjQ5NGM0NjdmNGU5OGEyYTQ5ZTBhMGVmNWFhNDMzY2JlYzQzOGJhYTI1NmFlNjFhMDU5MGNlNGU2N2QwYjNmZTVmMmRhOWFkOTBjMjZlMjI5ZDI4ZDY3OTQ1Njk3OGQ2NTczNTczZWJiNDYyMzliYWQ0YWJmYWI0ZmZlNjRlYTRhMzk3YzZiYzVmNzE3NWUxYWMzNjQ4OGYxN2M2NzczYjVlZDkzNDE1ZTU3YzQxYzZjZDg4NjZkYmQxMjM2MDA2MjMzOWNiMTU0ZDg1YWQxYWVjZDFkNzYwYTA5NDFkODBlMjhmZTI0NjAzM2E4NjhkYmJjODk3NWYzNjA0NDMxZmNlOWFmOTFkYzQyNTQwYWIwNGIxYmJlYzIxZjM1Zjc1ZjMwYWY2ZjA1Yzg5ODY0MjkzODZlMmI0MzliZmQyZmQ1OTEyYzgyNjAwODAxZTljMDg1N2YxNjVlODg3ZjIyMzNkYzJjMDk4YmExMjY3YjA1OWRiOGEyZGI3MTRmZTUwNjY4ZGJmMDkzNjA0YTVkZjNjMjMyMTQ2NzIzMjExY2U1ODU1MjYzOGZhZTlmYzA2NGE3ZjA5YjAyMDMwMTAwMDEyAhgDQgoKBKwbAAMQv4cDUAE=
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/component: importer
            topologyKey: kubernetes.io/hostname
          weight: 100
    alertmanager:
      inhibitRules:
        InhibitAllWhenPodIssues:
          enabled: true
          matches:
          - equal:
            - namespace
            - pod
            sourceMatch:
            - name: area
              value: resource
            targetMatch:
            - name: application
              value: hedera-mirror-importer
        InhibitCloudLatencyAlertsWhenCloudError:
          enabled: true
          matches:
          - equal:
            - type
            - namespace
            - pod
            sourceMatch:
            - name: alertname
              value: ImporterCloudStorageErrors
            targetMatch:
            - name: alertname
              value: ImporterCloudStorageLatency
        InhibitLogAlertsWhenErrors:
          enabled: true
          matches:
          - equal:
            - namespace
            - pod
            sourceMatch:
            - name: alertname
              regex: true
              value: Importer[a-zA-Z]+Errors
            targetMatch:
            - name: alertname
              value: ImporterLogErrors
        InhibitParserLatencyAlertsWhenParseOrVerificationErrors:
          enabled: true
          matches:
          - equal:
            - namespace
            - pod
            - type
            sourceMatch:
            - name: alertname
              regex: true
              value: (ImporterFileVerificationErrors|ImporterParseErrors)
            targetMatch:
            - name: area
              value: parser
        InhibitStreamAlertsWhenCloudErrors:
          enabled: true
          matches:
          - equal:
            - type
            - namespace
            - pod
            sourceMatch:
            - name: area
              value: cloud
            targetMatch:
            - name: area
              regex: true
              value: (parser|downloader)
        InhibitVerificationErrorsWhenNoConsensus:
          enabled: true
          matches:
          - equal:
            - type
            - namespace
            - pod
            sourceMatch:
            - name: alertname
              value: ImporterNoConsensus
            targetMatch:
            - name: alertname
              value: ImporterFileVerificationErrors
        enabled: false
    annotations: {}
    config:
      hedera:
        mirror:
          importer:
            db:
              loadBalance: false
    db:
      password: ""
      username: mirror_importer
    enabled: true
    env:
      SPRING_CLOUD_KUBERNETES_ENABLED: "true"
      SPRING_CONFIG_ADDITIONAL_LOCATION: file:/usr/etc/hedera-mirror-importer/
    envFrom:
    - secretRef:
        name: mirror-passwords
    - secretRef:
        name: mirror-redis
    fullnameOverride: ""
    global:
      hostname: ""
      image: {}
      namespaceOverride: ""
      podAnnotations: {}
      useReleaseForNameLabel: false
    image:
      pullPolicy: Always
      registry: gcr.io
      repository: mirrornode/hedera-mirror-importer
      tag: ""
    imagePullSecrets: []
    labels: {}
    livenessProbe:
      httpGet:
        path: /actuator/health/liveness
        port: http
      initialDelaySeconds: 60
      periodSeconds: 30
      timeoutSeconds: 2
    nameOverride: ""
    networkPolicy:
      enabled: false
    nodeSelector: {}
    podAnnotations: {}
    podDisruptionBudget:
      enabled: false
      minAvailable: 1
    podMonitor:
      enabled: false
      interval: 30s
    podSecurityContext:
      fsGroup: 1000
      runAsGroup: 1000
      runAsNonRoot: true
      runAsUser: 1000
      seccompProfile:
        type: RuntimeDefault
    priorityClassName: ""
    prometheusRules:
      ImporterBalanceParseLatency:
        annotations:
          description: Averaging {{ $value | humanizeDuration }} trying to parse balance
            stream files for {{ $labels.namespace }}/{{ $labels.pod }}
          summary: Took longer than 2m to parse balance stream files
        enabled: true
        expr: sum(rate(hedera_mirror_parse_duration_seconds_sum{application="hedera-mirror-importer",type="BALANCE"}[15m]))
          by (namespace, pod) / sum(rate(hedera_mirror_parse_duration_seconds_count{application="hedera-mirror-importer",type="BALANCE"}[15m]))
          by (namespace, pod) > 120
        for: 2m
        labels:
          application: hedera-mirror-importer
          area: parser
          severity: critical
          type: BALANCE
      ImporterBalanceStreamFallenBehind:
        annotations:
          description: The difference between the file timestamp and when it was processed
            is {{ $value | humanizeDuration }} for {{ $labels.namespace }}/{{ $labels.pod
            }}
          summary: Mirror Importer balance stream processing has fallen behind
        enabled: true
        expr: sum(rate(hedera_mirror_parse_latency_seconds_sum{application="hedera-mirror-importer",type="BALANCE"}[15m]))
          by (namespace, pod) / sum(rate(hedera_mirror_parse_latency_seconds_count{application="hedera-mirror-importer",type="BALANCE"}[15m]))
          by (namespace, pod) > 960
        for: 3m
        labels:
          application: hedera-mirror-importer
          area: parser
          severity: critical
          type: BALANCE
      ImporterCloudStorageErrors:
        annotations:
          description: Averaging {{ $value | humanizePercentage }} error rate trying
            to {{ if ne $labels.action "list" }} retrieve{{ end }} {{ $labels.action
            }} {{ $labels.type }} files from cloud storage for {{ $labels.namespace
            }}/{{ $labels.pod }}
          summary: Cloud storage error rate exceeds 5%
        enabled: true
        expr: (sum(rate(hedera_mirror_download_request_seconds_count{application="hedera-mirror-importer",
          status!~"^2.*"}[2m])) by (namespace, pod, type, action) / sum(rate(hedera_mirror_download_request_seconds_count{application="hedera-mirror-importer"}[2m]))
          by (namespace, pod, type, action)) > 0.05
        for: 2m
        labels:
          application: hedera-mirror-importer
          area: cloud
          severity: critical
      ImporterCloudStorageLatency:
        annotations:
          description: Averaging {{ $value | humanizeDuration }} cloud storage latency
            trying to {{ if ne $labels.action "list" }} retrieve{{ end }} {{ $labels.action
            }} {{ $labels.type }} files from cloud storage for {{ $labels.namespace
            }}/{{ $labels.pod }}
          summary: Cloud storage latency exceeds 2s
        enabled: true
        expr: sum(rate(hedera_mirror_download_request_seconds_sum{application="hedera-mirror-importer",
          status=~"^2.*"}[2m])) by (namespace, pod, type, action) / sum(rate(hedera_mirror_download_request_seconds_count{application="hedera-mirror-importer",
          status=~"^2.*"}[2m])) by (namespace, pod, type, action) > 2
        for: 2m
        labels:
          application: hedera-mirror-importer
          area: cloud
          severity: critical
      ImporterFileVerificationErrors:
        annotations:
          description: Error rate of {{ $value | humanizePercentage }} trying to download
            and verify {{ $labels.type }} stream files for {{ $labels.namespace }}/{{
            $labels.pod }}
          summary: '{{ $labels.type }} file verification error rate exceeds 5%'
        enabled: true
        expr: sum(rate(hedera_mirror_download_stream_verification_seconds_count{application="hedera-mirror-importer",
          success="false"}[3m])) by (namespace, pod, type) / sum(rate(hedera_mirror_download_stream_verification_seconds_count{application="hedera-mirror-importer"}[3m]))
          by (namespace, pod, type) > 0.05
        for: 2m
        labels:
          application: hedera-mirror-importer
          area: downloader
          severity: critical
      ImporterHighCPU:
        annotations:
          description: '{{ $labels.namespace }}/{{ $labels.pod }} CPU usage reached
            {{ $value | humanizePercentage }}'
          summary: Mirror Importer CPU usage exceeds 80%
        enabled: true
        expr: sum(process_cpu_usage{application="hedera-mirror-importer"}) by (namespace,
          pod) / sum(system_cpu_count{application="hedera-mirror-importer"}) by (namespace,
          pod) > 0.8
        for: 5m
        labels:
          application: hedera-mirror-importer
          area: resource
          severity: critical
      ImporterHighDBConnections:
        annotations:
          description: '{{ $labels.namespace }}/{{ $labels.pod }} is using {{ $value
            | humanizePercentage }} of available database connections'
          summary: Mirror Importer database connection utilization exceeds 75%
        enabled: true
        expr: sum(hikaricp_connections_active{application="hedera-mirror-importer"})
          by (namespace, pod) / sum(hikaricp_connections_max{application="hedera-mirror-importer"})
          by (namespace, pod) > 0.75
        for: 5m
        labels:
          application: hedera-mirror-importer
          area: resource
          severity: critical
      ImporterHighFileDescriptors:
        annotations:
          description: '{{ $labels.namespace }}/{{ $labels.pod }} file descriptor
            usage reached {{ $value | humanizePercentage }}'
          summary: Mirror Importer file descriptor usage exceeds 80%
        enabled: true
        expr: sum(process_files_open_files{application="hedera-mirror-importer"})
          by (namespace, pod) / sum(process_files_max_files{application="hedera-mirror-importer"})
          by (namespace, pod) > 0.8
        for: 5m
        labels:
          application: hedera-mirror-importer
          area: resource
          severity: critical
      ImporterHighMemory:
        annotations:
          description: '{{ $labels.namespace }}/{{ $labels.pod }} memory usage reached
            {{ $value | humanizePercentage }}'
          summary: Mirror Importer memory usage exceeds 80%
        enabled: true
        expr: sum(jvm_memory_used_bytes{application="hedera-mirror-importer"}) by
          (namespace, pod) / sum(jvm_memory_max_bytes{application="hedera-mirror-importer"})
          by (namespace, pod) > 0.8
        for: 5m
        labels:
          application: hedera-mirror-importer
          area: resource
          severity: critical
      ImporterLogErrors:
        annotations:
          description: Logs for {{ $labels.namespace }}/{{ $labels.pod }} have reached
            {{ $value }} error messages/s in a 3m period
          summary: High rate of log errors
        enabled: true
        expr: sum(increase(log4j2_events_total{application="hedera-mirror-importer",
          level="error"}[2m])) by (namespace, pod) >= 2
        for: 3m
        labels:
          application: hedera-mirror-importer
          area: log
          severity: critical
      ImporterNoBalanceFile:
        annotations:
          description: Have not processed a balance stream file in {{ $labels.namespace
            }} for the last 15 min
          summary: Missing balance stream files
        enabled: true
        expr: sum(increase(hedera_mirror_parse_latency_seconds_count{application="hedera-mirror-importer",type="BALANCE"}[16m]))
          by (namespace) < 1
        for: 5m
        labels:
          application: hedera-mirror-importer
          area: parser
          severity: critical
          type: BALANCE
      ImporterNoConsensus:
        annotations:
          description: '{{ $labels.namespace }} only able to achieve {{ $value | humanizePercentage
            }} consensus during {{ $labels.type }} stream signature verification'
          summary: Unable to verify {{ $labels.type }} stream signatures
        enabled: true
        expr: sum(rate(hedera_mirror_download_signature_verification_total{application="hedera-mirror-importer",
          status="CONSENSUS_REACHED"}[2m])) by (namespace, pod, type) / sum(rate(hedera_mirror_download_signature_verification_total{application="hedera-mirror-importer"}[2m]))
          by (namespace, pod, type) < 0.33
        for: 2m
        labels:
          application: hedera-mirror-importer
          area: downloader
          severity: critical
      ImporterNoPodsReady:
        annotations:
          description: No importer instances are currently ready in {{ $labels.namespace
            }}
          summary: No importer instances are ready
        enabled: true
        expr: sum(kube_pod_status_ready{pod=~".*-importer-.*",condition="true"}) by
          (namespace) < 1
        for: 2m
        labels:
          application: hedera-mirror-importer
          area: resource
          severity: critical
      ImporterNoTransactions:
        annotations:
          description: Record stream TPS has dropped to {{ $value }} for {{ $labels.namespace
            }}. This may be because importer is down, can't connect to cloud storage,
            main nodes are not uploading, error parsing the streams, no traffic, etc.
          summary: No transactions seen for 2m
        enabled: true
        expr: sum(rate(hedera_mirror_transaction_latency_seconds_count{application="hedera-mirror-importer"}[5m]))
          by (namespace) <= 0
        for: 2m
        labels:
          application: hedera-mirror-importer
          area: parser
          severity: critical
          type: RECORD
      ImporterParseErrors:
        annotations:
          description: Encountered {{ $value | humanizePercentage }} errors trying
            to parse {{ $labels.type }} stream files for {{ $labels.namespace }}/{{
            $labels.pod }}
          summary: Error rate parsing {{ $labels.type }} exceeds 5%
        enabled: true
        expr: sum(rate(hedera_mirror_parse_duration_seconds_count{application="hedera-mirror-importer",
          success="false"}[3m])) by (namespace, pod, type) / sum(rate(hedera_mirror_parse_duration_seconds_count{application="hedera-mirror-importer"}[3m]))
          by (namespace, pod, type) > 0.05
        for: 2m
        labels:
          application: hedera-mirror-importer
          area: parser
          severity: critical
      ImporterPublishLatency:
        annotations:
          description: Took {{ $value | humanizeDuration }} to publish {{ $labels.entity
            }}s to {{ $labels.type }} for {{ $labels.namespace }}/{{ $labels.pod }}
          summary: Slow {{ $labels.type }} publishing
        enabled: true
        expr: sum(rate(hedera_mirror_importer_publish_duration_seconds_sum{application="hedera-mirror-importer"}[3m]))
          by (namespace, pod, type, entity) / sum(rate(hedera_mirror_importer_publish_duration_seconds_count{application="hedera-mirror-importer"}[3m]))
          by (namespace, pod, type, entity) > 1
        for: 1m
        labels:
          application: hedera-mirror-importer
          area: publisher
          severity: critical
      ImporterQueryLatency:
        annotations:
          description: High average database query latency of {{ $value | humanizeDuration
            }} for {{ $labels.namespace }}/{{ $labels.pod }}
          summary: Mirror Importer query latency exceeds 1s
        enabled: true
        expr: sum(rate(spring_data_repository_invocations_seconds_sum{application="hedera-mirror-importer"}[5m]))
          by (namespace, pod) / sum(rate(spring_data_repository_invocations_seconds_count{application="hedera-mirror-importer"}[5m]))
          by (namespace, pod) > 1
        for: 1m
        labels:
          application: hedera-mirror-importer
          severity: warning
      ImporterReconciliationFailed:
        annotations:
          description: Unable to reconcile balance information for {{ $labels.namespace
            }}/{{ $labels.pod }}
          summary: Mirror reconciliation job failed
        enabled: true
        expr: sum(hedera_mirror_reconciliation{application="hedera-mirror-importer"})
          by (namespace, pod) > 2
        for: 1m
        labels:
          application: hedera-mirror-importer
          severity: critical
      ImporterRecordParseLatency:
        annotations:
          description: Averaging {{ $value | humanizeDuration }} trying to parse record
            stream files for {{ $labels.namespace }}/{{ $labels.pod }}
          summary: Took longer than 2s to parse record stream files
        enabled: true
        expr: sum(rate(hedera_mirror_parse_duration_seconds_sum{application="hedera-mirror-importer",type="RECORD"}[3m]))
          by (namespace, pod) / sum(rate(hedera_mirror_parse_duration_seconds_count{application="hedera-mirror-importer",type="RECORD"}[3m]))
          by (namespace, pod) > 2
        for: 1m
        labels:
          application: hedera-mirror-importer
          area: parser
          severity: critical
          type: RECORD
      ImporterRecordStreamFallenBehind:
        annotations:
          description: The difference between the file timestamp and when it was processed
            is {{ $value | humanizeDuration }} for {{ $labels.namespace }}/{{ $labels.pod
            }}
          summary: Mirror Importer record stream processing has fallen behind
        enabled: true
        expr: sum(rate(hedera_mirror_parse_latency_seconds_sum{application="hedera-mirror-importer",type="RECORD"}[3m]))
          by (namespace, pod) / sum(rate(hedera_mirror_parse_latency_seconds_count{application="hedera-mirror-importer",type="RECORD"}[3m]))
          by (namespace, pod) > 20
        for: 3m
        labels:
          application: hedera-mirror-importer
          area: parser
          severity: critical
          type: RECORD
      ImporterStreamCloseInterval:
        annotations:
          description: '{{ $labels.namespace }}/{{ $labels.pod }} file stream should
            close every 2s but is actually {{ $value | humanizeDuration }}. This could
            just be due to the lack of traffic in the environment, but it could potentially
            be something more serious to look into.'
          summary: Record stream close interval exceeds 10s
        enabled: true
        expr: sum(rate(hedera_mirror_stream_close_latency_seconds_sum{application="hedera-mirror-importer",
          type="RECORD"}[5m])) by (namespace, pod) / sum(rate(hedera_mirror_stream_close_latency_seconds_count{application="hedera-mirror-importer",
          type="RECORD"}[5m])) by (namespace, pod) > 10
        for: 1m
        labels:
          application: hedera-mirror-importer
          area: downloader
          severity: warning
          type: RECORD
      enabled: false
    rbac:
      enabled: true
    readinessProbe:
      httpGet:
        path: /actuator/health/readiness
        port: http
      initialDelaySeconds: 60
      timeoutSeconds: 2
    replicas: 1
    resources:
      limits:
        cpu: 1
        memory: 4Gi
      requests:
        cpu: 200m
        memory: 512Mi
    revisionHistoryLimit: 3
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true
    serviceAccount:
      create: true
    startupProbe:
      failureThreshold: 8640
      httpGet:
        path: /actuator/health/startup
        port: http
      initialDelaySeconds: 60
      periodSeconds: 10
      timeoutSeconds: 2
    terminationGracePeriodSeconds: 30
    tolerations: []
    updateStrategy:
      type: Recreate
    volumeMounts:
      abook:
        mountPath: /usr/etc/hedera-mirror-importer/local-dev-1-node.addressbook.f102.json.bin
      config:
        mountPath: /usr/etc/hedera
    volumes:
      abook:
        secret:
          secretName: abook-config
      config:
        secret:
          defaultMode: 420
          secretName: app-config
  labels: {}
  monitor:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/component: monitor
            topologyKey: kubernetes.io/hostname
          weight: 100
    alertmanager:
      inhibitRules:
        InhibitAllWhenPodIssues:
          enabled: true
          matches:
          - equal:
            - namespace
            - pod
            sourceMatch:
            - name: area
              value: resource
            targetMatch:
            - name: application
              value: hedera-mirror-monitor
        InhibitHandleLatencyAlertWhenHighPublishLatency:
          enabled: true
          matches:
          - equal:
            - namespace
            - pod
            sourceMatch:
            - name: alertname
              value: MonitorPublishLatency
            targetMatch:
            - name: alertname
              value: MonitorPublishToHandleLatency
        InhibitPublishAlertsWhenPublishStopped:
          enabled: true
          matches:
          - equal:
            - namespace
            - pod
            sourceMatch:
            - name: alertname
              value: MonitorPublishStopped
            targetMatch:
            - name: mode
              value: publish
        enabled: false
    annotations: {}
    config: {}
    enabled: true
    env:
      HEDERA_MIRROR_MONITOR_MIRROR_NODE_GRPC_HOST: '{{ .Release.Name }}-grpc'
      HEDERA_MIRROR_MONITOR_MIRROR_NODE_REST_HOST: '{{ .Release.Name }}-rest'
      HEDERA_MIRROR_MONITOR_MIRROR_NODE_REST_PORT: "80"
      SPRING_CLOUD_KUBERNETES_ENABLED: "true"
      SPRING_CONFIG_ADDITIONAL_LOCATION: file:/usr/etc/hedera/
    envFrom: []
    fullnameOverride: ""
    global:
      hostname: ""
      image: {}
      namespaceOverride: ""
      podAnnotations: {}
      useReleaseForNameLabel: false
    image:
      pullPolicy: IfNotPresent
      registry: gcr.io
      repository: mirrornode/hedera-mirror-monitor
      tag: ""
    imagePullSecrets: []
    ingress:
      annotations: {}
      enabled: true
      hosts:
      - host: ""
        paths:
        - /actuator/health/cluster
      tls:
        enabled: false
        secretName: ""
    labels: {}
    livenessProbe:
      failureThreshold: 5
      httpGet:
        path: /actuator/health/liveness
        port: http
      initialDelaySeconds: 90
      periodSeconds: 10
      timeoutSeconds: 2
    nodeSelector: {}
    podAnnotations: {}
    podSecurityContext:
      fsGroup: 1000
      runAsGroup: 1000
      runAsNonRoot: true
      runAsUser: 1000
      seccompProfile:
        type: RuntimeDefault
    priorityClassName: ""
    prometheusRules:
      MonitorHighCPU:
        annotations:
          description: '{{ $labels.namespace }}/{{ $labels.pod }} CPU usage reached
            {{ $value | humanizePercentage }}'
          summary: Mirror Monitor CPU usage exceeds 80%
        enabled: true
        expr: sum(process_cpu_usage{application="hedera-mirror-monitor"}) by (namespace,
          pod) / sum(system_cpu_count{application="hedera-mirror-monitor"}) by (namespace,
          pod) > 0.8
        for: 5m
        labels:
          application: hedera-mirror-monitor
          area: resource
          severity: critical
      MonitorHighMemory:
        annotations:
          description: '{{ $labels.namespace }}/{{ $labels.pod }} memory usage reached
            {{ $value | humanizePercentage }}'
          summary: Mirror Monitor memory usage exceeds 80%
        enabled: true
        expr: sum(jvm_memory_used_bytes{application="hedera-mirror-monitor"}) by (namespace,
          pod) / sum(jvm_memory_max_bytes{application="hedera-mirror-monitor"}) by
          (namespace, pod) > 0.8
        for: 5m
        labels:
          application: hedera-mirror-monitor
          area: resource
          severity: critical
      MonitorLogErrors:
        annotations:
          description: Logs for {{ $labels.namespace }}/{{ $labels.pod }} have reached
            {{ $value }} error messages/s in a 3m period
          summary: High rate of log errors
        enabled: true
        expr: sum(increase(log4j2_events_total{application="hedera-mirror-monitor",
          level="error"}[2m])) by (namespace, pod) >= 2
        for: 3m
        labels:
          application: hedera-mirror-monitor
          severity: critical
      MonitorNoPodsReady:
        annotations:
          description: No monitor instances are currently running in {{ $labels.namespace
            }}
          summary: No monitor instances running
        enabled: true
        expr: sum(kube_pod_status_ready{pod=~".*-monitor-.*",condition="true"}) by
          (namespace) < 1
        for: 2m
        labels:
          application: hedera-mirror-monitor
          area: resource
          severity: critical
      MonitorPublishErrors:
        annotations:
          description: Averaging {{ $value | humanizePercentage }} error rate publishing
            '{{ $labels.scenario }}' scenario from {{ $labels.namespace }}/{{ $labels.pod
            }}
          summary: Publish error rate exceeds 50%
        enabled: true
        expr: sum(rate(hedera_mirror_monitor_publish_submit_seconds_count{application="hedera-mirror-monitor",status!="SUCCESS"}[2m]))
          by (namespace, pod, scenario) / sum(rate(hedera_mirror_monitor_publish_submit_seconds_count{application="hedera-mirror-monitor"}[2m]))
          by (namespace, pod, scenario) > 0.50
        for: 3m
        labels:
          application: hedera-mirror-monitor
          mode: publish
          severity: critical
      MonitorPublishLatency:
        annotations:
          description: Averaging {{ $value | humanizeDuration }} publish latency for
            '{{ $labels.scenario }}' scenario for {{ $labels.namespace }}/{{ $labels.pod
            }}
          summary: Publish latency exceeds 7s
        enabled: true
        expr: sum(rate(hedera_mirror_monitor_publish_submit_seconds_sum{application="hedera-mirror-monitor"}[2m]))
          by (namespace, pod, scenario) / sum(rate(hedera_mirror_monitor_publish_submit_seconds_count{application="hedera-mirror-monitor"}[2m]))
          by (namespace, pod, scenario) > 7
        for: 5m
        labels:
          application: hedera-mirror-monitor
          mode: publish
          severity: warning
      MonitorPublishPlatformNotActive:
        annotations:
          description: Averaging {{ $value | humanizePercentage }} PLATFORM_NOT_ACTIVE
            or UNAVAILABLE errors while attempting to publish in {{ $labels.namespace
            }}
          summary: Platform is not active
        enabled: true
        expr: sum(rate(hedera_mirror_monitor_publish_submit_seconds_count{application="hedera-mirror-monitor",status=~"(PLATFORM_NOT_ACTIVE|UNAVAILABLE)"}[2m]))
          by (namespace) / sum(rate(hedera_mirror_monitor_publish_submit_seconds_count{application="hedera-mirror-monitor"}[2m]))
          by (namespace) > 0.33
        for: 1m
        labels:
          application: hedera-mirror-monitor
          mode: publish
          severity: warning
      MonitorPublishStopped:
        annotations:
          description: Publish TPS dropped to {{ $value }} for '{{ $labels.scenario
            }}' scenario for {{ $labels.namespace }}/{{ $labels.pod }}
          summary: Publishing stopped
        enabled: true
        expr: (sum(rate(hedera_mirror_monitor_publish_submit_seconds_sum{application="hedera-mirror-monitor"}[2m]))
          by (namespace, pod, scenario) / sum(rate(hedera_mirror_monitor_publish_submit_seconds_count{application="hedera-mirror-monitor"}[2m]))
          by (namespace, pod, scenario) > 0 or on() vector(0)) <= 0
        for: 2m
        labels:
          application: hedera-mirror-monitor
          mode: publish
          severity: critical
      MonitorPublishToHandleLatency:
        annotations:
          description: Averaging {{ $value | humanizeDuration }} transaction latency
            for '{{ $labels.scenario }}' scenario for {{ $labels.namespace }}/{{ $labels.pod
            }}
          summary: Submit to transaction being handled latency exceeds 11s
        enabled: true
        expr: sum(rate(hedera_mirror_monitor_publish_handle_seconds_sum{application="hedera-mirror-monitor"}[5m]))
          by (namespace, pod, scenario) / sum(rate(hedera_mirror_monitor_publish_handle_seconds_count{application="hedera-mirror-monitor"}[5m]))
          by (namespace, pod, scenario) > 11
        for: 5m
        labels:
          application: hedera-mirror-monitor
          mode: publish
          severity: warning
      MonitorSubscribeLatency:
        annotations:
          description: 'Latency averaging {{ $value | humanizeDuration }} for ''{{
            $labels.scenario }}'' #{{ $labels.subscriber }} scenario for {{ $labels.namespace
            }}/{{ $labels.pod }}'
          summary: End to end latency exceeds 14s
        enabled: true
        expr: sum(rate(hedera_mirror_monitor_subscribe_e2e_seconds_sum{application="hedera-mirror-monitor"}[2m]))
          by (namespace, pod, scenario, subscriber) / sum(rate(hedera_mirror_monitor_subscribe_e2e_seconds_count{application="hedera-mirror-monitor"}[2m]))
          by (namespace, pod, scenario, subscriber) > 14
        for: 5m
        labels:
          application: hedera-mirror-monitor
          severity: critical
      MonitorSubscribeStopped:
        annotations:
          description: 'TPS dropped to {{ $value }} for ''{{ $labels.scenario }}''
            #{{ $labels.subscriber }} scenario for {{ $labels.namespace }}/{{ $labels.pod
            }}'
          summary: Subscription stopped
        enabled: true
        expr: (sum(rate(hedera_mirror_monitor_subscribe_e2e_seconds_sum{application="hedera-mirror-monitor"}[2m]))
          by (namespace, pod, subscriber, scenario) / sum(rate(hedera_mirror_monitor_subscribe_e2e_seconds_count{application="hedera-mirror-monitor"}[2m]))
          by (namespace, pod, subscriber, scenario) > 0 or on() vector(0)) <= 0
        for: 2m
        labels:
          application: hedera-mirror-monitor
          severity: critical
      enabled: false
    rbac:
      enabled: true
    readinessProbe:
      failureThreshold: 5
      httpGet:
        path: /actuator/health/readiness
        port: http
      initialDelaySeconds: 60
      timeoutSeconds: 2
    replicas: 1
    resources:
      limits:
        cpu: 500m
        memory: 768Mi
      requests:
        cpu: 100m
        memory: 256Mi
    revisionHistoryLimit: 3
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true
    service:
      annotations: {}
      port: 80
      type: ClusterIP
    serviceAccount:
      create: true
    serviceMonitor:
      enabled: false
      interval: 30s
    terminationGracePeriodSeconds: 60
    tolerations: []
    updateStrategy:
      rollingUpdate:
        maxSurge: 10%
        maxUnavailable: 25%
      type: RollingUpdate
    volumeMounts:
      config:
        mountPath: /usr/etc/hedera
    volumes:
      config:
        secret:
          defaultMode: 420
          secretName: '{{ include "hedera-mirror-monitor.fullname" . }}'
  networkPolicy:
    enabled: false
  postgresql:
    clusterDomain: cluster.local
    common:
      exampleValue: common-chart
      global:
        hostname: ""
        image: {}
        imagePullSecrets: []
        imageRegistry: ""
        ldap:
          bindpw: ""
          existingSecret: ""
        namespaceOverride: ""
        pgpool:
          adminPassword: ""
          adminUsername: ""
          existingSecret: ""
        podAnnotations: {}
        postgresql:
          database: ""
          existingSecret: ""
          password: ""
          repmgrDatabase: ""
          repmgrPassword: ""
          repmgrUsername: ""
          username: ""
        storageClass: ""
        useReleaseForNameLabel: false
    commonAnnotations: {}
    commonLabels: {}
    diagnosticMode:
      args:
      - infinity
      command:
      - sleep
      enabled: false
    enabled: true
    extraDeploy: []
    fullnameOverride: ""
    global:
      hostname: ""
      image: {}
      imagePullSecrets: []
      imageRegistry: ""
      ldap:
        bindpw: ""
        existingSecret: ""
      namespaceOverride: ""
      pgpool:
        adminPassword: ""
        adminUsername: ""
        existingSecret: ""
      podAnnotations: {}
      postgresql:
        database: ""
        existingSecret: ""
        password: ""
        repmgrDatabase: ""
        repmgrPassword: ""
        repmgrUsername: ""
        username: ""
      storageClass: ""
      useReleaseForNameLabel: false
    kubeVersion: ""
    ldap:
      basedn: ""
      binddn: ""
      bindpw: ""
      bslookup: ""
      enabled: false
      existingSecret: ""
      nssInitgroupsIgnoreusers: root,nslcd
      scope: ""
      tlsReqcert: ""
      uri: ""
    metrics:
      annotations:
        prometheus.io/port: "9187"
        prometheus.io/scrape: "true"
      containerPorts:
        http: 9187
      customLivenessProbe: {}
      customMetrics: {}
      customReadinessProbe: {}
      customStartupProbe: {}
      enabled: false
      extraEnvVars: []
      extraEnvVarsCM: ""
      extraEnvVarsSecret: ""
      image:
        debug: false
        digest: ""
        pullPolicy: IfNotPresent
        pullSecrets: []
        registry: docker.io
        repository: bitnami/postgres-exporter
        tag: 0.11.1-debian-11-r22
      livenessProbe:
        enabled: true
        failureThreshold: 6
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      podSecurityContext:
        enabled: true
        runAsUser: 1001
      readinessProbe:
        enabled: true
        failureThreshold: 6
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: 50m
          memory: 50Mi
        requests:
          cpu: 20m
          memory: 25Mi
      service:
        clusterIP: ""
        externalTrafficPolicy: Cluster
        loadBalancerIP: ""
        loadBalancerSourceRanges: []
        nodePorts:
          metrics: ""
        ports:
          metrics: 9187
        type: ClusterIP
      serviceMonitor:
        annotations: {}
        enabled: false
        honorLabels: false
        interval: ""
        jobLabel: ""
        labels: {}
        metricRelabelings: []
        namespace: ""
        relabelings: []
        scrapeTimeout: ""
        selector:
          prometheus: kube-prometheus
      startupProbe:
        enabled: false
        failureThreshold: 10
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
    nameOverride: postgres
    namespaceOverride: ""
    networkPolicy:
      allowExternal: true
      egressRules:
        customRules: []
        denyConnectionsToExternal: false
      enabled: false
    persistence:
      accessModes:
      - ReadWriteOnce
      annotations: {}
      enabled: true
      existingClaim: ""
      labels: {}
      mountPath: /bitnami/postgresql
      selector: {}
      size: 500Gi
      storageClass: ""
    pgpool:
      adminPassword: ""
      adminUsername: admin
      affinity: {}
      args: []
      authenticationMethod: scram-sha-256
      childLifeTime: ""
      childMaxConnections: ""
      clientIdleLimit: ""
      clientMinMessages: error
      command: []
      configuration: ""
      configurationCM: ""
      connectionLifeTime: ""
      containerPorts:
        postgresql: 5432
      containerSecurityContext:
        enabled: true
        readOnlyRootFilesystem: false
        runAsNonRoot: true
        runAsUser: 1001
      customLivenessProbe: {}
      customReadinessProbe: {}
      customStartupProbe: {}
      customUsers: {}
      customUsersSecret: ""
      existingSecret: mirror-passwords
      extraEnvVars:
      - name: PGPOOL_POSTGRES_CUSTOM_PASSWORDS
        valueFrom:
          secretKeyRef:
            key: PGPOOL_POSTGRES_CUSTOM_PASSWORDS
            name: mirror-passwords
      - name: PGPOOL_POSTGRES_CUSTOM_USERS
        valueFrom:
          secretKeyRef:
            key: PGPOOL_POSTGRES_CUSTOM_USERS
            name: mirror-passwords
      extraEnvVarsCM: ""
      extraEnvVarsSecret: ""
      extraVolumeMounts: []
      extraVolumes: []
      hostAliases: []
      image:
        debug: true
        digest: ""
        pullPolicy: IfNotPresent
        pullSecrets: []
        registry: docker.io
        repository: bitnami/pgpool
        tag: 4.3.3-debian-11-r22
      initContainers: []
      initdbScripts: {}
      initdbScriptsCM: ""
      initdbScriptsSecret: ""
      labels: {}
      lifecycleHooks: {}
      livenessProbe:
        enabled: true
        failureThreshold: 5
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      loadBalancingOnWrite: transaction
      logConnections: false
      logHostname: true
      logLinePrefix: ""
      logPerNodeStatement: false
      maxPool: ""
      minReadySeconds: ""
      nodeAffinityPreset:
        key: ""
        type: ""
        values: []
      nodeSelector: {}
      numInitChildren: ""
      passwords: ""
      pdb:
        create: true
        maxUnavailable: ""
        minAvailable: 1
      podAffinityPreset: ""
      podAnnotations: {}
      podAntiAffinityPreset: soft
      podLabels:
        role: db
      podSecurityContext:
        enabled: true
        fsGroup: 1001
      priorityClassName: ""
      readinessProbe:
        enabled: true
        failureThreshold: 5
        initialDelaySeconds: 5
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 5
      replicaCount: 1
      reservedConnections: 1
      resources:
        limits:
          cpu: 300m
          memory: 750Mi
        requests:
          cpu: 150m
          memory: 256Mi
      schedulerName: ""
      serviceLabels: {}
      sidecars: []
      srCheckDatabase: postgres
      startupProbe:
        enabled: false
        failureThreshold: 10
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationGracePeriodSeconds: ""
      tls:
        autoGenerated: false
        certCAFilename: ""
        certFilename: ""
        certKeyFilename: ""
        certificatesSecret: ""
        enabled: false
        preferServerCiphers: true
      tolerations: []
      topologySpreadConstraints: []
      updateStrategy: {}
      useLoadBalancing: true
      usePasswordFile: ""
      usernames: ""
    postgresql:
      affinity: {}
      args: []
      audit:
        clientMinMessages: error
        logConnections: false
        logDisconnections: false
        logHostname: true
        logLinePrefix: ""
        logTimezone: ""
        pgAuditLog: ""
        pgAuditLogCatalog: "off"
      command: []
      configuration: ""
      configurationCM: ""
      containerPorts:
        postgresql: 5432
      containerSecurityContext:
        enabled: true
        readOnlyRootFilesystem: false
        runAsNonRoot: true
        runAsUser: 1001
      customLivenessProbe: {}
      customReadinessProbe: {}
      customStartupProbe: {}
      database: ""
      dbUserConnectionLimit: ""
      existingSecret: mirror-passwords
      extendedConf: ""
      extendedConfCM: ""
      extraEnvVars: []
      extraEnvVarsCM: ""
      extraEnvVarsSecret: mirror-passwords
      extraInitContainers: []
      extraVolumeMounts: []
      extraVolumes: []
      hostAliases: []
      hostIPC: false
      hostNetwork: false
      image:
        debug: true
        digest: ""
        pullPolicy: IfNotPresent
        pullSecrets: []
        registry: docker.io
        repository: bitnami/postgresql-repmgr
        tag: 14.5.0-debian-11-r31
      initContainers: []
      initdbScripts: {}
      initdbScriptsCM: ""
      initdbScriptsSecret: '{{ .Release.Name }}-init'
      labels: {}
      lifecycleHooks: {}
      livenessProbe:
        enabled: true
        failureThreshold: 6
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      maxConnections: ""
      nodeAffinityPreset:
        key: ""
        type: ""
        values: []
      nodeSelector: {}
      password: ""
      pdb:
        create: false
        maxUnavailable: ""
        minAvailable: 1
      pgHbaConfiguration: ""
      pgHbaTrustAll: false
      pghbaRemoveFilters: ""
      podAffinityPreset: ""
      podAnnotations: {}
      podAntiAffinityPreset: soft
      podLabels: {}
      podSecurityContext:
        enabled: true
        fsGroup: 1001
      postgresConnectionLimit: ""
      postgresPassword: ""
      priorityClassName: ""
      readinessProbe:
        enabled: true
        failureThreshold: 6
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      replicaCount: 1
      repmgrChildNodesCheckInterval: 5
      repmgrChildNodesConnectedMinCount: 1
      repmgrChildNodesDisconnectTimeout: 30
      repmgrConfiguration: ""
      repmgrConnectTimeout: 5
      repmgrDatabase: repmgr
      repmgrFenceOldPrimary: false
      repmgrLogLevel: DEBUG
      repmgrPassfilePath: ""
      repmgrPassword: ""
      repmgrReconnectAttempts: 2
      repmgrReconnectInterval: 3
      repmgrUsePassfile: ""
      repmgrUsername: repmgr
      resources:
        limits:
          cpu: 1500m
          memory: 1000Mi
        requests:
          cpu: 250m
          memory: 500Mi
      schedulerName: ""
      sharedPreloadLibraries: pgaudit, repmgr
      sidecars: []
      startupProbe:
        enabled: false
        failureThreshold: 10
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      statementTimeout: ""
      syncReplication: false
      tcpKeepalivesCount: ""
      tcpKeepalivesIdle: ""
      tcpKeepalivesInterval: ""
      terminationGracePeriodSeconds: ""
      tls:
        certFilename: ""
        certKeyFilename: ""
        certificatesSecret: ""
        enabled: false
        preferServerCiphers: true
      tolerations: []
      topologySpreadConstraints: []
      updateStrategy:
        type: RollingUpdate
      upgradeRepmgrExtension: false
      usePasswordFile: ""
      usePgRewind: false
      username: postgres
    psp:
      create: false
    rbac:
      create: false
      rules: []
    service:
      annotations: {}
      clusterIP: ""
      externalTrafficPolicy: Cluster
      extraPorts: []
      loadBalancerIP: ""
      loadBalancerSourceRanges: []
      nodePorts:
        postgresql: ""
      portName: postgresql
      ports:
        postgresql: 5432
      serviceLabels: {}
      sessionAffinity: None
      sessionAffinityConfig: {}
      type: ClusterIP
    serviceAccount:
      annotations: {}
      automountServiceAccountToken: true
      create: true
      name: ""
    volumePermissions:
      enabled: false
      image:
        digest: ""
        pullPolicy: IfNotPresent
        pullSecrets: []
        registry: docker.io
        repository: bitnami/bitnami-shell
        tag: 11-debian-11-r45
      podSecurityContext:
        runAsUser: 0
      resources:
        limits: {}
        requests: {}
  postgresql-ha:
    clusterDomain: cluster.local
    common:
      exampleValue: common-chart
      global:
        hostname: ""
        image: {}
        imagePullSecrets: []
        imageRegistry: ""
        ldap:
          bindpw: ""
          existingSecret: ""
        namespaceOverride: ""
        pgpool:
          adminPassword: ""
          adminUsername: ""
          existingSecret: ""
        podAnnotations: {}
        postgresql:
          database: ""
          existingSecret: ""
          password: ""
          repmgrDatabase: ""
          repmgrPassword: ""
          repmgrUsername: ""
          username: ""
        storageClass: ""
        useReleaseForNameLabel: false
    commonAnnotations: {}
    commonLabels: {}
    diagnosticMode:
      args:
      - infinity
      command:
      - sleep
      enabled: false
    extraDeploy: []
    fullnameOverride: ""
    global:
      hostname: ""
      image: {}
      imagePullSecrets: []
      imageRegistry: ""
      ldap:
        bindpw: ""
        existingSecret: ""
      namespaceOverride: ""
      pgpool:
        adminPassword: ""
        adminUsername: ""
        existingSecret: ""
      podAnnotations: {}
      postgresql:
        database: ""
        existingSecret: ""
        password: ""
        repmgrDatabase: ""
        repmgrPassword: ""
        repmgrUsername: ""
        username: ""
      storageClass: ""
      useReleaseForNameLabel: false
    kubeVersion: ""
    ldap:
      basedn: ""
      binddn: ""
      bindpw: ""
      bslookup: ""
      enabled: false
      existingSecret: ""
      nssInitgroupsIgnoreusers: root,nslcd
      scope: ""
      tlsReqcert: ""
      uri: ""
    metrics:
      annotations:
        prometheus.io/port: "9187"
        prometheus.io/scrape: "true"
      containerPorts:
        http: 9187
      customLivenessProbe: {}
      customMetrics: {}
      customReadinessProbe: {}
      customStartupProbe: {}
      enabled: false
      extraEnvVars: []
      extraEnvVarsCM: ""
      extraEnvVarsSecret: ""
      image:
        debug: false
        digest: ""
        pullPolicy: IfNotPresent
        pullSecrets: []
        registry: docker.io
        repository: bitnami/postgres-exporter
        tag: 0.11.1-debian-11-r22
      livenessProbe:
        enabled: true
        failureThreshold: 6
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      podSecurityContext:
        enabled: true
        runAsUser: 1001
      readinessProbe:
        enabled: true
        failureThreshold: 6
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits: {}
        requests: {}
      service:
        clusterIP: ""
        externalTrafficPolicy: Cluster
        loadBalancerIP: ""
        loadBalancerSourceRanges: []
        nodePorts:
          metrics: ""
        ports:
          metrics: 9187
        type: ClusterIP
      serviceMonitor:
        annotations: {}
        enabled: false
        honorLabels: false
        interval: ""
        jobLabel: ""
        labels: {}
        metricRelabelings: []
        namespace: ""
        relabelings: []
        scrapeTimeout: ""
        selector:
          prometheus: kube-prometheus
      startupProbe:
        enabled: false
        failureThreshold: 10
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
    nameOverride: ""
    namespaceOverride: ""
    networkPolicy:
      allowExternal: true
      egressRules:
        customRules: []
        denyConnectionsToExternal: false
      enabled: false
    persistence:
      accessModes:
      - ReadWriteOnce
      annotations: {}
      enabled: true
      existingClaim: ""
      labels: {}
      mountPath: /bitnami/postgresql
      selector: {}
      size: 8Gi
      storageClass: ""
    pgpool:
      adminPassword: ""
      adminUsername: admin
      affinity: {}
      args: []
      authenticationMethod: scram-sha-256
      childLifeTime: ""
      childMaxConnections: ""
      clientIdleLimit: ""
      clientMinMessages: error
      command: []
      configuration: ""
      configurationCM: ""
      connectionLifeTime: ""
      containerPorts:
        postgresql: 5432
      containerSecurityContext:
        enabled: true
        readOnlyRootFilesystem: false
        runAsNonRoot: true
        runAsUser: 1001
      customLivenessProbe: {}
      customReadinessProbe: {}
      customStartupProbe: {}
      customUsers: {}
      customUsersSecret: ""
      existingSecret: ""
      extraEnvVars: []
      extraEnvVarsCM: ""
      extraEnvVarsSecret: ""
      extraVolumeMounts: []
      extraVolumes: []
      hostAliases: []
      image:
        debug: false
        digest: ""
        pullPolicy: IfNotPresent
        pullSecrets: []
        registry: docker.io
        repository: bitnami/pgpool
        tag: 4.3.3-debian-11-r22
      initContainers: []
      initdbScripts: {}
      initdbScriptsCM: ""
      initdbScriptsSecret: ""
      labels: {}
      lifecycleHooks: {}
      livenessProbe:
        enabled: true
        failureThreshold: 5
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      loadBalancingOnWrite: transaction
      logConnections: false
      logHostname: true
      logLinePrefix: ""
      logPerNodeStatement: false
      maxPool: ""
      minReadySeconds: ""
      nodeAffinityPreset:
        key: ""
        type: ""
        values: []
      nodeSelector: {}
      numInitChildren: ""
      passwords: ""
      pdb:
        create: false
        maxUnavailable: ""
        minAvailable: 1
      podAffinityPreset: ""
      podAnnotations: {}
      podAntiAffinityPreset: soft
      podLabels: {}
      podSecurityContext:
        enabled: true
        fsGroup: 1001
      priorityClassName: ""
      readinessProbe:
        enabled: true
        failureThreshold: 5
        initialDelaySeconds: 5
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 5
      replicaCount: 1
      reservedConnections: 1
      resources:
        limits: {}
        requests: {}
      schedulerName: ""
      serviceLabels: {}
      sidecars: []
      srCheckDatabase: postgres
      startupProbe:
        enabled: false
        failureThreshold: 10
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationGracePeriodSeconds: ""
      tls:
        autoGenerated: false
        certCAFilename: ""
        certFilename: ""
        certKeyFilename: ""
        certificatesSecret: ""
        enabled: false
        preferServerCiphers: true
      tolerations: []
      topologySpreadConstraints: []
      updateStrategy: {}
      useLoadBalancing: true
      usePasswordFile: ""
      usernames: ""
    postgresql:
      affinity: {}
      args: []
      audit:
        clientMinMessages: error
        logConnections: false
        logDisconnections: false
        logHostname: true
        logLinePrefix: ""
        logTimezone: ""
        pgAuditLog: ""
        pgAuditLogCatalog: "off"
      command: []
      configuration: ""
      configurationCM: ""
      containerPorts:
        postgresql: 5432
      containerSecurityContext:
        enabled: true
        readOnlyRootFilesystem: false
        runAsNonRoot: true
        runAsUser: 1001
      customLivenessProbe: {}
      customReadinessProbe: {}
      customStartupProbe: {}
      database: ""
      dbUserConnectionLimit: ""
      existingSecret: ""
      extendedConf: ""
      extendedConfCM: ""
      extraEnvVars: []
      extraEnvVarsCM: ""
      extraEnvVarsSecret: ""
      extraInitContainers: []
      extraVolumeMounts: []
      extraVolumes: []
      hostAliases: []
      hostIPC: false
      hostNetwork: false
      image:
        debug: false
        digest: ""
        pullPolicy: IfNotPresent
        pullSecrets: []
        registry: docker.io
        repository: bitnami/postgresql-repmgr
        tag: 14.5.0-debian-11-r31
      initContainers: []
      initdbScripts: {}
      initdbScriptsCM: ""
      initdbScriptsSecret: ""
      labels: {}
      lifecycleHooks: {}
      livenessProbe:
        enabled: true
        failureThreshold: 6
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      maxConnections: ""
      nodeAffinityPreset:
        key: ""
        type: ""
        values: []
      nodeSelector: {}
      password: ""
      pdb:
        create: false
        maxUnavailable: ""
        minAvailable: 1
      pgHbaConfiguration: ""
      pgHbaTrustAll: false
      pghbaRemoveFilters: ""
      podAffinityPreset: ""
      podAnnotations: {}
      podAntiAffinityPreset: soft
      podLabels: {}
      podSecurityContext:
        enabled: true
        fsGroup: 1001
      postgresConnectionLimit: ""
      postgresPassword: ""
      priorityClassName: ""
      readinessProbe:
        enabled: true
        failureThreshold: 6
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      replicaCount: 3
      repmgrChildNodesCheckInterval: 5
      repmgrChildNodesConnectedMinCount: 1
      repmgrChildNodesDisconnectTimeout: 30
      repmgrConfiguration: ""
      repmgrConnectTimeout: 5
      repmgrDatabase: repmgr
      repmgrFenceOldPrimary: false
      repmgrLogLevel: NOTICE
      repmgrPassfilePath: ""
      repmgrPassword: ""
      repmgrReconnectAttempts: 2
      repmgrReconnectInterval: 3
      repmgrUsePassfile: ""
      repmgrUsername: repmgr
      resources:
        limits: {}
        requests: {}
      schedulerName: ""
      sharedPreloadLibraries: pgaudit, repmgr
      sidecars: []
      startupProbe:
        enabled: false
        failureThreshold: 10
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      statementTimeout: ""
      syncReplication: false
      tcpKeepalivesCount: ""
      tcpKeepalivesIdle: ""
      tcpKeepalivesInterval: ""
      terminationGracePeriodSeconds: ""
      tls:
        certFilename: ""
        certKeyFilename: ""
        certificatesSecret: ""
        enabled: false
        preferServerCiphers: true
      tolerations: []
      topologySpreadConstraints: []
      updateStrategy:
        type: RollingUpdate
      upgradeRepmgrExtension: false
      usePasswordFile: ""
      usePgRewind: false
      username: postgres
    psp:
      create: false
    rbac:
      create: false
      rules: []
    service:
      annotations: {}
      clusterIP: ""
      externalTrafficPolicy: Cluster
      extraPorts: []
      loadBalancerIP: ""
      loadBalancerSourceRanges: []
      nodePorts:
        postgresql: ""
      portName: postgresql
      ports:
        postgresql: 5432
      serviceLabels: {}
      sessionAffinity: None
      sessionAffinityConfig: {}
      type: ClusterIP
    serviceAccount:
      annotations: {}
      automountServiceAccountToken: true
      create: false
      name: ""
    volumePermissions:
      enabled: false
      image:
        digest: ""
        pullPolicy: IfNotPresent
        pullSecrets: []
        registry: docker.io
        repository: bitnami/bitnami-shell
        tag: 11-debian-11-r45
      podSecurityContext:
        runAsUser: 0
      resources:
        limits: {}
        requests: {}
  redis:
    architecture: replication
    auth:
      enabled: true
      existingSecret: mirror-redis
      existingSecretPasswordKey: SPRING_REDIS_PASSWORD
      password: ""
      sentinel: true
      usePasswordFiles: false
    clusterDomain: cluster.local
    common:
      exampleValue: common-chart
      global:
        hostname: ""
        image: {}
        imagePullSecrets: []
        imageRegistry: ""
        namespaceOverride: ""
        podAnnotations: {}
        redis:
          password: ""
        storageClass: ""
        useReleaseForNameLabel: false
    commonAnnotations: {}
    commonConfiguration: |-
      # Enable AOF https://redis.io/topics/persistence#append-only-file
      appendonly yes
      # Disable RDB persistence, AOF persistence already enabled.
      save ""
    commonLabels: {}
    diagnosticMode:
      args:
      - infinity
      command:
      - sleep
      enabled: false
    enabled: true
    existingConfigmap: ""
    extraDeploy: []
    fullnameOverride: ""
    global:
      hostname: ""
      image: {}
      imagePullSecrets: []
      imageRegistry: ""
      namespaceOverride: ""
      podAnnotations: {}
      redis:
        password: ""
      storageClass: ""
      useReleaseForNameLabel: false
    host: '{{ .Release.Name }}-redis'
    image:
      debug: false
      digest: ""
      pullPolicy: IfNotPresent
      pullSecrets: []
      registry: docker.io
      repository: bitnami/redis
      tag: 7.0.5-debian-11-r7
    kubeVersion: ""
    master:
      affinity: {}
      args: []
      command: []
      configuration: ""
      containerPorts:
        redis: 6379
      containerSecurityContext:
        enabled: true
        runAsUser: 1001
      count: 1
      customLivenessProbe: {}
      customReadinessProbe: {}
      customStartupProbe: {}
      disableCommands:
      - FLUSHDB
      - FLUSHALL
      dnsConfig: {}
      dnsPolicy: ""
      extraEnvVars: []
      extraEnvVarsCM: ""
      extraEnvVarsSecret: ""
      extraFlags: []
      extraVolumeMounts: []
      extraVolumes: []
      hostAliases: []
      initContainers: []
      kind: StatefulSet
      lifecycleHooks: {}
      livenessProbe:
        enabled: true
        failureThreshold: 5
        initialDelaySeconds: 20
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 5
      nodeAffinityPreset:
        key: ""
        type: ""
        values: []
      nodeSelector: {}
      persistence:
        accessModes:
        - ReadWriteOnce
        annotations: {}
        dataSource: {}
        enabled: true
        existingClaim: ""
        medium: ""
        path: /data
        selector: {}
        size: 8Gi
        sizeLimit: ""
        storageClass: ""
        subPath: ""
      podAffinityPreset: ""
      podAnnotations: {}
      podAntiAffinityPreset: soft
      podLabels: {}
      podSecurityContext:
        enabled: true
        fsGroup: 1001
      preExecCmds: []
      priorityClassName: ""
      readinessProbe:
        enabled: true
        failureThreshold: 5
        initialDelaySeconds: 20
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits: {}
        requests: {}
      schedulerName: ""
      service:
        annotations: {}
        clusterIP: ""
        externalTrafficPolicy: Cluster
        extraPorts: []
        internalTrafficPolicy: Cluster
        loadBalancerIP: ""
        loadBalancerSourceRanges: []
        nodePorts:
          redis: ""
        ports:
          redis: 6379
        sessionAffinity: None
        sessionAffinityConfig: {}
        type: ClusterIP
      serviceAccount:
        annotations: {}
        automountServiceAccountToken: true
        create: false
        name: ""
      shareProcessNamespace: false
      sidecars: []
      startupProbe:
        enabled: false
        failureThreshold: 5
        initialDelaySeconds: 20
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 5
      terminationGracePeriodSeconds: 30
      tolerations: []
      topologySpreadConstraints: []
      updateStrategy:
        rollingUpdate: {}
        type: RollingUpdate
    metrics:
      command: []
      containerSecurityContext:
        enabled: true
        runAsUser: 1001
      enabled: false
      extraArgs: {}
      extraEnvVars: []
      extraVolumeMounts: []
      extraVolumes: []
      image:
        digest: ""
        pullPolicy: IfNotPresent
        pullSecrets: []
        registry: docker.io
        repository: bitnami/redis-exporter
        tag: 1.44.0-debian-11-r16
      podAnnotations:
        prometheus.io/port: "9121"
        prometheus.io/scrape: "true"
      podLabels: {}
      prometheusRule:
        additionalLabels: {}
        enabled: false
        namespace: ""
        rules: []
      redisTargetHost: localhost
      resources:
        limits:
          cpu: 100m
          memory: 50Mi
        requests:
          cpu: 50m
          memory: 25Mi
      service:
        annotations: {}
        externalTrafficPolicy: Cluster
        extraPorts: []
        loadBalancerIP: ""
        loadBalancerSourceRanges: []
        port: 9121
        type: ClusterIP
      serviceMonitor:
        additionalLabels: {}
        enabled: true
        honorLabels: false
        interval: 30s
        metricRelabelings: []
        namespace: ""
        podTargetLabels: []
        relabellings: []
        scrapeTimeout: ""
    nameOverride: ""
    networkPolicy:
      allowExternal: true
      enabled: false
      extraEgress: []
      extraIngress: []
      ingressNSMatchLabels: {}
      ingressNSPodMatchLabels: {}
    pdb:
      create: true
      maxUnavailable: ""
      minAvailable: 1
    podSecurityPolicy:
      create: false
      enabled: false
    rbac:
      create: true
      rules: []
    replica:
      affinity: {}
      args: []
      autoscaling:
        enabled: false
        maxReplicas: 11
        minReplicas: 1
        targetCPU: ""
        targetMemory: ""
      command: []
      configuration: ""
      containerPorts:
        redis: 6379
      containerSecurityContext:
        enabled: true
        runAsUser: 1001
      customLivenessProbe: {}
      customReadinessProbe: {}
      customStartupProbe: {}
      disableCommands:
      - FLUSHDB
      - FLUSHALL
      dnsConfig: {}
      dnsPolicy: ""
      externalMaster:
        enabled: false
        host: ""
        port: 6379
      extraEnvVars: []
      extraEnvVarsCM: ""
      extraEnvVarsSecret: ""
      extraFlags: []
      extraVolumeMounts: []
      extraVolumes: []
      hostAliases: []
      initContainers: []
      lifecycleHooks: {}
      livenessProbe:
        enabled: true
        failureThreshold: 5
        initialDelaySeconds: 20
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 5
      nodeAffinityPreset:
        key: ""
        type: ""
        values: []
      nodeSelector: {}
      persistence:
        accessModes:
        - ReadWriteOnce
        annotations: {}
        dataSource: {}
        enabled: true
        existingClaim: ""
        medium: ""
        path: /data
        selector: {}
        size: 8Gi
        sizeLimit: ""
        storageClass: ""
        subPath: ""
      podAffinityPreset: ""
      podAnnotations: {}
      podAntiAffinityPreset: soft
      podLabels: {}
      podManagementPolicy: ""
      podSecurityContext:
        enabled: true
        fsGroup: 1001
        runAsGroup: 1001
        runAsUser: 1001
      preExecCmds: []
      priorityClassName: ""
      readinessProbe:
        enabled: true
        failureThreshold: 5
        initialDelaySeconds: 20
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      replicaCount: 1
      resources:
        limits:
          cpu: 1500m
          memory: 1000Mi
        requests:
          cpu: 250m
          memory: 500Mi
      schedulerName: ""
      service:
        annotations: {}
        clusterIP: ""
        externalTrafficPolicy: Cluster
        extraPorts: []
        internalTrafficPolicy: Cluster
        loadBalancerIP: ""
        loadBalancerSourceRanges: []
        nodePorts:
          redis: ""
        ports:
          redis: 6379
        sessionAffinity: None
        sessionAffinityConfig: {}
        type: ClusterIP
      serviceAccount:
        annotations: {}
        automountServiceAccountToken: true
        create: false
        name: ""
      shareProcessNamespace: false
      sidecars: []
      startupProbe:
        enabled: true
        failureThreshold: 22
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationGracePeriodSeconds: 30
      tolerations: []
      topologySpreadConstraints: []
      updateStrategy:
        rollingUpdate: {}
        type: RollingUpdate
    secretAnnotations: {}
    sentinel:
      args: []
      automateClusterRecovery: false
      command: []
      configuration: ""
      containerPorts:
        sentinel: 26379
      containerSecurityContext:
        enabled: true
        runAsUser: 1001
      customLivenessProbe: {}
      customReadinessProbe: {}
      customStartupProbe: {}
      downAfterMilliseconds: 60000
      enabled: true
      externalMaster:
        enabled: false
        host: ""
        port: 6379
      extraEnvVars: []
      extraEnvVarsCM: ""
      extraEnvVarsSecret: ""
      extraVolumeMounts: []
      extraVolumes: []
      failoverTimeout: 180000
      getMasterTimeout: 220
      image:
        debug: false
        digest: ""
        pullPolicy: IfNotPresent
        pullSecrets: []
        registry: docker.io
        repository: bitnami/redis-sentinel
        tag: 7.0.5-debian-11-r6
      lifecycleHooks: {}
      livenessProbe:
        enabled: true
        failureThreshold: 5
        initialDelaySeconds: 20
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 5
      masterSet: mirror
      parallelSyncs: 1
      persistence:
        accessModes:
        - ReadWriteOnce
        annotations: {}
        dataSource: {}
        enabled: true
        medium: ""
        selector: {}
        size: 100Mi
        storageClass: ""
      preExecCmds: []
      quorum: 2
      readinessProbe:
        enabled: true
        failureThreshold: 5
        initialDelaySeconds: 20
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 150m
          memory: 256Mi
        requests:
          cpu: 75m
          memory: 75Mi
      service:
        annotations: {}
        clusterIP: ""
        externalTrafficPolicy: Cluster
        extraPorts: []
        loadBalancerIP: ""
        loadBalancerSourceRanges: []
        nodePorts:
          redis: ""
          sentinel: ""
        ports:
          redis: 6379
          sentinel: 26379
        sessionAffinity: None
        sessionAffinityConfig: {}
        type: ClusterIP
      startupProbe:
        enabled: true
        failureThreshold: 22
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationGracePeriodSeconds: 30
    serviceAccount:
      annotations: {}
      automountServiceAccountToken: true
      create: true
      name: ""
    sysctl:
      command: []
      enabled: false
      image:
        digest: ""
        pullPolicy: IfNotPresent
        pullSecrets: []
        registry: docker.io
        repository: bitnami/bitnami-shell
        tag: 11-debian-11-r40
      mountHostSys: false
      resources:
        limits: {}
        requests: {}
    tls:
      authClients: true
      autoGenerated: false
      certCAFilename: ""
      certFilename: ""
      certKeyFilename: ""
      certificatesSecret: ""
      dhParamsFilename: ""
      enabled: false
      existingSecret: ""
    useExternalDNS:
      additionalAnnotations: {}
      annotationKey: external-dns.alpha.kubernetes.io/
      enabled: false
      suffix: ""
    volumePermissions:
      containerSecurityContext:
        runAsUser: 0
      enabled: false
      image:
        digest: ""
        pullPolicy: IfNotPresent
        pullSecrets: []
        registry: docker.io
        repository: bitnami/bitnami-shell
        tag: 11-debian-11-r40
      resources:
        limits: {}
        requests: {}
  rest:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/component: rest
            topologyKey: kubernetes.io/hostname
          weight: 100
    alertmanager:
      inhibitRules:
        InhibitAllWhenPodIssues:
          enabled: true
          matches:
          - equal:
            - namespace
            - pod
            sourceMatch:
            - name: area
              value: resource
            targetMatch:
            - name: application
              value: hedera-mirror-rest
        enabled: false
    annotations: {}
    config:
      hedera:
        mirror:
          rest:
            metrics:
              config:
                authentication: false
    db:
      password: ""
      username: mirror_rest
    enabled: true
    env:
      CONFIG_PATH: /usr/etc/hedera/
      HEDERA_MIRROR_REST_DB_HOST:
        valueFrom:
          secretKeyRef:
            key: HEDERA_MIRROR_IMPORTER_DB_HOST
            name: mirror-passwords
      HEDERA_MIRROR_REST_DB_NAME:
        valueFrom:
          secretKeyRef:
            key: HEDERA_MIRROR_IMPORTER_DB_NAME
            name: mirror-passwords
      HEDERA_MIRROR_REST_DB_PASSWORD:
        valueFrom:
          secretKeyRef:
            key: HEDERA_MIRROR_IMPORTER_DB_RESTPASSWORD
            name: mirror-passwords
      HEDERA_MIRROR_REST_DB_USERNAME:
        valueFrom:
          secretKeyRef:
            key: HEDERA_MIRROR_IMPORTER_DB_RESTUSERNAME
            name: mirror-passwords
    envFrom: []
    fullnameOverride: mirror-node-rest
    global:
      hostname: ""
      image: {}
      middleware: false
      namespaceOverride: ""
      podAnnotations: {}
      useReleaseForNameLabel: false
    hpa:
      behavior: {}
      enabled: true
      maxReplicas: 15
      metrics:
      - resource:
          name: cpu
          target:
            averageUtilization: 80
            type: Utilization
        type: Resource
      minReplicas: 1
    image:
      pullPolicy: IfNotPresent
      registry: docker.io/
      repository: cabob/hedera-mirror-rest
      tag: latest
    imagePullSecrets: []
    ingress:
      annotations:
        traefik.ingress.kubernetes.io/router.middlewares: '{{ include "hedera-mirror-rest.namespace"
          . }}-{{ include "hedera-mirror-rest.fullname" . }}@kubernetescrd'
      enabled: true
      hosts:
      - host: ""
        paths:
        - /api/v1
      tls:
        enabled: false
        secretName: ""
    labels: {}
    livenessProbe:
      httpGet:
        path: /health/liveness
        port: http
      initialDelaySeconds: 25
      timeoutSeconds: 2
    middleware:
    - circuitBreaker:
        expression: NetworkErrorRatio() > 0.25 || ResponseCodeRatio(500, 600, 0, 600)
          > 0.25
    - retry:
        attempts: 10
        initialInterval: 100ms
    nodeSelector: {}
    podAnnotations: {}
    podDisruptionBudget:
      enabled: false
      minAvailable: 50%
    podSecurityContext:
      fsGroup: 1000
      runAsGroup: 1000
      runAsNonRoot: true
      runAsUser: 1000
      seccompProfile:
        type: RuntimeDefault
    priorityClassName: ""
    prometheusRules:
      RestErrors:
        annotations:
          description: REST API 5xx error rate for {{ $labels.namespace }}/{{ $labels.pod
            }} is {{ $value | humanizePercentage }}
          summary: Mirror REST API error rate exceeds 5%
        enabled: true
        expr: sum(rate(api_request_total{container="rest",code=~"^5.."}[5m])) by (namespace,
          pod) / sum(rate(api_request_total{container="rest"}[5m])) by (namespace,
          pod) > 0.05
        for: 1m
        labels:
          application: hedera-mirror-rest
          severity: critical
      RestHighCPU:
        annotations:
          description: '{{ $labels.namespace }}/{{ $labels.pod }} CPU usage reached
            {{ $value | humanizePercentage }}'
          summary: Mirror REST API CPU usage exceeds 80%
        enabled: true
        expr: sum(nodejs_process_cpu_usage_percentage{container="rest"}) by (namespace,
          pod) > 80
        for: 5m
        labels:
          application: hedera-mirror-rest
          area: resource
          severity: critical
      RestNoPodsReady:
        annotations:
          description: No REST API instances are currently running in {{ $labels.namespace
            }}
          summary: No REST API instances running
        enabled: true
        expr: sum(kube_pod_status_ready{pod=~".*-rest-.*",condition="true"}) by (namespace)
          < 1
        for: 2m
        labels:
          application: hedera-mirror-rest
          area: resource
          severity: critical
      RestNoRequests:
        annotations:
          description: REST API has not seen any requests to {{ $labels.namespace
            }} for 5m
          summary: No Mirror REST API requests seen for awhile
        enabled: true
        expr: sum(rate(api_all_request_total{container="rest"}[3m])) by (namespace)
          <= 0
        for: 5m
        labels:
          application: hedera-mirror-rest
          severity: warning
      RestRequestLatency:
        annotations:
          description: '{{ $labels.namespace }}/{{ $labels.pod }} is taking {{ $value
            | humanizeDuration }} to generate a response'
          summary: Mirror REST API request latency exceeds 2s
        enabled: true
        expr: sum(rate(api_request_duration_milliseconds_sum{container="rest"}[5m]))
          by (namespace, pod) / sum(rate(api_request_duration_milliseconds_count{container="rest"}[5m]))
          by (namespace, pod) > 2000
        for: 1m
        labels:
          application: hedera-mirror-rest
          severity: warning
      enabled: false
    readinessProbe:
      httpGet:
        path: /health/readiness
        port: http
      initialDelaySeconds: 30
      timeoutSeconds: 2
    resources:
      limits:
        cpu: 750m
        memory: 350Mi
      requests:
        cpu: 300m
        memory: 64Mi
    revisionHistoryLimit: 3
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true
    service:
      annotations: {}
      port: 80
      type: ClusterIP
    serviceAccount:
      create: true
    serviceMonitor:
      enabled: false
      interval: 30s
    terminationGracePeriodSeconds: 60
    test:
      checks:
        accounts: true
        transactions: true
      enabled: true
      image:
        pullPolicy: IfNotPresent
        repository: bats/bats
        tag: 1.7.0
    tolerations: []
    updateStrategy:
      rollingUpdate:
        maxSurge: 10%
        maxUnavailable: 25%
      type: RollingUpdate
    volumeMounts:
      config:
        mountPath: /usr/etc/hedera
    volumes:
      config:
        secret:
          defaultMode: 420
          secretName: '{{ include "hedera-mirror-rest.fullname" . }}'
  rosetta:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/component: rosetta
            topologyKey: kubernetes.io/hostname
          weight: 100
    alertmanager:
      inhibitRules:
        RosettaInhibitAll:
          enabled: true
          matches:
          - equal:
            - namespace
            - pod
            sourceMatch:
            - name: area
              value: resource
            targetMatch:
            - name: application
              value: hedera-mirror-rosetta
        enabled: false
    annotations: {}
    config: {}
    db:
      password: ""
      username: mirror_rosetta
    enabled: true
    env:
      CONFIG_PATH: /usr/etc/hedera/
      HEDERA_MIRROR_ROSETTA_DB_HOST:
        valueFrom:
          secretKeyRef:
            key: HEDERA_MIRROR_IMPORTER_DB_HOST
            name: mirror-passwords
      HEDERA_MIRROR_ROSETTA_DB_NAME:
        valueFrom:
          secretKeyRef:
            key: HEDERA_MIRROR_IMPORTER_DB_NAME
            name: mirror-passwords
      HEDERA_MIRROR_ROSETTA_DB_PASSWORD:
        valueFrom:
          secretKeyRef:
            key: HEDERA_MIRROR_ROSETTA_DB_PASSWORD
            name: mirror-passwords
      HEDERA_MIRROR_ROSETTA_DB_USERNAME:
        valueFrom:
          secretKeyRef:
            key: HEDERA_MIRROR_ROSETTA_DB_USERNAME
            name: mirror-passwords
    envFrom: []
    fullnameOverride: ""
    global:
      hostname: ""
      image: {}
      middleware: false
      namespaceOverride: ""
      podAnnotations: {}
      useReleaseForNameLabel: false
    hpa:
      behavior: {}
      enabled: true
      maxReplicas: 10
      metrics:
      - resource:
          name: cpu
          target:
            averageUtilization: 80
            type: Utilization
        type: Resource
      minReplicas: 1
    image:
      pullPolicy: IfNotPresent
      registry: gcr.io
      repository: mirrornode/hedera-mirror-rosetta
      tag: ""
    imagePullSecrets: []
    ingress:
      annotations:
        traefik.ingress.kubernetes.io/router.middlewares: '{{ include "hedera-mirror-rosetta.namespace"
          . }}-{{ include "hedera-mirror-rosetta.fullname" . }}@kubernetescrd'
      enabled: true
      hosts:
      - host: ""
        paths:
        - /rosetta/account
        - /rosetta/block
        - /rosetta/call
        - /rosetta/construction
        - /rosetta/events
        - /rosetta/mempool
        - /rosetta/network
        - /rosetta/search
      tls:
        enabled: false
        secretName: ""
    labels: {}
    livenessProbe:
      failureThreshold: 5
      httpGet:
        path: /health/liveness
        port: http
      initialDelaySeconds: 30
      periodSeconds: 10
      timeoutSeconds: 2
    middleware:
    - circuitBreaker:
        expression: NetworkErrorRatio() > 0.25 || ResponseCodeRatio(500, 600, 0, 600)
          > 0.25
    - inFlightReq:
        amount: 5
        sourceCriterion:
          ipStrategy:
            depth: 1
    - rateLimit:
        average: 10
        sourceCriterion:
          requestHost: true
    - retry:
        attempts: 3
        initialInterval: 100ms
    - stripPrefix:
        prefixes:
        - /rosetta
    nodeSelector: {}
    podAnnotations: {}
    podDisruptionBudget:
      enabled: false
      minAvailable: 50%
    podSecurityContext:
      fsGroup: 1000
      runAsGroup: 1000
      runAsNonRoot: true
      runAsUser: 1000
      seccompProfile:
        type: RuntimeDefault
    priorityClassName: ""
    prometheusRules:
      RosettaApiErrors:
        annotations:
          description: Rosetta API 5xx error rate for {{ $labels.namespace }}/{{ $labels.pod
            }} is {{ $value | humanizePercentage }}
          summary: Mirror Rosetta API error rate exceeds 5%
        enabled: true
        expr: sum(rate(hedera_mirror_rosetta_request_duration_count{application="hedera-mirror-rosetta",status_code=~"^5.."}[5m]))
          by (namespace, pod) / sum(rate(hedera_mirror_rosetta_request_duration_count{application="hedera-mirror-rosetta"}[5m]))
          by (namespace, pod) > 0.05
        for: 1m
        labels:
          application: hedera-mirror-rosetta
          severity: critical
      RosettaNoPodsReady:
        annotations:
          description: No Rosetta API instances are currently running in {{ $labels.namespace
            }}
          summary: No Rosetta API instances running
        enabled: true
        expr: sum(kube_pod_status_ready{pod=~".*-rosetta-.*",condition="true"}) by
          (namespace) < 1
        for: 2m
        labels:
          application: hedera-mirror-rosetta
          area: resource
          severity: critical
      RosettaNoRequests:
        annotations:
          description: Rosetta API has not seen any requests to {{ $labels.namespace
            }} for 5m
          summary: No Mirror Rosetta API requests seen for awhile
        enabled: false
        expr: sum(rate(hedera_mirror_rosetta_request_duration_count{application="hedera-mirror-rosetta"}[3m]))
          by (namespace) <= 0
        for: 5m
        labels:
          application: hedera-mirror-rosetta
          severity: warning
      RosettaSlowResponse:
        annotations:
          description: '{{ $labels.namespace }}/{{ $labels.pod }} is taking {{ $value
            | humanizeDuration }} to generate a response'
          summary: Mirror Rosetta API is taking too long to respond
        enabled: true
        expr: sum(rate(hedera_mirror_rosetta_request_duration_sum{application="hedera-mirror-rosetta"}[5m]))
          by (namespace, pod) / sum(rate(hedera_mirror_rosetta_request_duration_count{application="hedera-mirror-rosetta"}[5m]))
          by (namespace, pod) > 2000
        for: 1m
        labels:
          application: hedera-mirror-rosetta
          severity: warning
      enabled: false
    readinessProbe:
      failureThreshold: 5
      httpGet:
        path: /health/readiness
        port: http
      initialDelaySeconds: 30
      timeoutSeconds: 2
    resources:
      limits:
        cpu: 500m
        memory: 200Mi
      requests:
        cpu: 50m
        memory: 64Mi
    revisionHistoryLimit: 3
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true
    service:
      annotations: {}
      port: 80
      type: ClusterIP
    serviceAccount:
      create: true
    serviceMonitor:
      enabled: false
      interval: 30s
    terminationGracePeriodSeconds: 60
    test:
      enabled: true
      git:
        branch: ""
        repository: hashgraph/hedera-mirror-node
      image:
        pullPolicy: IfNotPresent
        repository: postman/newman
        tag: 5.3.1-alpine
    tolerations: []
    updateStrategy:
      rollingUpdate:
        maxSurge: 10%
        maxUnavailable: 25%
      type: RollingUpdate
    volumeMounts:
      config:
        mountPath: /usr/etc/hedera
    volumes:
      config:
        secret:
          defaultMode: 420
          secretName: '{{ include "hedera-mirror-rosetta.fullname" . }}'
  test:
    config:
      hedera:
        mirror:
          test:
            acceptance:
              mirrorNodeAddress: '{{ .Release.Name }}-grpc:5600'
              rest:
                baseUrl: http://{{ .Release.Name }}-rest/api/v1
    cucumberTags: '@acceptance'
    enabled: false
    git:
      branch: ""
      repository: hashgraph/hedera-mirror-node
    image:
      pullPolicy: IfNotPresent
      repository: alpine
      tag: latest
  web3:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/component: web3
            topologyKey: kubernetes.io/hostname
          weight: 100
    alertmanager:
      inhibitRules:
        InhibitAllWhenPodIssues:
          enabled: true
          matches:
          - equal:
            - namespace
            - pod
            sourceMatch:
            - name: area
              value: resource
            targetMatch:
            - name: application
              value: hedera-mirror-web3
        enabled: false
    annotations: {}
    config: {}
    db:
      password: ""
      username: mirror_web3
    enabled: true
    env:
      HEDERA_MIRROR_WEB3_DB_HOST:
        valueFrom:
          secretKeyRef:
            key: HEDERA_MIRROR_IMPORTER_DB_HOST
            name: mirror-passwords
      HEDERA_MIRROR_WEB3_DB_NAME:
        valueFrom:
          secretKeyRef:
            key: HEDERA_MIRROR_IMPORTER_DB_NAME
            name: mirror-passwords
      HEDERA_MIRROR_WEB3_DB_PASSWORD:
        valueFrom:
          secretKeyRef:
            key: HEDERA_MIRROR_WEB3_DB_PASSWORD
            name: mirror-passwords
      HEDERA_MIRROR_WEB3_DB_USERNAME:
        valueFrom:
          secretKeyRef:
            key: HEDERA_MIRROR_WEB3_DB_USERNAME
            name: mirror-passwords
      SPRING_CLOUD_KUBERNETES_ENABLED: "true"
      SPRING_CONFIG_ADDITIONAL_LOCATION: file:/usr/etc/hedera/
    envFrom: []
    fullnameOverride: ""
    global:
      hostname: ""
      image: {}
      middleware: false
      namespaceOverride: ""
      podAnnotations: {}
      useReleaseForNameLabel: false
    hpa:
      behavior: {}
      enabled: false
      maxReplicas: 8
      metrics:
      - resource:
          name: cpu
          target:
            averageUtilization: 80
            type: Utilization
        type: Resource
      minReplicas: 1
    image:
      pullPolicy: IfNotPresent
      registry: gcr.io
      repository: mirrornode/hedera-mirror-web3
      tag: ""
    imagePullSecrets: []
    ingress:
      annotations:
        traefik.ingress.kubernetes.io/router.middlewares: '{{ include "hedera-mirror-web3.namespace"
          . }}-{{ include "hedera-mirror-web3.fullname" . }}@kubernetescrd'
      enabled: true
      hosts:
      - host: ""
        paths:
        - /web3
      tls:
        enabled: false
        secretName: ""
    labels: {}
    livenessProbe:
      httpGet:
        path: /actuator/health/liveness
        port: http
      initialDelaySeconds: 50
      periodSeconds: 10
      timeoutSeconds: 2
    middleware:
    - circuitBreaker:
        expression: NetworkErrorRatio() > 0.10 || ResponseCodeRatio(500, 600, 0, 600)
          > 0.25
    - inFlightReq:
        amount: 5
        sourceCriterion:
          ipStrategy:
            depth: 1
    - rateLimit:
        average: 10
        sourceCriterion:
          requestHost: true
    - retry:
        attempts: 3
        initialInterval: 100ms
    nodeSelector: {}
    podAnnotations: {}
    podDisruptionBudget:
      enabled: false
      minAvailable: 50%
    podSecurityContext:
      fsGroup: 1000
      runAsGroup: 1000
      runAsNonRoot: true
      runAsUser: 1000
      seccompProfile:
        type: RuntimeDefault
    priorityClassName: ""
    prometheusRules:
      Web3Errors:
        annotations:
          description: '{{ $value | humanizePercentage }} Web3 server error rate for
            {{ $labels.namespace }}/{{ $labels.pod }}'
          summary: Mirror Web3 API error rate exceeds 5%
        enabled: true
        expr: sum(rate(http_server_requests_seconds_count{application="hedera-mirror-web3",
          status="SERVER_ERROR"}[5m])) by (namespace, pod) / sum(rate(http_server_requests_seconds_count{application="hedera-mirror-web3"}[5m]))
          by (namespace, pod) > 0.05
        for: 2m
        labels:
          application: hedera-mirror-web3
          severity: critical
      Web3HighCPU:
        annotations:
          description: '{{ $labels.namespace }}/{{ $labels.pod }} CPU usage reached
            {{ $value | humanizePercentage }}'
          summary: Mirror Web3 API CPU usage exceeds 80%
        enabled: true
        expr: sum(process_cpu_usage{application="hedera-mirror-web3"}) by (namespace,
          pod) / sum(system_cpu_count{application="hedera-mirror-web3"}) by (namespace,
          pod) > 0.8
        for: 5m
        labels:
          application: hedera-mirror-web3
          area: resource
          severity: critical
      Web3HighDBConnections:
        annotations:
          description: '{{ $labels.namespace }}/{{ $labels.pod }} is using {{ $value
            | humanizePercentage }} of available database connections'
          summary: Mirror Web3 API database connection utilization exceeds 75%
        enabled: true
        expr: sum(hikaricp_connections_active{application="hedera-mirror-web3"}) by
          (namespace, pod) / sum(hikaricp_connections_max{application="hedera-mirror-web3"})
          by (namespace, pod) > 0.75
        for: 5m
        labels:
          application: hedera-mirror-web3
          area: resource
          severity: critical
      Web3HighFileDescriptors:
        annotations:
          description: '{{ $labels.namespace }}/{{ $labels.pod }} file descriptor
            usage reached {{ $value | humanizePercentage }}'
          summary: Mirror Web3 API file descriptor usage exceeds 80%
        enabled: true
        expr: sum(process_files_open_files{application="hedera-mirror-web3"}) by (namespace,
          pod) / sum(process_files_max_files{application="hedera-mirror-web3"}) by
          (namespace, pod) > 0.8
        for: 5m
        labels:
          application: hedera-mirror-web3
          area: resource
          severity: critical
      Web3HighMemory:
        annotations:
          description: '{{ $labels.namespace }}/{{ $labels.pod }} memory usage reached
            {{ $value | humanizePercentage }}'
          summary: Mirror Web3 API memory usage exceeds 80%
        enabled: true
        expr: sum(jvm_memory_used_bytes{application="hedera-mirror-web3"}) by (namespace,
          pod) / sum(jvm_memory_max_bytes{application="hedera-mirror-web3"}) by (namespace,
          pod) > 0.8
        for: 5m
        labels:
          application: hedera-mirror-web3
          area: resource
          severity: critical
      Web3LogErrors:
        annotations:
          description: Logs for {{ $labels.namespace }}/{{ $labels.pod }} have reached
            {{ $value }} error messages/s in a 3m period
          summary: High rate of log errors
        enabled: true
        expr: sum(increase(logback_events_total{application="hedera-mirror-web3",
          level="error"}[1m])) by (namespace, pod) >= 2
        for: 3m
        labels:
          application: hedera-mirror-web3
          severity: critical
      Web3NoPodsReady:
        annotations:
          description: No Web3 API instances are currently running in {{ $labels.namespace
            }}
          summary: No Web3 API instances running
        enabled: true
        expr: sum(kube_pod_status_ready{pod=~".*-web3-.*",condition="true"}) by (namespace)
          < 1
        for: 2m
        labels:
          application: hedera-mirror-web3
          area: resource
          severity: critical
      Web3NoRequests:
        annotations:
          description: Web3 API has not seen any requests to {{ $labels.namespace
            }} for 5m
          summary: No Web3 API requests seen for awhile
        enabled: true
        expr: sum(rate(http_server_requests_seconds_count{application="hedera-mirror-web3"}[3m]))
          by (namespace) <= 0
        for: 5m
        labels:
          application: hedera-mirror-web3
          severity: warning
      Web3QueryLatency:
        annotations:
          description: High average database query latency of {{ $value | humanizeDuration
            }} for {{ $labels.namespace }}/{{ $labels.pod }}
          summary: Mirror Web3 API query latency exceeds 1s
        enabled: true
        expr: sum(rate(spring_data_repository_invocations_seconds_sum{application="hedera-mirror-web3"}[5m]))
          by (namespace, pod) / sum(rate(spring_data_repository_invocations_seconds_count{application="hedera-mirror-web3"}[5m]))
          by (namespace, pod) > 1
        for: 1m
        labels:
          application: hedera-mirror-web3
          severity: warning
      Web3RequestLatency:
        annotations:
          description: High average request latency of {{ $value | humanizeDuration
            }} for {{ $labels.namespace }}/{{ $labels.pod }}
          summary: Mirror Web3 API request latency exceeds 2s
        enabled: true
        expr: sum(rate(http_server_requests_seconds_sum{application="hedera-mirror-web3"}[5m]))
          by (namespace, pod) / sum(rate(http_server_requests_seconds_count{application="hedera-mirror-web3"}[5m]))
          by (namespace, pod) > 2
        for: 1m
        labels:
          application: hedera-mirror-web3
          severity: warning
      enabled: false
    rbac:
      enabled: true
    readinessProbe:
      httpGet:
        path: /actuator/health/readiness
        port: http
      initialDelaySeconds: 40
      timeoutSeconds: 2
    resources:
      limits:
        cpu: 2
        memory: 2048Mi
      requests:
        cpu: 100m
        memory: 128Mi
    revisionHistoryLimit: 3
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true
    service:
      annotations: {}
      port: 80
      type: ClusterIP
    serviceAccount:
      create: true
    serviceMonitor:
      enabled: false
      interval: 30s
    terminationGracePeriodSeconds: 60
    test:
      enabled: true
      image:
        pullPolicy: IfNotPresent
        repository: postman/newman
        tag: 5.3.1-alpine
    tolerations: []
    updateStrategy:
      rollingUpdate:
        maxSurge: 10%
        maxUnavailable: 25%
      type: RollingUpdate
    volumeMounts:
      config:
        mountPath: /usr/etc/hedera
    volumes:
      config:
        secret:
          defaultMode: 420
          secretName: '{{ include "hedera-mirror-web3.fullname" . }}'
image:
  pullPolicy: IfNotPresent
  repository: nginx
  tag: ""
imagePullSecrets: []
ingress:
  annotations: {}
  className: ""
  enabled: false
  hosts:
  - host: chart-example.local
    paths:
    - path: /
      pathType: ImplementationSpecific
  tls: []
nameOverride: ""
network-node:
  affinity: {}
  autoscaling:
    enabled: false
    maxReplicas: 100
    minReplicas: 1
    targetCPUUtilizationPercentage: 80
  configMap:
    configPath: /home/mobile/limechain/hedera-local-node/compose-network/network-node/data/config
    rootPath: ../../compose-network/network-node
  env:
    JAVA_HEAP_MAX: 2g
    JAVA_HEAP_MIN: 256m
    JAVA_OPTS: -XX:+UnlockExperimentalVMOptions -XX:+UseZGC -Xlog:gc*:gc.log
  envFrom: []
  fullnameOverride: ""
  global: {}
  image:
    pullPolicy: IfNotPresent
    repository: cabob/hedera-services
    tag: latest
  imagePullSecrets: []
  ingress:
    annotations: {}
    className: ""
    enabled: false
    hosts:
    - host: chart-example.local
      paths:
      - path: /
        pathType: ImplementationSpecific
    tls: []
  nameOverride: ""
  nodeSelector: {}
  podAnnotations: {}
  podSecurityContext: {}
  replicaCount: 1
  resources: {}
  securityContext:
    allowPrivilegeEscalation: false
    runAsUser: 0
  service:
    port: 50211
    type: ClusterIP
  serviceAccount:
    annotations: {}
    create: true
    name: ""
  tolerations: []
  volumeMounts:
  - mountPath: /opt/hgcapp/accountBalances
    name: network-node-logs-root-path
    subPath: accountBalances
  - mountPath: /opt/hgcapp/recordStreams
    name: network-node-logs-root-path
    subPath: recordStreams
  - mountPath: /opt/hedera/services/services/output
    name: network-node-logs-root-path
    subPath: logs
  volumes:
    application-config-path:
      configMap:
        name: application-config
    application-root-path:
      configMap:
        name: application-root
    data-empty:
      emptyDir: {}
    network-node-logs-root-path:
      persistentVolumeClaim:
        claimName: node-logs
nodeSelector: {}
podAnnotations: {}
podSecurityContext: {}
replicaCount: 1
resources: {}
securityContext: {}
service:
  port: 80
  type: ClusterIP
serviceAccount:
  annotations: {}
  create: true
  name: ""
tolerations: []

HOOKS:
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/rest/templates/tests/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  annotations:
    helm.sh/hook: test-success
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
  labels:
    app.kubernetes.io/component: rest
    app.kubernetes.io/name: rest
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: hedera-mirror-node
    app.kubernetes.io/version: "0.68.0"
    helm.sh/chart: rest-0.68.0
  name: mirror-node-rest-test
  namespace: default
data:
  test.sh: |-
    #!/usr/bin/env bash
    set -o pipefail
    set -o errexit

    URI='http://mirror-node-rest:80/api/v1'

    function setup() {
      apk add -qu curl jq
    }

    # Infinite loop is okay since helm test itself has a timeout
    function has_data() {
      local name="${1}"
      until (curl -s "${URI}/${name}?limit=1" | jq -e ".${name} | length >= 1"); do
        echo "Waiting for ${name} data" >&3
        sleep 2
      done
    }

    @test "Has accounts" {
      has_data "accounts"
    }

    @test "Has transactions" {
      has_data "transactions"
    }
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/rest/templates/tests/pod.yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    helm.sh/hook: test-success
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
  labels:
    app.kubernetes.io/component: rest
    app.kubernetes.io/name: rest
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: hedera-mirror-node
    app.kubernetes.io/version: "0.68.0"
    helm.sh/chart: rest-0.68.0
  name: mirror-node-rest-test
  namespace: default
spec:
  containers:
    - name: test
      image: "bats/bats:1.7.0"
      imagePullPolicy: IfNotPresent
      args:
        - /usr/lib/hedera-mirror-rest/test.sh
      volumeMounts:
        - name: tests
          mountPath: /usr/lib/hedera-mirror-rest
          readOnly: true
  terminationGracePeriodSeconds: 1
  restartPolicy: Never
  volumes:
    - name: tests
      configMap:
        defaultMode: 0555
        name: mirror-node-rest-test
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/rosetta/templates/tests/pod.yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    helm.sh/hook: test-success
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
  labels:
    app.kubernetes.io/component: rosetta
    app.kubernetes.io/name: rosetta
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: hedera-mirror-node
    app.kubernetes.io/version: "0.68.0"
    helm.sh/chart: rosetta-0.68.0
  name: mirror-1-rosetta-test
  namespace: default
spec:
  containers:
    - name: test
      image: "postman/newman:5.3.1-alpine"
      imagePullPolicy: IfNotPresent
      args:
        - run
        - https://raw.githubusercontent.com/hashgraph/hedera-mirror-node/v0.68.0/hedera-mirror-rosetta/scripts/validation/postman/rosetta-api-postman.json
        - --env-var
        - base_url=http://mirror-1-rosetta:80
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop: [ALL]
        readOnlyRootFilesystem: true
  restartPolicy: Never
  securityContext:
    fsGroup: 1000
    runAsGroup: 1000
    runAsNonRoot: true
    runAsUser: 1000
    seccompProfile:
      type: RuntimeDefault
  terminationGracePeriodSeconds: 1
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/web3/templates/tests/pod.yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    helm.sh/hook: test-success
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
  labels:
    app.kubernetes.io/component: web3
    app.kubernetes.io/name: web3
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: hedera-mirror-node
    app.kubernetes.io/version: "0.68.0"
    helm.sh/chart: web3-0.68.0
  name: mirror-1-web3-test
  namespace: default
spec:
  containers:
    - name: test
      image: "postman/newman:5.3.1-alpine"
      imagePullPolicy: IfNotPresent
      args:
        - run
        - https://raw.githubusercontent.com/hashgraph/hedera-mirror-node/v0.68.0/hedera-mirror-web3/postman.json
        - --env-var
        - baseUrl=http://mirror-1-web3:80
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop: [ALL]
        readOnlyRootFilesystem: true
  restartPolicy: Never
  securityContext:
    fsGroup: 1000
    runAsGroup: 1000
    runAsNonRoot: true
    runAsUser: 1000
    seccompProfile:
      type: RuntimeDefault
  terminationGracePeriodSeconds: 1
---
# Source: hedera-mirror-node/charts/network-node/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "mirror-1-network-node-test-connection"
  labels:
    helm.sh/chart: network-node-0.1.0
    app.kubernetes.io/name: network-node
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
spec:
  containers:
    - name: wget
      image: busybox
      command: ['wget']
      args: ['mirror-1-network-node:50211']
  restartPolicy: Never
---
# Source: hedera-mirror-node/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "mirror-1-hedera-mirror-node-test-connection"
  labels:
    helm.sh/chart: hedera-mirror-node-0.1.0
    app.kubernetes.io/name: hedera-mirror-node
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
spec:
  containers:
    - name: wget
      image: busybox
      command: ['wget']
      args: ['mirror-1-hedera-mirror-node:80']
  restartPolicy: Never
MANIFEST:
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/postgresql/templates/pgpool/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: mirror-1-postgres-pgpool
  namespace: "default"
  labels:
    app.kubernetes.io/name: postgres
    helm.sh/chart: postgresql-9.4.11
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: pgpool
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: postgres
      app.kubernetes.io/instance: mirror-1
      app.kubernetes.io/component: pgpool
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/redis/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: mirror-1-redis
  namespace: "default"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-17.3.7
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: redis
      app.kubernetes.io/instance: mirror-1
---
# Source: hedera-mirror-node/charts/hedera-explorer/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels: 
    app.kubernetes.io/component: hedera-explorer
    app.kubernetes.io/name: hedera-explorer
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: hedera-explorer
    app.kubernetes.io/version: "0.1.0-SNAPSHOT"
    helm.sh/chart: hedera-explorer-0.1.0
  name: mirror-1-hedera-explorer
  namespace: default
---
# Source: hedera-mirror-node/charts/hedera-json-rpc-relay/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: mirror-1-hedera-json-rpc-relay
  labels:
    helm.sh/chart: hedera-json-rpc-relay-0.12.0
    app.kubernetes.io/name: hedera-json-rpc-relay
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/version: "0.12.0-SNAPSHOT"
    app.kubernetes.io/managed-by: Helm
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/grpc/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels: 
    app.kubernetes.io/component: grpc
    app.kubernetes.io/name: grpc
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: hedera-mirror-node
    app.kubernetes.io/version: "0.68.0"
    helm.sh/chart: grpc-0.68.0
  name: mirror-1-grpc
  namespace: default
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/importer/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels: 
    app.kubernetes.io/component: importer
    app.kubernetes.io/name: importer
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: hedera-mirror-node
    app.kubernetes.io/version: "0.68.0"
    helm.sh/chart: importer-0.68.0
  name: mirror-1-importer
  namespace: default
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/monitor/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels: 
    app.kubernetes.io/component: monitor
    app.kubernetes.io/name: monitor
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: hedera-mirror-node
    app.kubernetes.io/version: "0.68.0"
    helm.sh/chart: monitor-0.68.0
  name: mirror-1-monitor
  namespace: default
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/postgresql/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: mirror-1-postgres
  namespace: "default"
  labels:
    app.kubernetes.io/name: postgres
    helm.sh/chart: postgresql-9.4.11
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
automountServiceAccountToken: true
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/redis/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
automountServiceAccountToken: true
metadata:
  name: mirror-1-redis
  namespace: "default"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-17.3.7
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/rest/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels: 
    app.kubernetes.io/component: rest
    app.kubernetes.io/name: rest
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: hedera-mirror-node
    app.kubernetes.io/version: "0.68.0"
    helm.sh/chart: rest-0.68.0
  name: mirror-node-rest
  namespace: default
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/rosetta/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels: 
    app.kubernetes.io/component: rosetta
    app.kubernetes.io/name: rosetta
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: hedera-mirror-node
    app.kubernetes.io/version: "0.68.0"
    helm.sh/chart: rosetta-0.68.0
  name: mirror-1-rosetta
  namespace: default
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/web3/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels: 
    app.kubernetes.io/component: web3
    app.kubernetes.io/name: web3
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: hedera-mirror-node
    app.kubernetes.io/version: "0.68.0"
    helm.sh/chart: web3-0.68.0
  name: mirror-1-web3
  namespace: default
---
# Source: hedera-mirror-node/charts/network-node/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: mirror-1-network-node
  labels:
    helm.sh/chart: network-node-0.1.0
    app.kubernetes.io/name: network-node
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: hedera-mirror-node/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: mirror-1-hedera-mirror-node
  labels:
    helm.sh/chart: hedera-mirror-node-0.1.0
    app.kubernetes.io/name: hedera-mirror-node
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: hedera-mirror-node/charts/hedera-json-rpc-relay/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  labels: 
    helm.sh/chart: hedera-json-rpc-relay-0.12.0
    app.kubernetes.io/name: hedera-json-rpc-relay
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/version: "0.12.0-SNAPSHOT"
    app.kubernetes.io/managed-by: Helm
  name: mirror-1-hedera-json-rpc-relay
type: Opaque
stringData:
  OPERATOR_ID_MAIN: 0.0.2
  OPERATOR_KEY_MAIN: 302e020100300506032b65700422042091132178e72057a1d7528025956fe39b0b847f200ab59b2fdd367017f3087137
  OPERATOR_ID_ETH_SENDRAWTRANSACTION: ""
  OPERATOR_KEY_ETH_SENDRAWTRANSACTION: ""
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/grpc/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  labels: 
    app.kubernetes.io/component: grpc
    app.kubernetes.io/name: grpc
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: hedera-mirror-node
    app.kubernetes.io/version: "0.68.0"
    helm.sh/chart: grpc-0.68.0
  name: mirror-1-grpc
  namespace: default
type: Opaque
stringData:
  application.yaml: |-
    {}
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/importer/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  labels: 
    app.kubernetes.io/component: importer
    app.kubernetes.io/name: importer
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: hedera-mirror-node
    app.kubernetes.io/version: "0.68.0"
    helm.sh/chart: importer-0.68.0
  name: mirror-1-importer
  namespace: default
type: Opaque
data:
  addressbook.bin: CvQGCgoxNzIuMjcuMC4zGgUwLjAuMyLMBjMwODIwMWEyMzAwZDA2MDkyYTg2NDg4NmY3MGQwMTAxMDEwNTAwMDM4MjAxOGYwMDMwODIwMThhMDI4MjAxODEwMGExMjAwNmYyNTI3MjQyY2Q3ZjUzYjljNGZkNWRlODY5YzU2MmZmNGVkOGE0YWIzYTYyOTYzZjNmODE4OGJlNzhmMjU4ZWFmNDJiOWMzZTNlOGY1ODY5NzE2MDQxM2JiZjc3YWFkZmFmMGQ5ZmZkODY5ODRiM2JjNGZiNWYwMmUxMWU4YWNiNmVjNjdlNDI2NGFkZGNhMTNmZmRlODM2NzU3OGMyNzkyZmM2YWUzMjZlN2IzNDg1NmQyM2YwMjU3NGY1YTI3NDgxNTYwMDMyZDczNzQ5ZGFmOWQzMzJhNzg4MzUxMGU2MGVhMjIyODg5YTNiZmMzYmVkZjM1MGYzYTM2Y2FkYTM5Yjk0NTM5Yzc0ZmI4MjY1Njc4YmQ1YzE0OGZiYzkxNjlhZGNhY2Y2NzE5YjkyZWU4ZDQyYTlkMmY4OTIzY2M0ZmUzYjQ5NGM0NjdmNGU5OGEyYTQ5ZTBhMGVmNWFhNDMzY2JlYzQzOGJhYTI1NmFlNjFhMDU5MGNlNGU2N2QwYjNmZTVmMmRhOWFkOTBjMjZlMjI5ZDI4ZDY3OTQ1Njk3OGQ2NTczNTczZWJiNDYyMzliYWQ0YWJmYWI0ZmZlNjRlYTRhMzk3YzZiYzVmNzE3NWUxYWMzNjQ4OGYxN2M2NzczYjVlZDkzNDE1ZTU3YzQxYzZjZDg4NjZkYmQxMjM2MDA2MjMzOWNiMTU0ZDg1YWQxYWVjZDFkNzYwYTA5NDFkODBlMjhmZTI0NjAzM2E4NjhkYmJjODk3NWYzNjA0NDMxZmNlOWFmOTFkYzQyNTQwYWIwNGIxYmJlYzIxZjM1Zjc1ZjMwYWY2ZjA1Yzg5ODY0MjkzODZlMmI0MzliZmQyZmQ1OTEyYzgyNjAwODAxZTljMDg1N2YxNjVlODg3ZjIyMzNkYzJjMDk4YmExMjY3YjA1OWRiOGEyZGI3MTRmZTUwNjY4ZGJmMDkzNjA0YTVkZjNjMjMyMTQ2NzIzMjExY2U1ODU1MjYzOGZhZTlmYzA2NGE3ZjA5YjAyMDMwMTAwMDEyAhgDQgoKBKwbAAMQv4cDUAE=
  application.yaml: aGVkZXJhOgogIG1pcnJvcjoKICAgIGltcG9ydGVyOgogICAgICBkYjoKICAgICAgICBsb2FkQmFsYW5jZTogZmFsc2UKICAgICAgaW5pdGlhbEFkZHJlc3NCb29rOiAvdXNyL2V0Yy9oZWRlcmEvYWRkcmVzc2Jvb2suYmlu
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/monitor/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  labels: 
    app.kubernetes.io/component: monitor
    app.kubernetes.io/name: monitor
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: hedera-mirror-node
    app.kubernetes.io/version: "0.68.0"
    helm.sh/chart: monitor-0.68.0
  name: mirror-1-monitor
  namespace: default
type: Opaque
stringData:
  application.yaml: |-
    {}
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/rest/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  labels: 
    app.kubernetes.io/component: rest
    app.kubernetes.io/name: rest
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: hedera-mirror-node
    app.kubernetes.io/version: "0.68.0"
    helm.sh/chart: rest-0.68.0
  name: mirror-node-rest
  namespace: default
type: Opaque
stringData:
  application.yaml: |-
    hedera:
      mirror:
        rest:
          metrics:
            config:
              authentication: false
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/rosetta/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  labels: 
    app.kubernetes.io/component: rosetta
    app.kubernetes.io/name: rosetta
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: hedera-mirror-node
    app.kubernetes.io/version: "0.68.0"
    helm.sh/chart: rosetta-0.68.0
  name: mirror-1-rosetta
  namespace: default
type: Opaque
stringData:
  application.yaml: |-
    {}
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/web3/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  labels: 
    app.kubernetes.io/component: web3
    app.kubernetes.io/name: web3
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: hedera-mirror-node
    app.kubernetes.io/version: "0.68.0"
    helm.sh/chart: web3-0.68.0
  name: mirror-1-web3
  namespace: default
type: Opaque
stringData:
  application.yaml: |-
    {}
---
# Source: hedera-mirror-node/charts/hedera-mirror/templates/secret-init.yaml
apiVersion: v1
kind: Secret
metadata:
  labels:
    app.kubernetes.io/component: hedera-mirror
    app.kubernetes.io/name: hedera-mirror
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: hedera-mirror-node
    app.kubernetes.io/version: "0.68.0"
    helm.sh/chart: hedera-mirror-0.68.0
  name: mirror-1-init
  namespace: default
stringData:
  init.sh: |-
    #!/bin/bash
    set -e

    PGHBACONF="/opt/bitnami/postgresql/conf/pg_hba.conf"
    cp "${PGHBACONF}" "${PGHBACONF}.bak"
    echo "local all all trust" > "${PGHBACONF}"
    pg_ctl reload

    psql -d "user=postgres connect_timeout=3" \
      --set ON_ERROR_STOP=1 \
      --set "dbName=${HEDERA_MIRROR_IMPORTER_DB_NAME}" \
      --set "dbSchema=${HEDERA_MIRROR_IMPORTER_DB_SCHEMA}" \
      --set "grpcPassword=${HEDERA_MIRROR_GRPC_DB_PASSWORD}" \
      --set "grpcUsername=${HEDERA_MIRROR_GRPC_DB_USERNAME}" \
      --set "importerPassword=${HEDERA_MIRROR_IMPORTER_DB_PASSWORD}" \
      --set "importerUsername=${HEDERA_MIRROR_IMPORTER_DB_USERNAME}" \
      --set "ownerUsername=${HEDERA_MIRROR_IMPORTER_DB_OWNER}" \
      --set "ownerPassword=${HEDERA_MIRROR_IMPORTER_DB_OWNERPASSWORD}" \
      --set "restPassword=${HEDERA_MIRROR_IMPORTER_DB_RESTPASSWORD}" \
      --set "restUsername=${HEDERA_MIRROR_IMPORTER_DB_RESTUSERNAME}" \
      --set "rosettaPassword=${HEDERA_MIRROR_ROSETTA_DB_PASSWORD}" \
      --set "rosettaUsername=${HEDERA_MIRROR_ROSETTA_DB_USERNAME}" \
      --set "web3Password=${HEDERA_MIRROR_WEB3_DB_PASSWORD}" \
      --set "web3Username=${HEDERA_MIRROR_WEB3_DB_USERNAME}" <<__SQL__

    -- Create database & owner
    create user :ownerUsername with login password :'ownerPassword';
    create database :dbName with owner :ownerUsername;

    -- Add extensions
    create extension if not exists pg_stat_statements;

    -- Create roles
    create role readonly;
    create role readwrite in role readonly;

    -- Create users
    create user :grpcUsername with login password :'grpcPassword' in role readonly;
    create user :importerUsername with login password :'importerPassword' in role readwrite;
    create user :rosettaUsername with login password :'rosettaPassword' in role readonly;
    create user :web3Username with login password :'web3Password' in role readonly;
    alter user :ownerUsername with createrole;

    -- Create schema
    \connect :dbName :ownerUsername
    create schema if not exists :dbSchema authorization :ownerUsername;
    grant usage on schema :dbSchema to public;
    revoke create on schema :dbSchema from public;

    -- Grant readonly privileges
    grant connect on database :dbName to readonly;
    grant select on all tables in schema :dbSchema to readonly;
    grant select on all sequences in schema :dbSchema to readonly;
    grant usage on schema :dbSchema to readonly;
    alter default privileges in schema :dbSchema grant select on tables to readonly;
    alter default privileges in schema :dbSchema grant select on sequences to readonly;

    -- Grant readwrite privileges
    grant insert, update, delete on all tables in schema :dbSchema to readwrite;
    grant usage on all sequences in schema :dbSchema to readwrite;
    alter default privileges in schema :dbSchema grant insert, update, delete on tables to readwrite;
    alter default privileges in schema :dbSchema grant usage on sequences to readwrite;

    -- Alter search path
    \connect postgres postgres
    alter database :dbName set search_path = :dbSchema, public;
    __SQL__

    mv "${PGHBACONF}.bak" "${PGHBACONF}"
    pg_ctl reload
---
# Source: hedera-mirror-node/charts/hedera-mirror/templates/secret-passwords.yaml
# postgresl-ha doesn't support templated names
apiVersion: v1
kind: Secret
metadata:
  labels: 
    app.kubernetes.io/component: hedera-mirror
    app.kubernetes.io/name: hedera-mirror
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: hedera-mirror-node
    app.kubernetes.io/version: "0.68.0"
    helm.sh/chart: hedera-mirror-0.68.0
  name: mirror-passwords
  namespace: default
stringData:

  HEDERA_MIRROR_GRPC_DB_PASSWORD: "W5FmCJf14v2BdWL4Dctr57bChcdA01O5SR3XE1Pw"
  HEDERA_MIRROR_GRPC_DB_USERNAME: "mirror_grpc"
  HEDERA_MIRROR_IMPORTER_DB_HOST: "mirror-1-postgres-pgpool"
  HEDERA_MIRROR_IMPORTER_DB_NAME: "mirror_node"
  HEDERA_MIRROR_IMPORTER_DB_SCHEMA: "public"
  HEDERA_MIRROR_IMPORTER_DB_PASSWORD: "BPRfuzAjQQkmtHuDXE7f9pw0CYQQIPqVHCulPXxA"
  HEDERA_MIRROR_IMPORTER_DB_USERNAME: "mirror_importer"
  HEDERA_MIRROR_IMPORTER_DB_OWNERPASSWORD: "OAVNte5mjTKUWWQR7t3VdBmXvFV6IovxrxsD03Yv"
  HEDERA_MIRROR_IMPORTER_DB_OWNER: "mirror_node"
  HEDERA_MIRROR_IMPORTER_DB_RESTPASSWORD: "RUpbZXigIEEGPF0DRlHB9IKTDMvaJfGYwqlrfL0B"
  HEDERA_MIRROR_IMPORTER_DB_RESTUSERNAME: "mirror_rest"
  HEDERA_MIRROR_ROSETTA_DB_PASSWORD: "wZa96Yb4qKPint6xVovyYcgnFsydAdFgjP4iSCCh"
  HEDERA_MIRROR_ROSETTA_DB_USERNAME: "mirror_rosetta"
  HEDERA_MIRROR_WEB3_DB_PASSWORD: "82U0XXrzRWzjYrnqj9l5C6NssEmQvJ3sR1PEfUxE"
  HEDERA_MIRROR_WEB3_DB_USERNAME: "mirror_web3"

  admin-password: "ZDSW57eh16H2AApZ0pqmO75efGjUbgV3p63iLUhz"
  PGPOOL_POSTGRES_CUSTOM_PASSWORDS: "W5FmCJf14v2BdWL4Dctr57bChcdA01O5SR3XE1Pw,BPRfuzAjQQkmtHuDXE7f9pw0CYQQIPqVHCulPXxA,OAVNte5mjTKUWWQR7t3VdBmXvFV6IovxrxsD03Yv,RUpbZXigIEEGPF0DRlHB9IKTDMvaJfGYwqlrfL0B,wZa96Yb4qKPint6xVovyYcgnFsydAdFgjP4iSCCh,82U0XXrzRWzjYrnqj9l5C6NssEmQvJ3sR1PEfUxE"
  PGPOOL_POSTGRES_CUSTOM_USERS: "mirror_grpc,mirror_importer,mirror_node,mirror_rest,mirror_rosetta,mirror_web3"
  postgresql-password: "lRs6i7gwqPRF4n7NciDZRPBKzXZtyC2xmd19CKaz"
  repmgr-password: "Hv6KcYYCRG1WKAX0O4gWAh64tulRCnP7Tqvjx46d"
---
# Source: hedera-mirror-node/charts/hedera-mirror/templates/secret-redis.yaml
# redis chart doesn't support template names
apiVersion: v1
kind: Secret
metadata:
  labels: 
    app.kubernetes.io/component: hedera-mirror
    app.kubernetes.io/name: hedera-mirror
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: hedera-mirror-node
    app.kubernetes.io/version: "0.68.0"
    helm.sh/chart: hedera-mirror-0.68.0
  name: mirror-redis
  namespace: default
stringData:

  SPRING_REDIS_HOST: "mirror-1-redis"
  SPRING_REDIS_PASSWORD: "qeWXvZz6NTEDvEngCAirCfDfpv6Wfh9ZKucHRofj"
  SPRING_REDIS_SENTINEL_MASTER: "mirror"
  SPRING_REDIS_SENTINEL_NODES: "mirror-1-redis:26379"
  SPRING_REDIS_SENTINEL_PASSWORD: "qeWXvZz6NTEDvEngCAirCfDfpv6Wfh9ZKucHRofj"
---
# Source: hedera-mirror-node/templates/abook-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: abook-config
type: Opaque
data:
  application.yaml: CvQGCgoxNzIuMjcuMC4zGgUwLjAuMyLMBjMwODIwMWEyMzAwZDA2MDkyYTg2NDg4NmY3MGQwMTAxMDEwNTAwMDM4MjAxOGYwMDMwODIwMThhMDI4MjAxODEwMGExMjAwNmYyNTI3MjQyY2Q3ZjUzYjljNGZkNWRlODY5YzU2MmZmNGVkOGE0YWIzYTYyOTYzZjNmODE4OGJlNzhmMjU4ZWFmNDJiOWMzZTNlOGY1ODY5NzE2MDQxM2JiZjc3YWFkZmFmMGQ5ZmZkODY5ODRiM2JjNGZiNWYwMmUxMWU4YWNiNmVjNjdlNDI2NGFkZGNhMTNmZmRlODM2NzU3OGMyNzkyZmM2YWUzMjZlN2IzNDg1NmQyM2YwMjU3NGY1YTI3NDgxNTYwMDMyZDczNzQ5ZGFmOWQzMzJhNzg4MzUxMGU2MGVhMjIyODg5YTNiZmMzYmVkZjM1MGYzYTM2Y2FkYTM5Yjk0NTM5Yzc0ZmI4MjY1Njc4YmQ1YzE0OGZiYzkxNjlhZGNhY2Y2NzE5YjkyZWU4ZDQyYTlkMmY4OTIzY2M0ZmUzYjQ5NGM0NjdmNGU5OGEyYTQ5ZTBhMGVmNWFhNDMzY2JlYzQzOGJhYTI1NmFlNjFhMDU5MGNlNGU2N2QwYjNmZTVmMmRhOWFkOTBjMjZlMjI5ZDI4ZDY3OTQ1Njk3OGQ2NTczNTczZWJiNDYyMzliYWQ0YWJmYWI0ZmZlNjRlYTRhMzk3YzZiYzVmNzE3NWUxYWMzNjQ4OGYxN2M2NzczYjVlZDkzNDE1ZTU3YzQxYzZjZDg4NjZkYmQxMjM2MDA2MjMzOWNiMTU0ZDg1YWQxYWVjZDFkNzYwYTA5NDFkODBlMjhmZTI0NjAzM2E4NjhkYmJjODk3NWYzNjA0NDMxZmNlOWFmOTFkYzQyNTQwYWIwNGIxYmJlYzIxZjM1Zjc1ZjMwYWY2ZjA1Yzg5ODY0MjkzODZlMmI0MzliZmQyZmQ1OTEyYzgyNjAwODAxZTljMDg1N2YxNjVlODg3ZjIyMzNkYzJjMDk4YmExMjY3YjA1OWRiOGEyZGI3MTRmZTUwNjY4ZGJmMDkzNjA0YTVkZjNjMjMyMTQ2NzIzMjExY2U1ODU1MjYzOGZhZTlmYzA2NGE3ZjA5YjAyMDMwMTAwMDEyAhgDQgoKBKwbAAMQv4cDUAE=
---
# Source: hedera-mirror-node/templates/application-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: app-config
type: Opaque
data:
  application.yaml: aGVkZXJhOgogIG1pcnJvcjoKICAgIGdycGM6CiAgICAgIGxpc3RlbmVyOgogICAgICAgIHR5cGU6IFNIQVJFRF9QT0xMCiAgICBpbXBvcnRlcjoKICAgICAgcGFyc2VyOgogICAgICAgIHJlY29yZDoKICAgICAgICAgIGVudGl0eToKICAgICAgICAgICAgcmVkaXM6CiAgICAgICAgICAgICAgZW5hYmxlZDogZmFsc2UKICAgICAgICAgIHNpZGVjYXI6CiAgICAgICAgICAgIGVuYWJsZWQ6IGZhbHNlCiAgICAgIGRvd25sb2FkZXI6CiAgICAgICAgYWNjZXNzS2V5OiBtaW5pb2FkbWluCiAgICAgICAgY2xvdWRQcm92aWRlcjogIlMzIgogICAgICAgIHNlY3JldEtleTogbWluaW9hZG1pbgogICAgICAgIGJ1Y2tldE5hbWU6IGhlZGVyYS1zdHJlYW1zCiAgICAgICAgZW5kcG9pbnRPdmVycmlkZTogaHR0cDovL21pbmlvOjkwMDAKICAgICAgaW5pdGlhbEFkZHJlc3NCb29rOiAiL3Vzci9ldGMvaGVkZXJhLW1pcnJvci1pbXBvcnRlci9sb2NhbC1kZXYtMS1ub2RlLmFkZHJlc3Nib29rLmYxMDIuanNvbi5iaW4iCiAgICAgIG5ldHdvcms6IE9USEVSCiAgICBtb25pdG9yOgogICAgICBtaXJyb3JOb2RlOgogICAgICAgIGdycGM6CiAgICAgICAgICBob3N0OiBtaXJyb3Itbm9kZS1ncnBjCiAgICAgICAgICBwb3J0OiA1NjAwCiAgICAgICAgcmVzdDoKICAgICAgICAgIGhvc3Q6IG1pcnJvci1ub2RlLXJlc3QKICAgICAgICAgIHBvcnQ6IDU1NTEKICAgICAgcHVibGlzaDoKICAgICAgICBzY2VuYXJpb3M6CiAgICAgICAgICBwaW5nZXI6CiAgICAgICAgICAgIHByb3BlcnRpZXM6CiAgICAgICAgICAgICAgYW1vdW50OiAxCiAgICAgICAgICAgICAgbWF4VHJhbnNhY3Rpb25GZWU6IDEwMDAwCiAgICAgICAgICAgICAgc2VuZGVyQWNjb3VudElkOiAwLjAuMgogICAgICAgICAgICAgIHJlY2lwaWVudEFjY291bnRJZDogMC4wLjU1CiAgICAgICAgICAgICAgdHJhbnNmZXJUeXBlczoKICAgICAgICAgICAgICAgIC0gQ1JZUFRPCiAgICAgICAgICAgIHJlY2VpcHRQZXJjZW50OiAxLjAKICAgICAgICAgICAgdHBzOiAxLjAKICAgICAgICAgICAgdHlwZTogQ1JZUFRPX1RSQU5TRkVSCiAgICAgIHN1YnNjcmliZToKICAgICAgICBncnBjOgogICAgICAgICAgaGNzOgogICAgICAgICAgICBlbmFibGVkOiBmYWxzZQogICAgICAgIHJlc3Q6CiAgICAgICAgICB0cmFuc2FjdGlvbklkOgogICAgICAgICAgICBlbmFibGVkOiB0cnVlCiAgICAgICAgICAgIHNhbXBsZVBlcmNlbnQ6IDEuMAogICAgICBuZXR3b3JrOiBPVEhFUgogICAgICBub2RlczoKICAgICAgICAtIGFjY291bnRJZDogMC4wLjMKICAgICAgICAgIGhvc3Q6IG5ldHdvcmstbm9kZQogICAgICBvcGVyYXRvcjoKICAgICAgICBhY2NvdW50SWQ6IDAuMC4yCiAgICAgICAgcHJpdmF0ZUtleTogMzAyZTAyMDEwMDMwMDUwNjAzMmI2NTcwMDQyMjA0MjA5MTEzMjE3OGU3MjA1N2ExZDc1MjgwMjU5NTZmZTM5YjBiODQ3ZjIwMGFiNTliMmZkZDM2NzAxN2YzMDg3MTM3Cg==
---
# Source: hedera-mirror-node/charts/hedera-json-rpc-relay/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: mirror-1-hedera-json-rpc-relay
  labels:
    app:  hedera-json-rpc-relay
    
    helm.sh/chart: hedera-json-rpc-relay-0.12.0
    app.kubernetes.io/name: hedera-json-rpc-relay
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/version: "0.12.0-SNAPSHOT"
    app.kubernetes.io/managed-by: Helm
data:
  HEDERA_NETWORK: "{ \"network-node:50211\": \"0.0.3\" }"
  MIRROR_NODE_URL: "http://mirror-node-rest:5551"
  LOCAL_NODE: "true"
  SERVER_PORT: "7546"
  CHAIN_ID: '0x12a'
  DEFAULT_RATE_LIMIT: "200"
  TIER_1_RATE_LIMIT: "100"
  TIER_2_RATE_LIMIT: "800"
  TIER_3_RATE_LIMIT: "1600"
  LIMIT_DURATION: "60000"
  HBAR_RATE_LIMIT_TINYBAR: "8e+09"
  HBAR_RATE_LIMIT_DURATION: "60000"
  ETH_GET_LOGS_BLOCK_RANGE_LIMIT: "2000"
  RATE_LIMIT_DISABLED: "true"
  DEV_MODE: "FALSE"
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/postgresql/templates/postgresql/hooks-scripts-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: mirror-1-postgres-postgresql-hooks-scripts
  namespace: "default"
  labels:
    app.kubernetes.io/name: postgres
    helm.sh/chart: postgresql-9.4.11
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: postgresql
data:
  pre-stop.sh: |-
    #!/bin/bash
    set -o errexit
    set -o pipefail
    set -o nounset

    # Debug section
    exec 3>&1
    exec 4>&2

    # Load Libraries
    . /opt/bitnami/scripts/liblog.sh
    . /opt/bitnami/scripts/libpostgresql.sh
    . /opt/bitnami/scripts/librepmgr.sh

    # Load PostgreSQL & repmgr environment variables
    . /opt/bitnami/scripts/postgresql-env.sh

    # Auxiliary functions
    is_new_primary_ready() {
        return_value=1
        currenty_primary_node="$(repmgr_get_primary_node)"
        currenty_primary_host="$(echo $currenty_primary_node | awk '{print $1}')"

        info "$currenty_primary_host != $REPMGR_NODE_NETWORK_NAME"
        if [[ $(echo $currenty_primary_node | wc -w) -eq 2 ]] && [[ "$currenty_primary_host" != "$REPMGR_NODE_NETWORK_NAME" ]]; then
            info "New primary detected, leaving the cluster..."
            return_value=0
        else
            info "Waiting for a new primary to be available..."
        fi
        return $return_value
    }

    export MODULE="pre-stop-hook"

    if [[ "${BITNAMI_DEBUG}" == "true" ]]; then
        info "Bash debug is on"
    else
        info "Bash debug is off"
        exec 1>/dev/null
        exec 2>/dev/null
    fi

    postgresql_enable_nss_wrapper

    # Prepare env vars for managing roles
    readarray -t primary_node < <(repmgr_get_upstream_node)
    primary_host="${primary_node[0]}"

    # Stop postgresql for graceful exit.
    postgresql_stop

    if [[ -z "$primary_host" ]] || [[ "$primary_host" == "$REPMGR_NODE_NETWORK_NAME" ]]; then
        info "Primary node need to wait for a new primary node before leaving the cluster"
        retry_while is_new_primary_ready 10 5
    else
        info "Standby node doesn't need to wait, leaving the cluster."
    fi
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/redis/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: mirror-1-redis-configuration
  namespace: "default"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-17.3.7
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
data:
  redis.conf: |-
    # User-supplied common configuration:
    # Enable AOF https://redis.io/topics/persistence#append-only-file
    appendonly yes
    # Disable RDB persistence, AOF persistence already enabled.
    save ""
    # End of common configuration
  master.conf: |-
    dir /data
    # User-supplied master configuration:
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
    # End of master configuration
  replica.conf: |-
    dir /data
    # User-supplied replica configuration:
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
    # End of replica configuration
  sentinel.conf: |-
    dir "/tmp"
    port 26379
    sentinel monitor mirror mirror-1-redis-node-0.mirror-1-redis-headless.default.svc.cluster.local 6379 2
    sentinel down-after-milliseconds mirror 60000
    sentinel failover-timeout mirror 180000
    sentinel parallel-syncs mirror 1
    # User-supplied sentinel configuration:
    # End of sentinel configuration
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/redis/templates/health-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: mirror-1-redis-health
  namespace: "default"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-17.3.7
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
data:
  ping_readiness_local.sh: |-
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -n "$REDIS_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_PASSWORD"
    response=$(
      timeout -s 3 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_local.sh: |-
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -n "$REDIS_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_PASSWORD"
    response=$(
      timeout -s 3 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    responseFirstWord=$(echo $response | head -n1 | awk '{print $1;}')
    if [ "$response" != "PONG" ] && [ "$responseFirstWord" != "LOADING" ] && [ "$responseFirstWord" != "MASTERDOWN" ]; then
      echo "$response"
      exit 1
    fi
  ping_sentinel.sh: |-
    #!/bin/bash
    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -n "$REDIS_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_PASSWORD"
    response=$(
      timeout -s 3 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_SENTINEL_PORT \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  parse_sentinels.awk: |-
    /ip/ {FOUND_IP=1}
    /port/ {FOUND_PORT=1}
    /runid/ {FOUND_RUNID=1}
    !/ip|port|runid/ {
      if (FOUND_IP==1) {
        IP=$1; FOUND_IP=0;
      }
      else if (FOUND_PORT==1) {
        PORT=$1;
        FOUND_PORT=0;
      } else if (FOUND_RUNID==1) {
        printf "\nsentinel known-sentinel mirror %s %s %s", IP, PORT, $0; FOUND_RUNID=0;
      }
    }
  ping_readiness_master.sh: |-
    #!/bin/bash

    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    [[ -n "$REDIS_MASTER_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_MASTER_PASSWORD"
    response=$(
      timeout -s 3 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_master.sh: |-
    #!/bin/bash

    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    [[ -n "$REDIS_MASTER_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_MASTER_PASSWORD"
    response=$(
      timeout -s 3 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    responseFirstWord=$(echo $response | head -n1 | awk '{print $1;}')
    if [ "$response" != "PONG" ] && [ "$responseFirstWord" != "LOADING" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_readiness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_readiness_master.sh" $1 || exit_status=$?
    exit $exit_status
  ping_liveness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_liveness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_liveness_master.sh" $1 || exit_status=$?
    exit $exit_status
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/redis/templates/scripts-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: mirror-1-redis-scripts
  namespace: "default"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-17.3.7
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
data:
  start-node.sh: |
    #!/bin/bash

    . /opt/bitnami/scripts/libos.sh
    . /opt/bitnami/scripts/liblog.sh
    . /opt/bitnami/scripts/libvalidations.sh

    get_port() {
        hostname="$1"
        type="$2"

        port_var=$(echo "${hostname^^}_SERVICE_PORT_$type" | sed "s/-/_/g")
        port=${!port_var}

        if [ -z "$port" ]; then
            case $type in
                "SENTINEL")
                    echo 26379
                    ;;
                "REDIS")
                    echo 6379
                    ;;
            esac
        else
            echo $port
        fi
    }

    get_full_hostname() {
        hostname="$1"
        echo "${hostname}.${HEADLESS_SERVICE}"
    }

    REDISPORT=$(get_port "$HOSTNAME" "REDIS")

    HEADLESS_SERVICE="mirror-1-redis-headless.default.svc.cluster.local"

    if [ -n "$REDIS_EXTERNAL_MASTER_HOST" ]; then
        REDIS_SERVICE="$REDIS_EXTERNAL_MASTER_HOST"
    else
        REDIS_SERVICE="mirror-1-redis.default.svc.cluster.local"
    fi

    SENTINEL_SERVICE_PORT=$(get_port "mirror-1-redis" "TCP_SENTINEL")
    validate_quorum() {
        if is_boolean_yes "$REDIS_TLS_ENABLED"; then
            quorum_info_command="REDISCLI_AUTH="\$REDIS_PASSWORD" redis-cli -h $REDIS_SERVICE -p $SENTINEL_SERVICE_PORT --tls --cert ${REDIS_TLS_CERT_FILE} --key ${REDIS_TLS_KEY_FILE} --cacert ${REDIS_TLS_CA_FILE} sentinel master mirror"
        else
            quorum_info_command="REDISCLI_AUTH="\$REDIS_PASSWORD" redis-cli -h $REDIS_SERVICE -p $SENTINEL_SERVICE_PORT sentinel master mirror"
        fi
        info "about to run the command: $quorum_info_command"
        eval $quorum_info_command | grep -Fq "s_down"
    }

    trigger_manual_failover() {
        if is_boolean_yes "$REDIS_TLS_ENABLED"; then
            failover_command="REDISCLI_AUTH="\$REDIS_PASSWORD" redis-cli -h $REDIS_SERVICE -p $SENTINEL_SERVICE_PORT --tls --cert ${REDIS_TLS_CERT_FILE} --key ${REDIS_TLS_KEY_FILE} --cacert ${REDIS_TLS_CA_FILE} sentinel failover mirror"
        else
            failover_command="REDISCLI_AUTH="\$REDIS_PASSWORD" redis-cli -h $REDIS_SERVICE -p $SENTINEL_SERVICE_PORT sentinel failover mirror"
        fi

        info "about to run the command: $failover_command"
        eval $failover_command
    }

    get_sentinel_master_info() {
        if is_boolean_yes "$REDIS_TLS_ENABLED"; then
            sentinel_info_command="REDISCLI_AUTH="\$REDIS_PASSWORD" timeout 220 redis-cli -h $REDIS_SERVICE -p $SENTINEL_SERVICE_PORT --tls --cert ${REDIS_TLS_CERT_FILE} --key ${REDIS_TLS_KEY_FILE} --cacert ${REDIS_TLS_CA_FILE} sentinel get-master-addr-by-name mirror"
        else
            sentinel_info_command="REDISCLI_AUTH="\$REDIS_PASSWORD" timeout 220 redis-cli -h $REDIS_SERVICE -p $SENTINEL_SERVICE_PORT sentinel get-master-addr-by-name mirror"
        fi

        info "about to run the command: $sentinel_info_command"
        eval $sentinel_info_command
    }

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"

    # check if there is a master
    master_in_persisted_conf="$(get_full_hostname "$HOSTNAME")"
    master_port_in_persisted_conf="$REDIS_MASTER_PORT_NUMBER"
    master_in_sentinel="$(get_sentinel_master_info)"
    redisRetVal=$?
    if [[ -f /opt/bitnami/redis-sentinel/etc/sentinel.conf ]]; then
        master_in_persisted_conf="$(awk '/monitor/ {print $4}' /opt/bitnami/redis-sentinel/etc/sentinel.conf)"
        master_port_in_persisted_conf="$(awk '/monitor/ {print $5}' /opt/bitnami/redis-sentinel/etc/sentinel.conf)"
        info "Found previous master ${master_in_persisted_conf}:${master_port_in_persisted_conf} in /opt/bitnami/redis-sentinel/etc/sentinel.conf"
        debug "$(cat /opt/bitnami/redis-sentinel/etc/sentinel.conf | grep monitor)"
        touch /opt/bitnami/redis-sentinel/etc/.node_read
    fi

    if [[ $redisRetVal -ne 0 ]]; then
        if [[ "$master_in_persisted_conf" == "$(get_full_hostname "$HOSTNAME")" ]]; then
            # Case 1: No active sentinel and in previous sentinel.conf we were the master --> MASTER
            info "Configuring the node as master"
            export REDIS_REPLICATION_MODE="master"
        else
            # Case 2: No active sentinel and in previous sentinel.conf we were not master --> REPLICA
            info "Configuring the node as replica"
            export REDIS_REPLICATION_MODE="replica"
            REDIS_MASTER_HOST=${master_in_persisted_conf}
            REDIS_MASTER_PORT_NUMBER=${master_port_in_persisted_conf}
        fi
    else
        # Fetches current master's host and port
        REDIS_SENTINEL_INFO=($(get_sentinel_master_info))
        info "Current master: REDIS_SENTINEL_INFO=(${REDIS_SENTINEL_INFO[0]},${REDIS_SENTINEL_INFO[1]})"
        REDIS_MASTER_HOST=${REDIS_SENTINEL_INFO[0]}
        REDIS_MASTER_PORT_NUMBER=${REDIS_SENTINEL_INFO[1]}

        if [[ "$REDIS_MASTER_HOST" == "$(get_full_hostname "$HOSTNAME")" ]]; then
            # Case 3: Active sentinel and master it is this node --> MASTER
            info "Configuring the node as master"
            export REDIS_REPLICATION_MODE="master"
        else
            # Case 4: Active sentinel and master is not this node --> REPLICA
            info "Configuring the node as replica"
            export REDIS_REPLICATION_MODE="replica"
        fi
    fi

    if [[ -n "$REDIS_EXTERNAL_MASTER_HOST" ]]; then
      REDIS_MASTER_HOST="$REDIS_EXTERNAL_MASTER_HOST"
      REDIS_MASTER_PORT_NUMBER="${REDIS_EXTERNAL_MASTER_PORT}"
    fi

    if [[ -f /opt/bitnami/redis/mounted-etc/replica.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/replica.conf /opt/bitnami/redis/etc/replica.conf
    fi

    if [[ -f /opt/bitnami/redis/mounted-etc/redis.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf
    fi

    echo "" >> /opt/bitnami/redis/etc/replica.conf
    echo "replica-announce-port $REDISPORT" >> /opt/bitnami/redis/etc/replica.conf
    echo "replica-announce-ip $(get_full_hostname "$HOSTNAME")" >> /opt/bitnami/redis/etc/replica.conf
    ARGS=("--port" "${REDIS_PORT}")

    if [[ "$REDIS_REPLICATION_MODE" = "slave" ]] || [[ "$REDIS_REPLICATION_MODE" = "replica" ]]; then
        ARGS+=("--replicaof" "${REDIS_MASTER_HOST}" "${REDIS_MASTER_PORT_NUMBER}")
    fi
    ARGS+=("--requirepass" "${REDIS_PASSWORD}")
    ARGS+=("--masterauth" "${REDIS_MASTER_PASSWORD}")
    ARGS+=("--include" "/opt/bitnami/redis/etc/replica.conf")
    ARGS+=("--include" "/opt/bitnami/redis/etc/redis.conf")
    exec redis-server "${ARGS[@]}"

  start-sentinel.sh: |
    #!/bin/bash

    . /opt/bitnami/scripts/libos.sh
    . /opt/bitnami/scripts/libvalidations.sh
    . /opt/bitnami/scripts/libfile.sh

    HEADLESS_SERVICE="mirror-1-redis-headless.default.svc.cluster.local"
    REDIS_SERVICE="mirror-1-redis.default.svc.cluster.local"

    get_port() {
        hostname="$1"
        type="$2"

        port_var=$(echo "${hostname^^}_SERVICE_PORT_$type" | sed "s/-/_/g")
        port=${!port_var}

        if [ -z "$port" ]; then
            case $type in
                "SENTINEL")
                    echo 26379
                    ;;
                "REDIS")
                    echo 6379
                    ;;
            esac
        else
            echo $port
        fi
    }

    get_full_hostname() {
        hostname="$1"
        echo "${hostname}.${HEADLESS_SERVICE}"
    }

    SERVPORT=$(get_port "$HOSTNAME" "SENTINEL")
    REDISPORT=$(get_port "$HOSTNAME" "REDIS")
    SENTINEL_SERVICE_PORT=$(get_port "mirror-1-redis" "TCP_SENTINEL")

    sentinel_conf_set() {
        local -r key="${1:?missing key}"
        local value="${2:-}"

        # Sanitize inputs
        value="${value//\\/\\\\}"
        value="${value//&/\\&}"
        value="${value//\?/\\?}"
        [[ "$value" = "" ]] && value="\"$value\""

        replace_in_file "/opt/bitnami/redis-sentinel/etc/sentinel.conf" "^#*\s*${key} .*" "${key} ${value}" false
    }
    sentinel_conf_add() {
        echo $'\n'"$@" >> "/opt/bitnami/redis-sentinel/etc/sentinel.conf"
    }
    host_id() {
        echo "$1" | openssl sha1 | awk '{print $2}'
    }
    get_sentinel_master_info() {
        if is_boolean_yes "$REDIS_SENTINEL_TLS_ENABLED"; then
            sentinel_info_command="REDISCLI_AUTH="\$REDIS_PASSWORD" timeout 220 redis-cli -h $REDIS_SERVICE -p $SENTINEL_SERVICE_PORT --tls --cert ${REDIS_SENTINEL_TLS_CERT_FILE} --key ${REDIS_SENTINEL_TLS_KEY_FILE} --cacert ${REDIS_SENTINEL_TLS_CA_FILE} sentinel get-master-addr-by-name mirror"
        else
            sentinel_info_command="REDISCLI_AUTH="\$REDIS_PASSWORD" timeout 220 redis-cli -h $REDIS_SERVICE -p $SENTINEL_SERVICE_PORT sentinel get-master-addr-by-name mirror"
        fi
        info "about to run the command: $sentinel_info_command"
        eval $sentinel_info_command
    }

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"

    master_in_persisted_conf="$(get_full_hostname "$HOSTNAME")"
    if [[ -f /opt/bitnami/redis-sentinel/etc/sentinel.conf ]]; then
        check_lock_file() {
            [[ -f /opt/bitnami/redis-sentinel/etc/.node_read ]]
        }
        retry_while "check_lock_file"
        rm -f /opt/bitnami/redis-sentinel/etc/.node_read
        master_in_persisted_conf="$(awk '/monitor/ {print $4}' /opt/bitnami/redis-sentinel/etc/sentinel.conf)"
        info "Found previous master $master_in_persisted_conf in /opt/bitnami/redis-sentinel/etc/sentinel.conf"
        debug "$(cat /opt/bitnami/redis-sentinel/etc/sentinel.conf | grep monitor)"
    fi
    if ! get_sentinel_master_info && [[ "$master_in_persisted_conf" == "$(get_full_hostname "$HOSTNAME")" ]]; then
        # No master found, lets create a master node
        export REDIS_REPLICATION_MODE="master"

        REDIS_MASTER_HOST=$(get_full_hostname "$HOSTNAME")
        REDIS_MASTER_PORT_NUMBER="$REDISPORT"
    else
        export REDIS_REPLICATION_MODE="replica"

        # Fetches current master's host and port
        REDIS_SENTINEL_INFO=($(get_sentinel_master_info))
        info "printing REDIS_SENTINEL_INFO=(${REDIS_SENTINEL_INFO[0]},${REDIS_SENTINEL_INFO[1]})"
        REDIS_MASTER_HOST=${REDIS_SENTINEL_INFO[0]}
        REDIS_MASTER_PORT_NUMBER=${REDIS_SENTINEL_INFO[1]}
    fi

    if [[ -n "$REDIS_EXTERNAL_MASTER_HOST" ]]; then
      REDIS_MASTER_HOST="$REDIS_EXTERNAL_MASTER_HOST"
      REDIS_MASTER_PORT_NUMBER="${REDIS_EXTERNAL_MASTER_PORT}"
    fi

    cp /opt/bitnami/redis-sentinel/mounted-etc/sentinel.conf /opt/bitnami/redis-sentinel/etc/sentinel.conf
    printf "\nsentinel auth-pass %s %s" "mirror" "$REDIS_PASSWORD" >> /opt/bitnami/redis-sentinel/etc/sentinel.conf
    printf "\nrequirepass %s" "$REDIS_PASSWORD" >> /opt/bitnami/redis-sentinel/etc/sentinel.conf
    printf "\nsentinel myid %s" "$(host_id "$HOSTNAME")" >> /opt/bitnami/redis-sentinel/etc/sentinel.conf

    if [[ -z "$REDIS_MASTER_HOST" ]] || [[ -z "$REDIS_MASTER_PORT_NUMBER" ]]
    then
        # Prevent incorrect configuration to be written to sentinel.conf
        error "Redis master host is configured incorrectly (host: $REDIS_MASTER_HOST, port: $REDIS_MASTER_PORT_NUMBER)"
        exit 1
    fi

    sentinel_conf_set "sentinel monitor" "mirror "$REDIS_MASTER_HOST" "$REDIS_MASTER_PORT_NUMBER" 2"

    add_known_sentinel() {
        hostname="$1"
        ip="$2"

        if [[ -n "$hostname" && -n "$ip" && "$hostname" != "$HOSTNAME" ]]; then
            sentinel_conf_add "sentinel known-sentinel mirror $(get_full_hostname "$hostname") $(get_port "$hostname" "SENTINEL") $(host_id "$hostname")"
        fi
    }
    add_known_replica() {
        hostname="$1"
        ip="$2"

        if [[ -n "$ip" && "$(get_full_hostname "$hostname")" != "$REDIS_MASTER_HOST" ]]; then
            sentinel_conf_add "sentinel known-replica mirror $(get_full_hostname "$hostname") $(get_port "$hostname" "REDIS")"
        fi
    }

    # Add available hosts on the network as known replicas & sentinels
    for node in $(seq 0 $((1-1))); do
        hostname="mirror-1-redis-node-$node"
        ip="$(getent hosts "$hostname.$HEADLESS_SERVICE" | awk '{ print $1 }')"
        add_known_sentinel "$hostname" "$ip"
        add_known_replica "$hostname" "$ip"
    done

    echo "" >> /opt/bitnami/redis-sentinel/etc/sentinel.conf
    echo "sentinel announce-hostnames yes" >> /opt/bitnami/redis-sentinel/etc/sentinel.conf
    echo "sentinel resolve-hostnames yes" >> /opt/bitnami/redis-sentinel/etc/sentinel.conf
    echo "sentinel announce-port $SERVPORT" >> /opt/bitnami/redis-sentinel/etc/sentinel.conf
    echo "sentinel announce-ip $(get_full_hostname "$HOSTNAME")" >> /opt/bitnami/redis-sentinel/etc/sentinel.conf
    exec redis-server /opt/bitnami/redis-sentinel/etc/sentinel.conf --sentinel
  prestop-sentinel.sh: |
    #!/bin/bash

    . /opt/bitnami/scripts/libvalidations.sh
    . /opt/bitnami/scripts/libos.sh

    HEADLESS_SERVICE="mirror-1-redis-headless.default.svc.cluster.local"
    SENTINEL_SERVICE_ENV_NAME=MIRROR_1_REDIS_SERVICE_PORT_TCP_SENTINEL
    SENTINEL_SERVICE_PORT=${!SENTINEL_SERVICE_ENV_NAME}

    get_full_hostname() {
        hostname="$1"
        echo "${hostname}.${HEADLESS_SERVICE}"
    }
    run_sentinel_command() {
        if is_boolean_yes "$REDIS_SENTINEL_TLS_ENABLED"; then
            redis-cli -h "$REDIS_SERVICE" -p "$SENTINEL_SERVICE_PORT" --tls --cert "$REDIS_SENTINEL_TLS_CERT_FILE" --key "$REDIS_SENTINEL_TLS_KEY_FILE" --cacert "$REDIS_SENTINEL_TLS_CA_FILE" sentinel "$@"
        else
            redis-cli -h "$REDIS_SERVICE" -p "$SENTINEL_SERVICE_PORT" sentinel "$@"
        fi
    }
    failover_finished() {
      REDIS_SENTINEL_INFO=($(run_sentinel_command get-master-addr-by-name "mirror"))
      REDIS_MASTER_HOST="${REDIS_SENTINEL_INFO[0]}"
      [[ "$REDIS_MASTER_HOST" != "$(get_full_hostname $HOSTNAME)" ]]
    }

    REDIS_SERVICE="mirror-1-redis.default.svc.cluster.local"

    # redis-cli automatically consumes credentials from the REDISCLI_AUTH variable
    [[ -n "$REDIS_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_PASSWORD"
    [[ -f "$REDIS_PASSWORD_FILE" ]] && export REDISCLI_AUTH="$(< "${REDIS_PASSWORD_FILE}")"

    if ! failover_finished; then
        echo "I am the master pod and you are stopping me. Starting sentinel failover"
        # if I am the master, issue a command to failover once and then wait for the failover to finish
        run_sentinel_command failover "mirror"
        if retry_while "failover_finished" "20" 1; then
            echo "Master has been successfuly failed over to a different pod."
            exit 0
        else
            echo "Master failover failed"
            exit 1
        fi
    else
        exit 0
    fi
  prestop-redis.sh: |
    #!/bin/bash

    . /opt/bitnami/scripts/libvalidations.sh
    . /opt/bitnami/scripts/libos.sh

    run_redis_command() {
        if is_boolean_yes "$REDIS_TLS_ENABLED"; then
            redis-cli -h 127.0.0.1 -p "$REDIS_TLS_PORT" --tls --cert "$REDIS_TLS_CERT_FILE" --key "$REDIS_TLS_KEY_FILE" --cacert "$REDIS_TLS_CA_FILE" "$@"
        else
            redis-cli -h 127.0.0.1 -p ${REDIS_PORT} "$@"
        fi
    }
    failover_finished() {
        REDIS_ROLE=$(run_redis_command role | head -1)
        [[ "$REDIS_ROLE" != "master" ]]
    }

    # redis-cli automatically consumes credentials from the REDISCLI_AUTH variable
    [[ -n "$REDIS_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_PASSWORD"
    [[ -f "$REDIS_PASSWORD_FILE" ]] && export REDISCLI_AUTH="$(< "${REDIS_PASSWORD_FILE}")"

    if ! failover_finished; then
        echo "Waiting for sentinel to run failover for up to 20s"
        retry_while "failover_finished" "20" 1
    else
        exit 0
    fi
---
# Source: hedera-mirror-node/charts/network-node/templates/configMapapp.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: application-root

binaryData:


  f2cc712e966feb696a4f0e6d540754849f998d15210192ebb23e3da74ec479dd: IyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMKIyBDb25maWd1cmF0aW9uIGZpbGUsIGZvciBhdXRvbWF0aWNhbGx5IHJ1bm5pbmcgbXVsdGlwbGUgaW5zdGFuY2VzCiMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjCnN3aXJsZCwgaGVkZXJhCmFwcCwgSGVkZXJhTm9kZS5qYXIKYWRkcmVzcywgbjAsIG5vZGUwLCAxLCBuZXR3b3JrLW5vZGUsIDUwMTExLCBuZXR3b3JrLW5vZGUsIDUwMTExLCAwLjAuMw==

  fff42a8c2cb099f0ccc7e617bd13806177c34b16dd863656439ef8b318c77d09: IyBDcnlwdG8KY3JlYXRlQWNjb3VudD0wLSoKY3J5cHRvVHJhbnNmZXI9MC0qCnVwZGF0ZUFjY291bnQ9MC0qCmNyeXB0b0dldEJhbGFuY2U9MC0qCmdldEFjY291bnRJbmZvPTAtKgpjcnlwdG9EZWxldGU9MC0qCmdldEFjY291bnRSZWNvcmRzPTAtKgpnZXRUeFJlY29yZEJ5VHhJRD0wLSoKZ2V0VHJhbnNhY3Rpb25SZWNlaXB0cz0wLSoKYXBwcm92ZUFsbG93YW5jZXM9MC0qCmRlbGV0ZUFsbG93YW5jZXM9MC0qCiMgRmlsZQpjcmVhdGVGaWxlPTAtKgp1cGRhdGVGaWxlPTAtKgpkZWxldGVGaWxlPTAtKgphcHBlbmRDb250ZW50PTAtKgpnZXRGaWxlQ29udGVudD0wLSoKZ2V0RmlsZUluZm89MC0qCiMgQ29udHJhY3QKY3JlYXRlQ29udHJhY3Q9MC0qCnVwZGF0ZUNvbnRyYWN0PTAtKgpjb250cmFjdENhbGxNZXRob2Q9MC0qCmdldENvbnRyYWN0SW5mbz0wLSoKY29udHJhY3RDYWxsTG9jYWxNZXRob2Q9MC0qCmNvbnRyYWN0R2V0Qnl0ZWNvZGU9MC0qCmdldFR4UmVjb3JkQnlDb250cmFjdElEPTAtKgpkZWxldGVDb250cmFjdD0wLSoKIyBDb25zZW5zdXMKY3JlYXRlVG9waWM9MC0qCnVwZGF0ZVRvcGljPTAtKgpkZWxldGVUb3BpYz0wLSoKc3VibWl0TWVzc2FnZT0wLSoKZ2V0VG9waWNJbmZvPTAtKgojIEV0aGVyZXVtCmV0aGVyZXVtVHJhbnNhY3Rpb249MC0qCiMgU2NoZWR1bGluZwpzY2hlZHVsZUNyZWF0ZT0wLSoKc2NoZWR1bGVTaWduPTAtKgpzY2hlZHVsZURlbGV0ZT0wLSoKc2NoZWR1bGVHZXRJbmZvPTAtKgojIFRva2VuCnRva2VuQ3JlYXRlPTAtKgp0b2tlbkZyZWV6ZUFjY291bnQ9MC0qCnRva2VuVW5mcmVlemVBY2NvdW50PTAtKgp0b2tlbkdyYW50S3ljVG9BY2NvdW50PTAtKgp0b2tlblJldm9rZUt5Y0Zyb21BY2NvdW50PTAtKgp0b2tlbkRlbGV0ZT0wLSoKdG9rZW5NaW50PTAtKgp0b2tlbkJ1cm49MC0qCnRva2VuQWNjb3VudFdpcGU9MC0qCnRva2VuVXBkYXRlPTAtKgp0b2tlbkdldEluZm89MC0qCnRva2VuR2V0TmZ0SW5mbz0wLSoKdG9rZW5HZXROZnRJbmZvcz0wLSoKdG9rZW5HZXRBY2NvdW50TmZ0SW5mb3M9MC0qCnRva2VuQXNzb2NpYXRlVG9BY2NvdW50PTAtKgp0b2tlbkRpc3NvY2lhdGVGcm9tQWNjb3VudD0wLSoKdG9rZW5GZWVTY2hlZHVsZVVwZGF0ZT0wLSoKdG9rZW5QYXVzZT0wLSoKdG9rZW5VbnBhdXNlPTAtKgojIE5ldHdvcmsgCmdldFZlcnNpb25JbmZvPTAtKgpuZXR3b3JrR2V0RXhlY3V0aW9uVGltZT0yLTUwCnN5c3RlbURlbGV0ZT0yLTU5CnN5c3RlbVVuZGVsZXRlPTItNjAKZnJlZXplPTItNTgKZ2V0QWNjb3VudERldGFpbHM9Mi01MAojIFV0aWwKdXRpbFBybmc9MC0q

  6d00b1694ca05c63de005f98ad3c11513f068198ac965eabdef601b59c2ba468: YXV0b1JlbmV3LnRhcmdldFR5cGVzPQo=

  afed3074a19539c939a88c6c8e0af1a497410b7d54cf3f80a9d1ed17e40c6569: bGVkZ2VyLmlkPTB4MDEKbmV0dHkubW9kZT1ERVYKY29udHJhY3RzLmNoYWluSWQ9Mjk4CmhlZGVyYS5yZWNvcmRTdHJlYW0ubG9nUGVyaW9kPTEKYmFsYW5jZXMuZXhwb3J0UGVyaW9kU2Vjcz00MDAKZmlsZXMubWF4U2l6ZUtiPTIwNDgK

  0225d92109512382e4db92bc9ab1a7c52ae6f3c1b433ba58cd45b083a67733c5: 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

  29d12e603d75d76c087739c53873fb50028166a8e846d40dc027719f7a809977: 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

  8eb93144b5258363c7257da297fa50f0c9c52cef301274d61f1eff3dcc2dc0e8: MzAyZTAyMDEwMDMwMDUwNjAzMmI2NTcwMDQyMjA0MjA5MTEzMjE3OGU3MjA1N2ExZDc1MjgwMjU5NTZmZTM5YjBiODQ3ZjIwMGFiNTliMmZkZDM2NzAxN2YzMDg3MTM3

  3dce3e857c64de1041e1381126d72e253358bc068548babf913422a635d5f741: MzAyYTMwMDUwNjAzMmI2NTcwMDMyMTAwMGFhOGUyMTA2NGM2MWVhYjg2ZTJhOWMxNjQ1NjViNGU3YTlhNDE0NjEwNmUwYTZjZDAzYThjMzk1YTExMGU5Mg==

  80d384465a899703c90c247667ba88cf4faaa792eb0e019b1550d1c63a1de1da: ck8wQUJYTnlBQkZxWVhaaExuVjBhV3d1UTI5c2JGTmxjbGVPcTdZNkc2Z1JBd0FCU1FBRGRHRm5lSEFBQUFBRGR3UUFBQUFDZEFBTlUxUkJVbFJmUVVORFQxVk9WSE54QUg0QUFBQUFBQUYzQkFBQUFBRnpjZ0F4WTI5dExtaGxaR1Z5WVM1elpYSjJhV05sY3k1c1pXZGhZM2t1WTI5eVpTNUJZMk52ZFc1MFMyVjVUR2x6ZEU5aWFzS0dwQkJTMUVlYkFnQUNUQUFKWVdOamIzVnVkRWxrZEFBdVRHTnZiUzlvWldSbGNtRm9ZWE5vWjNKaGNHZ3ZZWEJwTDNCeWIzUnZMMnBoZG1FdlFXTmpiM1Z1ZEVsRU8wd0FDMnRsZVZCaGFYSk1hWE4wZEFBUVRHcGhkbUV2ZFhScGJDOU1hWE4wTzNod2MzSUFOMk52YlM1bmIyOW5iR1V1Y0hKdmRHOWlkV1l1UjJWdVpYSmhkR1ZrVFdWemMyRm5aVXhwZEdVa1UyVnlhV0ZzYVhwbFpFWnZjbTBBQUFBQUFBQUFBQUlBQTFzQUIyRnpRbmwwWlhOMEFBSmJRa3dBREcxbGMzTmhaMlZEYkdGemMzUUFFVXhxWVhaaEwyeGhibWN2UTJ4aGMzTTdUQUFRYldWemMyRm5aVU5zWVhOelRtRnRaWFFBRWt4cVlYWmhMMnhoYm1jdlUzUnlhVzVuTzNod2RYSUFBbHRDclBNWCtBWUlWT0FDQUFCNGNBQUFBQUlZQW5aeUFDeGpiMjB1YUdWa1pYSmhhR0Z6YUdkeVlYQm9MbUZ3YVM1d2NtOTBieTVxWVhaaExrRmpZMjkxYm5SSlJBQUFBQUFBQUFBQUFnQUVTZ0FMWVdOamIzVnVkRTUxYlY5Q0FCVnRaVzF2YVhwbFpFbHpTVzVwZEdsaGJHbDZaV1JLQUFseVpXRnNiVTUxYlY5S0FBbHphR0Z5WkU1MWJWOTRjZ0FtWTI5dExtZHZiMmRzWlM1d2NtOTBiMkoxWmk1SFpXNWxjbUYwWldSTlpYTnpZV2RsVmpNQUFBQUFBQUFBQVFJQUFVd0FEWFZ1YTI1dmQyNUdhV1ZzWkhOMEFDVk1ZMjl0TDJkdmIyZHNaUzl3Y205MGIySjFaaTlWYm10dWIzZHVSbWxsYkdSVFpYUTdlSEIwQUN4amIyMHVhR1ZrWlhKaGFHRnphR2R5WVhCb0xtRndhUzV3Y205MGJ5NXFZWFpoTGtGalkyOTFiblJKUkhOeEFINEFBQUFBQUFGM0JBQUFBQUZ6Y2dBcVkyOXRMbWhsWkdWeVlTNXpaWEoyYVdObGN5NXNaV2RoWTNrdVkyOXlaUzVMWlhsUVlXbHlUMkpxZnU1ME1JRFlWc2NDQUFOTUFBZHdjbWwyUzJWNWRBQWFUR3BoZG1FdmMyVmpkWEpwZEhrdlVISnBkbUYwWlV0bGVUdE1BQXB3Y21sMllYUmxTMlY1Y1FCK0FBdE1BQWx3ZFdKc2FXTkxaWGx4QUg0QUMzaHdjSFFBWURNd01tVXdNakF4TURBek1EQTFNRFl3TXpKaU5qVTNNREEwTWpJd05ESXdPVEV4TXpJeE56aGxOekl3TlRkaE1XUTNOVEk0TURJMU9UVTJabVV6T1dJd1lqZzBOMll5TURCaFlqVTVZakptWkdRek5qY3dNVGRtTXpBNE56RXpOM1FBV0RNd01tRXpNREExTURZd016SmlOalUzTURBek1qRXdNREJoWVRobE1qRXdOalJqTmpGbFlXSTRObVV5WVRsak1UWTBOVFkxWWpSbE4yRTVZVFF4TkRZeE1EWmxNR0UyWTJRd00yRTRZek01TldFeE1UQmxPVEo0ZUhnPQ==

  a31d7b72ab811ab8682b550efdc57504a8aa413f85d9e278b56a9df3fb6f67b8: ChIKCTEyNy4wLjAuMRoFMC4wLjM=

  9f408ac0f831a62adad92ae9fe1790de78fc9108eedc7a5f39d83a45c5f491f8: LS0tLS1CRUdJTiBFTkNSWVBURUQgUFJJVkFURSBLRVktLS0tLQpNSUhETUg4R0NTcUdTSWIzRFFFRkRUQnlNRkVHQ1NxR1NJYjNEUUVGRERCRUJERHZmWDJieXczZ2o1bUlYVndxCnlQSUNoS0hwVmF4c0NQaXFJY1BwbVpvNFgyREd1TktKTmxJc0EvMzFlVllkYXRNQ0FpY1FNQXdHQ0NxR1NJYjMKRFFJS0JRQXdIUVlKWUlaSUFXVURCQUVxQkJEUzJ6T3hOYWkvVEVaWm1PcHVWajFRQkVDSUdET2VQYjlIZFBxbgp1d3UzZXhyWmhWMXFycWR4ajFPV0ZmUUJKdFRRVHZXbEpDVWtGUGxmb2l6cThvOTBNY0hWdHl4NVJNVUV3WUwrCjJHL0lOdm1CCi0tLS0tRU5EIEVOQ1JZUFRFRCBQUklWQVRFIEtFWS0tLS0tCg==

  51cdc88d75ce7fe2a721cf1dd701216852c6530bf2baa0e3072ebd261696fcee: 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

  db77ec4dce1ccb420185a2458b815dfeedef26e67a652ac9229d35b7693e0d37: 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

  2250858faf37c92e1f4533f266195ed6431d45c28071ec802b1bf7444db9e3a2: <?xml version="1.0" encoding="UTF-8"?>
<!-- monitorInterval="600" , if any change to log level will be effective after 10 minute -->
<Configuration status="WARN" monitorInterval="600">
  <Appenders>
    <Console name="Console" target="SYSTEM_OUT">
      <PatternLayout pattern="%d{yyyy-MM-dd HH:mm:ss.SSS} %-5p %-4L %c{1} - %m{nolookups}%n"/>
    </Console>

    <RollingFile name="RollingFile" fileName="output/hgcaa.log"
                 filePattern="output/hgcaa.log-%d{yyyy-MM-dd}-%i.log">
      <PatternLayout>
        <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} %-5p %-4L %c{1} - %m{nolookups}%n</pattern>
      </PatternLayout>
      <Policies>
        <SizeBasedTriggeringPolicy size="100 MB"/>
      </Policies>
      <DefaultRolloverStrategy max="10"/>
    </RollingFile>

    <RollingFile name="QueriesRollingFile" fileName="output/queries.log"
                 filePattern="output/queries.log-%d{yyyy-MM-dd}-%i.log">
      <BurstFilter level="INFO" rate="50" maxBurst="500"/>
      <PatternLayout>
        <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} %-5p %-4L %c{1} - %m{nolookups}%n</pattern>
      </PatternLayout>
      <Policies>
        <SizeBasedTriggeringPolicy size="100 MB"/>
      </Policies>
      <DefaultRolloverStrategy max="10"/>
    </RollingFile>

    <RollingFile name="fileLog" fileName="output/swirlds.log"
                 filePattern="output/swirlds.log-%d{yyyy-MM-dd}-%i.log">
      <PatternLayout>
        <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} %-8sn %-5p %-16marker &lt;%t&gt; %c{1}: %msg{nolookups}%n</pattern>
      </PatternLayout>
      <Policies>
        <SizeBasedTriggeringPolicy size="100 MB"/>
      </Policies>
      <DefaultRolloverStrategy max="10"/>
    </RollingFile>

    <RollingFile name="vMapLog" fileName="output/swirlds-vmap.log"
                 filePattern="output/swirlds-vmap.log-%d{yyyy-MM-dd}-%i.log">
      <PatternLayout>
        <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} %-8sn %-5p %-16marker &lt;%t&gt; %c{1}: %msg{nolookups}%n</pattern>
      </PatternLayout>
      <Policies>
        <SizeBasedTriggeringPolicy size="100 MB"/>
      </Policies>
      <DefaultRolloverStrategy max="10"/>
    </RollingFile>

  </Appenders>
  <Loggers>
    <Root level="FATAL">
      <!-- <AppenderRef ref="Console"/> -->
      <AppenderRef ref="fileLog"/>
    </Root>

    <Logger name="com.swirlds" level="INFO" additivity="false">
      <AppenderRef ref="fileLog">
        <Filters>
          <!-- JasperDB & Virtual Merkle -->
          <MarkerFilter marker="JASPER_DB"              onMatch="DENY" onMismatch="NEUTRAL"/>
          <MarkerFilter marker="VIRTUAL_MERKLE_STATS"   onMatch="DENY" onMismatch="NEUTRAL"/>
        </Filters>
      </AppenderRef>

      <AppenderRef ref="vMapLog">
        <Filters>
          <!-- JasperDB & Virtual Merkle -->
          <MarkerFilter marker="JASPER_DB"              onMatch="ACCEPT" onMismatch="NEUTRAL"/>
          <MarkerFilter marker="VIRTUAL_MERKLE_STATS"   onMatch="ACCEPT" onMismatch="NEUTRAL"/>
          <MarkerFilter marker="DISABLED"               onMatch="DENY"   onMismatch="DENY" />
        </Filters>
      </AppenderRef>

      <!--
	  Due to known log4j2 issues with how Markers and LogLevels are evaluated there must be a top level <Filter> element
	  to ensure that the root logger does not execute all the lambda arguments erroneously. Potential work around in the
	  future is to use a top-level <Filter> and <Logger> specific filters in combination to achieve the desired
	  multi-logger setup for diagnostic logging.
	  -->
      <Filters>
        <!-- Filter out levels above INFO (ex: DEBUG & TRACE) -->
        <!-- Intentionally left disabled by default -->
        <!-- <ThresholdFilter level="INFO"                 onMatch="NEUTRAL" onMismatch="DENY" />-->

        <!-- In the following, enable a marker with onMatch="ACCEPT" and disable with onMatch="DENY". -->
        <!-- More markers can be added, but ensure that every onMismatch="NEUTRAL", except the last is "DENY". -->

        <!-- Exceptions -->
        <MarkerFilter marker="EXCEPTION"              onMatch="ACCEPT" onMismatch="NEUTRAL"/>
        <MarkerFilter marker="TESTING_EXCEPTIONS"     onMatch="ACCEPT" onMismatch="NEUTRAL"/>
        <MarkerFilter marker="SOCKET_EXCEPTIONS"      onMatch="ACCEPT" onMismatch="NEUTRAL"/>
        <MarkerFilter marker="TCP_CONNECT_EXCEPTIONS" onMatch="DENY"   onMismatch="NEUTRAL"/>

        <!-- Errors -->
        <MarkerFilter marker="INVALID_EVENT_ERROR"    onMatch="ACCEPT" onMismatch="NEUTRAL"/>

        <!-- Synchronization/Gossip (Debug) -->
        <MarkerFilter marker="SYNC_START"             onMatch="DENY"   onMismatch="NEUTRAL"/>
        <MarkerFilter marker="SYNC_DONE"              onMatch="DENY"   onMismatch="NEUTRAL"/>
        <MarkerFilter marker="SYNC_ERROR"             onMatch="DENY"   onMismatch="NEUTRAL"/>
        <MarkerFilter marker="SYNC"                   onMatch="DENY"   onMismatch="NEUTRAL"/>
        <MarkerFilter marker="HEARTBEAT"              onMatch="DENY"   onMismatch="NEUTRAL"/>

        <!-- Platform Events (Debug) -->
        <MarkerFilter marker="CREATE_EVENT"           onMatch="DENY"   onMismatch="NEUTRAL"/>
        <MarkerFilter marker="INTAKE_EVENT"           onMatch="DENY"   onMismatch="NEUTRAL"/>
        <MarkerFilter marker="WATCH_EVENTS_SEND_REC"  onMatch="DENY"   onMismatch="NEUTRAL"/>
        <MarkerFilter marker="EVENT_SIG"              onMatch="DENY"   onMismatch="NEUTRAL"/>
        <MarkerFilter marker="EVENT_STREAM"           onMatch="DENY"   onMismatch="NEUTRAL"/>
        <MarkerFilter marker="EVENT_RESTART"          onMatch="DENY"   onMismatch="NEUTRAL"/>
        <MarkerFilter marker="STALE_EVENTS"           onMatch="DENY"   onMismatch="NEUTRAL"/>
        <MarkerFilter marker="EVENT_PARSER"           onMatch="DENY"   onMismatch="NEUTRAL"/>
        <MarkerFilter marker="EVENT_CONTENT"          onMatch="DENY"   onMismatch="NEUTRAL"/>

        <!-- Queues/Certificates/Utilities -->
        <MarkerFilter marker="QUEUES"                 onMatch="DENY"   onMismatch="NEUTRAL"/>
        <MarkerFilter marker="CERTIFICATES"           onMatch="DENY"   onMismatch="NEUTRAL"/>
        <MarkerFilter marker="LOCKS"                  onMatch="DENY"   onMismatch="NEUTRAL"/>
        <MarkerFilter marker="TIME_MEASURE"           onMatch="DENY"   onMismatch="NEUTRAL"/>

        <!-- Signed State Signatures -->
        <MarkerFilter marker="STATE_SIG_DIST"         onMatch="DENY"   onMismatch="NEUTRAL"/>
        <MarkerFilter marker="STATE_DELETER"          onMatch="DENY"   onMismatch="NEUTRAL"/>
        <MarkerFilter marker="OBJECT_STREAM_DETAIL"   onMatch="DENY"   onMismatch="NEUTRAL"/>

        <!-- Cryptography -->
        <MarkerFilter marker="OPENCL_INIT_EXCEPTIONS" onMatch="DENY"   onMismatch="NEUTRAL"/>
        <MarkerFilter marker="ADV_CRYPTO_SYSTEM"      onMatch="DENY"   onMismatch="NEUTRAL"/>

        <!-- Startup/Restart/Reconnect -->
        <MarkerFilter marker="STARTUP"                onMatch="ACCEPT" onMismatch="NEUTRAL"/>
        <MarkerFilter marker="PLATFORM_STATUS"        onMatch="ACCEPT" onMismatch="NEUTRAL"/>
        <MarkerFilter marker="RECONNECT"              onMatch="ACCEPT" onMismatch="NEUTRAL"/>
        <MarkerFilter marker="FREEZE"                 onMatch="ACCEPT" onMismatch="NEUTRAL"/>

        <!-- Saved States -->
        <MarkerFilter marker="SNAPSHOT_MANAGER"       onMatch="ACCEPT" onMismatch="NEUTRAL"/>
        <MarkerFilter marker="STATE_TO_DISK"          onMatch="ACCEPT" onMismatch="NEUTRAL"/>

        <!-- Beta Mirror -->
        <MarkerFilter marker="BETA_MIRROR_NODE"       onMatch="ACCEPT" onMismatch="NEUTRAL"/>

        <!-- FCMap -->
        <MarkerFilter marker="FCM_COPY"               onMatch="DENY"   onMismatch="NEUTRAL"/>
        <MarkerFilter marker="FCM_COPY_FROM"          onMatch="DENY"   onMismatch="NEUTRAL"/>
        <MarkerFilter marker="FCM_COPY_TO"            onMatch="DENY"   onMismatch="NEUTRAL"/>
        <MarkerFilter marker="FCM_DEMO"               onMatch="DENY"   onMismatch="NEUTRAL"/>
        <MarkerFilter marker="FCM_COPY_FROM_DIFF"     onMatch="DENY"   onMismatch="NEUTRAL"/>
        <MarkerFilter marker="FCM_COPY_TO_DIFF"       onMatch="DENY"   onMismatch="NEUTRAL"/>
        <MarkerFilter marker="FC_SERIALIZATION"       onMatch="DENY"   onMismatch="NEUTRAL"/>

        <!-- Merkle Trees & Hashing -->
        <MarkerFilter marker="MERKLE_FORCE_FLUSH"     onMatch="DENY"   onMismatch="NEUTRAL"/>
        <MarkerFilter marker="MERKLE_HASHING"         onMatch="DENY"   onMismatch="NEUTRAL"/>
        <MarkerFilter marker="MERKLE_GENERATION"      onMatch="DENY"   onMismatch="NEUTRAL"/>
        <MarkerFilter marker="MERKLE_LOCKS"           onMatch="DENY"   onMismatch="NEUTRAL"/>

        <!-- JasperDB & Virtual Merkle -->
        <MarkerFilter marker="JASPER_DB"              onMatch="ACCEPT" onMismatch="NEUTRAL"/>
        <MarkerFilter marker="VIRTUAL_MERKLE_STATS"   onMatch="ACCEPT" onMismatch="NEUTRAL"/>

        <MarkerFilter marker="DISABLED"               onMatch="DENY"   onMismatch="DENY" />
      </Filters>
    </Logger>

    <Logger name="com.hedera" level="info" additivity="false">
      <AppenderRef ref="Console"/>
      <AppenderRef ref="RollingFile"/>
    </Logger>
    <Logger name="com.hedera.services.sigs" level="error" additivity="false">
      <AppenderRef ref="Console"/>
      <AppenderRef ref="RollingFile"/>
    </Logger>
    <Logger name="com.hedera.services.queries.answering" level="warn" additivity="false">
      <AppenderRef ref="QueriesRollingFile"/>
    </Logger>

    <Logger name="com.hedera.services.legacy" level="warn" additivity="false">
      <!-- <AppenderRef ref="Console"/> -->
      <AppenderRef ref="RollingFile"/>
    </Logger>
    <Logger name="com.hedera.services.legacy.netty" level="info" additivity="false">
      <!-- <AppenderRef ref="Console"/> -->
      <AppenderRef ref="RollingFile"/>
    </Logger>
    <Logger name="com.hedera.services.legacy.service" level="warn" additivity="false">
      <!-- <AppenderRef ref="Console"/> -->
      <AppenderRef ref="RollingFile"/>
    </Logger>
    <Logger name="com.hedera.services.legacy.services" level="warn" additivity="false">
      <!-- <AppenderRef ref="Console"/> -->
      <AppenderRef ref="RollingFile"/>
    </Logger>
    <Logger name="com.hedera.services.legacy.handler" level="warn" additivity="false">
      <!-- <AppenderRef ref="Console"/> -->
      <AppenderRef ref="RollingFile"/>
    </Logger>
    <Logger name="com.hedera.services.utils.UnzipUtility" level="info" additivity="false">
      <!-- <AppenderRef ref="Console"/> -->
      <AppenderRef ref="RollingFile"/>
    </Logger>
    <Logger name="com.hedera.services.legacy.utils" level="warn" additivity="false">
      <!-- <AppenderRef ref="Console"/> -->
      <AppenderRef ref="RollingFile"/>
    </Logger>
    <Logger name="com.hedera.services.legacy.hgcca.core" level="warn" additivity="false">
      <!-- <AppenderRef ref="Console"/> -->
      <AppenderRef ref="RollingFile"/>
    </Logger>
    <Logger name="com.hedera.services.legacy.evm" level="warn" additivity="false">
      <!-- <AppenderRef ref="Console"/> -->
      <AppenderRef ref="RollingFile"/>
    </Logger>
    <Logger name="com.hedera.services.legacy.initialization" level="warn" additivity="false">
      <!-- <AppenderRef ref="Console"/> -->
      <AppenderRef ref="RollingFile"/>
    </Logger>
    <Logger name="com.hedera.services.legacy.config" level="info" additivity="false">
      <AppenderRef ref="RollingFile"/>
      <AppenderRef ref="Console"/>
    </Logger>


    <Logger name="org.springframework" level="ERROR" additivity="false">
      <!-- <AppenderRef ref="Console"/> -->
      <AppenderRef ref="RollingFile"/>
    </Logger>
    <Logger name="state" level="WARN" additivity="false">
      <!-- <AppenderRef ref="Console"/> -->
      <AppenderRef ref="RollingFile"/>
    </Logger>
    <Logger name="trie" level="WARN" additivity="false">
      <!-- <AppenderRef ref="Console"/> -->
      <AppenderRef ref="RollingFile"/>
    </Logger>
    <Logger name="net" level="WARN" additivity="false">
      <!-- <AppenderRef ref="Console"/> -->
      <AppenderRef ref="RollingFile"/>
    </Logger>
    <Logger name="execute" level="ERROR" additivity="false">
      <!-- <AppenderRef ref="Console"/> -->
      <AppenderRef ref="RollingFile"/>
    </Logger>
    <Logger name="VM" level="ERROR" additivity="false">
      <!-- <AppenderRef ref="Console"/> -->
      <AppenderRef ref="RollingFile"/>
    </Logger>
    <Logger name="pending" level="WARN" additivity="false">
      <!-- <AppenderRef ref="Console"/> -->
      <AppenderRef ref="RollingFile"/>
    </Logger>
    <Logger name="sync" level="WARN" additivity="false">
      <!-- <AppenderRef ref="Console"/> -->
      <AppenderRef ref="RollingFile"/>
    </Logger>
    <Logger name="wire" level="ERROR" additivity="false">
      <!-- <AppenderRef ref="Console"/> -->
      <AppenderRef ref="RollingFile"/>
    </Logger>
    <Logger name="db" level="WARN" additivity="false">
      <!-- <AppenderRef ref="Console"/> -->
      <AppenderRef ref="RollingFile"/>
    </Logger>
    <Logger name="general" level="WARN" additivity="false">
      <!-- <AppenderRef ref="Console"/> -->
      <AppenderRef ref="RollingFile"/>
    </Logger>
    <Logger name="TCK-Test" level="ERROR" additivity="false">
      <!-- <AppenderRef ref="Console"/> -->
      <AppenderRef ref="RollingFile"/>
    </Logger>
    <Logger name="org.hibernate" level="ERROR" additivity="false">
      <!-- <AppenderRef ref="Console"/> -->
      <AppenderRef ref="RollingFile"/>
    </Logger>
    <Logger name="repository" level="WARN" additivity="false">
      <!-- <AppenderRef ref="Console"/> -->
      <AppenderRef ref="RollingFile"/>
    </Logger>
    <Logger name="blockchain" level="WARN" additivity="false">
      <!-- <AppenderRef ref="Console"/> -->
      <AppenderRef ref="RollingFile"/>
    </Logger>
    <Logger name="mine" level="WARN" additivity="false">
      <!-- <AppenderRef ref="Console"/> -->
      <AppenderRef ref="RollingFile"/>
    </Logger>
    <Logger name="blockqueue" level="WARN" additivity="false">
      <!-- <AppenderRef ref="Console"/> -->
      <AppenderRef ref="RollingFile"/>
    </Logger>
    <Logger name="rlp" level="ERROR" additivity="false">
      <!-- <AppenderRef ref="Console"/> -->
      <AppenderRef ref="RollingFile"/>
    </Logger>
    <Logger name="java.nio" level="ERROR" additivity="false">
      <!-- <AppenderRef ref="Console"/> -->
      <AppenderRef ref="RollingFile"/>
    </Logger>
    <Logger name="io.netty" level="ERROR" additivity="false">
      <!-- <AppenderRef ref="Console"/> -->
      <AppenderRef ref="RollingFile"/>
    </Logger>
    <Logger name="discover" level="WARN" additivity="false">
      <!-- <AppenderRef ref="Console"/> -->
      <AppenderRef ref="RollingFile"/>
    </Logger>
    <Logger name="hsqldb.db" level="ERROR" additivity="false">
      <!-- <AppenderRef ref="Console"/> -->
      <AppenderRef ref="RollingFile"/>
    </Logger>
  </Loggers>
</Configuration>

  5944af072cc85da3a45532b4b00ab484595f8a042de5798427db26ade98a1338: Y2hlY2tTaWduZWRTdGF0ZUZyb21EaXNrLCAgICAgICAgICAgICAgICAgICAgICAxCmNzdkZpbGVOYW1lLCAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgTWFpbk5ldFN0YXRzCmNzdk91dHB1dEZvbGRlciwgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgZGF0YS9zdGF0cwpkb1VwbnAsICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIGZhbHNlCmVuYWJsZUV2ZW50U3RyZWFtaW5nLCAgICAgICAgICAgICAgICAgICAgICAgICAgZmFsc2UKZXZlbnRzTG9nRGlyLCAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAvb3B0L2hnY2FwcC9ldmVudHNTdHJlYW1zCmV2ZW50c0xvZ1BlcmlvZCwgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgNQptYXhFdmVudFF1ZXVlRm9yQ29ucywgICAgICAgICAgICAgICAgICAgICAgICAgIDEwMDAKbWF4T3V0Z29pbmdTeW5jcywgICAgICAgICAgICAgICAgICAgICAgICAgICAgICA4Cm51bUNvbm5lY3Rpb25zLCAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgMTAwMApyZWNvbm5lY3QuYWN0aXZlLCAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIDEKcmVjb25uZWN0LmFzeW5jU3RyZWFtVGltZW91dE1pbGxpc2Vjb25kcywgICAgICA2MDAwMApyZWNvbm5lY3QucmVjb25uZWN0V2luZG93U2Vjb25kcywgICAgICAgICAgICAgIC0xCnNob3dJbnRlcm5hbFN0YXRzLCAgICAgICAgICAgICAgICAgICAgICAgICAgICAgMQpzdGF0ZS5yb3VuZHNFeHBpcmVkLCAgICAgICAgICAgICAgICAgICAgICAgICAgIDUwMApzdGF0ZS5zYXZlU3RhdGVQZXJpb2QsICAgICAgICAgICAgICAgICAgICAgICAgIDkwMApzdGF0ZS5zaWduZWRTdGF0ZURpc2ssICAgICAgICAgICAgICAgICAgICAgICAgIDUKc3RhdGUuc2lnbmVkU3RhdGVLZWVwLCAgICAgICAgICAgICAgICAgICAgICAgICAxMAp0aHJvdHRsZTdleHRyYSwgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIDAuNQp1c2VMb29wYmFja0lwLCAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIGZhbHNlCndhaXRBdFN0YXJ0dXAsICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgZmFsc2UKamFzcGVyRGIuc3RvcmFnZVBhdGgsICAgICAgICAgICAgICAgICAgICAgICAgICAvb3B0L2hnY2FwcC9zZXJ2aWNlcy1oZWRlcmEvSGFwaUFwcDIuMC9kYXRhL3NhdmVkCmphc3BlckRiLml0ZXJhdG9ySW5wdXRCdWZmZXJCeXRlcywgICAgICAgICAgICAgMTY3NzcyMTYK
---
# Source: hedera-mirror-node/charts/network-node/templates/configMaproot.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: application-config
binaryData:


  1202c1ef941ef443d566693d4f4fe66b232e78467b6c9c776c03790420d25b83: IyBDcnlwdG8KY3JlYXRlQWNjb3VudD0wLSoKY3J5cHRvVHJhbnNmZXI9MC0qCnVwZGF0ZUFjY291bnQ9MC0qCmNyeXB0b0dldEJhbGFuY2U9MC0qCmdldEFjY291bnRJbmZvPTAtKgpjcnlwdG9EZWxldGU9MC0qCmdldEFjY291bnRSZWNvcmRzPTAtKgpnZXRUeFJlY29yZEJ5VHhJRD0wLSoKZ2V0VHJhbnNhY3Rpb25SZWNlaXB0cz0wLSoKYXBwcm92ZUFsbG93YW5jZXM9MC0qCmRlbGV0ZUFsbG93YW5jZXM9MC0qCiMgRmlsZQpjcmVhdGVGaWxlPTAtKgp1cGRhdGVGaWxlPTAtKgpkZWxldGVGaWxlPTAtKgphcHBlbmRDb250ZW50PTAtKgpnZXRGaWxlQ29udGVudD0wLSoKZ2V0RmlsZUluZm89MC0qCiMgQ29udHJhY3QKY3JlYXRlQ29udHJhY3Q9MC0qCnVwZGF0ZUNvbnRyYWN0PTAtKgpjb250cmFjdENhbGxNZXRob2Q9MC0qCmdldENvbnRyYWN0SW5mbz0wLSoKY29udHJhY3RDYWxsTG9jYWxNZXRob2Q9MC0qCmNvbnRyYWN0R2V0Qnl0ZWNvZGU9MC0qCmdldFR4UmVjb3JkQnlDb250cmFjdElEPTAtKgpkZWxldGVDb250cmFjdD0wLSoKIyBDb25zZW5zdXMKY3JlYXRlVG9waWM9MC0qCnVwZGF0ZVRvcGljPTAtKgpkZWxldGVUb3BpYz0wLSoKc3VibWl0TWVzc2FnZT0wLSoKZ2V0VG9waWNJbmZvPTAtKgojIEV0aGVyZXVtCmV0aGVyZXVtVHJhbnNhY3Rpb249MC0qCiMgU2NoZWR1bGluZwpzY2hlZHVsZUNyZWF0ZT0wLSoKc2NoZWR1bGVTaWduPTAtKgpzY2hlZHVsZURlbGV0ZT0wLSoKc2NoZWR1bGVHZXRJbmZvPTAtKgojIFRva2VuCnRva2VuQ3JlYXRlPTAtKgp0b2tlbkZyZWV6ZUFjY291bnQ9MC0qCnRva2VuVW5mcmVlemVBY2NvdW50PTAtKgp0b2tlbkdyYW50S3ljVG9BY2NvdW50PTAtKgp0b2tlblJldm9rZUt5Y0Zyb21BY2NvdW50PTAtKgp0b2tlbkRlbGV0ZT0wLSoKdG9rZW5NaW50PTAtKgp0b2tlbkJ1cm49MC0qCnRva2VuQWNjb3VudFdpcGU9MC0qCnRva2VuVXBkYXRlPTAtKgp0b2tlbkdldEluZm89MC0qCnRva2VuR2V0TmZ0SW5mbz0wLSoKdG9rZW5HZXROZnRJbmZvcz0wLSoKdG9rZW5HZXRBY2NvdW50TmZ0SW5mb3M9MC0qCnRva2VuQXNzb2NpYXRlVG9BY2NvdW50PTAtKgp0b2tlbkRpc3NvY2lhdGVGcm9tQWNjb3VudD0wLSoKdG9rZW5GZWVTY2hlZHVsZVVwZGF0ZT0wLSoKdG9rZW5QYXVzZT0wLSoKdG9rZW5VbnBhdXNlPTAtKgojIE5ldHdvcmsgCmdldFZlcnNpb25JbmZvPTAtKgpuZXR3b3JrR2V0RXhlY3V0aW9uVGltZT0yLTUwCnN5c3RlbURlbGV0ZT0yLTU5CnN5c3RlbVVuZGVsZXRlPTItNjAKZnJlZXplPTItNTgKZ2V0QWNjb3VudERldGFpbHM9Mi01MAojIFV0aWwKdXRpbFBybmc9MC0q

  8c4feac6bcb94afde681db33f96763e349685f781f3db91f7019bce3744f6e95: YXV0b1JlbmV3LnRhcmdldFR5cGVzPQo=

  9ac74c654a84e0c745f2df63ec2574fd43f116d5a7200683e287a4baa97c5de7: bGVkZ2VyLmlkPTB4MDEKbmV0dHkubW9kZT1ERVYKY29udHJhY3RzLmNoYWluSWQ9Mjk4CmhlZGVyYS5yZWNvcmRTdHJlYW0ubG9nUGVyaW9kPTEKYmFsYW5jZXMuZXhwb3J0UGVyaW9kU2Vjcz00MDAKZmlsZXMubWF4U2l6ZUtiPTIwNDgK
---
# Source: hedera-mirror-node/charts/network-node/templates/logsVolumeClaim.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: node-logs
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi
---
# Source: hedera-mirror-node/charts/network-node/templates/minio-volumeClaim.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: minio-data
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/grpc/templates/role.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  labels: 
    app.kubernetes.io/component: grpc
    app.kubernetes.io/name: grpc
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: hedera-mirror-node
    app.kubernetes.io/version: "0.68.0"
    helm.sh/chart: grpc-0.68.0
  name: mirror-1-grpc
  namespace: default
rules:
  - apiGroups: [""]
    resources: ["configmaps"]
    verbs: ["get", "watch"]
  - apiGroups: [""]
    resources: ["pods"]
    verbs: ["get"]
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/importer/templates/role.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  labels: 
    app.kubernetes.io/component: importer
    app.kubernetes.io/name: importer
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: hedera-mirror-node
    app.kubernetes.io/version: "0.68.0"
    helm.sh/chart: importer-0.68.0
  name: mirror-1-importer
  namespace: default
rules:
- apiGroups: [""]
  resources: ["configmaps"]
  verbs: ["create", "get", "list", "update", "watch"]
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "watch"]
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/monitor/templates/role.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  labels: 
    app.kubernetes.io/component: monitor
    app.kubernetes.io/name: monitor
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: hedera-mirror-node
    app.kubernetes.io/version: "0.68.0"
    helm.sh/chart: monitor-0.68.0
  name: mirror-1-monitor
  namespace: default
rules:
  - apiGroups: [""]
    resources: ["configmaps"]
    verbs: ["get", "watch"]
  - apiGroups: [""]
    resources: ["pods"]
    verbs: ["get"]
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/redis/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: mirror-1-redis
  namespace: "default"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-17.3.7
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
rules:
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/web3/templates/role.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  labels: 
    app.kubernetes.io/component: web3
    app.kubernetes.io/name: web3
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: hedera-mirror-node
    app.kubernetes.io/version: "0.68.0"
    helm.sh/chart: web3-0.68.0
  name: mirror-1-web3
  namespace: default
rules:
  - apiGroups: [""]
    resources: ["configmaps"]
    verbs: ["get", "watch"]
  - apiGroups: [""]
    resources: ["pods"]
    verbs: ["get"]
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/grpc/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels: 
    app.kubernetes.io/component: grpc
    app.kubernetes.io/name: grpc
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: hedera-mirror-node
    app.kubernetes.io/version: "0.68.0"
    helm.sh/chart: grpc-0.68.0
  name: mirror-1-grpc
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: mirror-1-grpc
subjects:
- kind: ServiceAccount
  name: mirror-1-grpc
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/importer/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels: 
    app.kubernetes.io/component: importer
    app.kubernetes.io/name: importer
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: hedera-mirror-node
    app.kubernetes.io/version: "0.68.0"
    helm.sh/chart: importer-0.68.0
  name: mirror-1-importer
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: mirror-1-importer
subjects:
- kind: ServiceAccount
  name: mirror-1-importer
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/monitor/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels: 
    app.kubernetes.io/component: monitor
    app.kubernetes.io/name: monitor
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: hedera-mirror-node
    app.kubernetes.io/version: "0.68.0"
    helm.sh/chart: monitor-0.68.0
  name: mirror-1-monitor
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: mirror-1-monitor
subjects:
- kind: ServiceAccount
  name: mirror-1-monitor
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/redis/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: mirror-1-redis
  namespace: "default"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-17.3.7
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: mirror-1-redis
subjects:
  - kind: ServiceAccount
    name: mirror-1-redis
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/web3/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels: 
    app.kubernetes.io/component: web3
    app.kubernetes.io/name: web3
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: hedera-mirror-node
    app.kubernetes.io/version: "0.68.0"
    helm.sh/chart: web3-0.68.0
  name: mirror-1-web3
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: mirror-1-web3
subjects:
- kind: ServiceAccount
  name: mirror-1-web3
---
# Source: hedera-mirror-node/charts/hedera-explorer/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  annotations: 
    {}
  labels: 
    app.kubernetes.io/component: hedera-explorer
    app.kubernetes.io/name: hedera-explorer
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: hedera-explorer
    app.kubernetes.io/version: "0.1.0-SNAPSHOT"
    helm.sh/chart: hedera-explorer-0.1.0
  name: mirror-1-hedera-explorer
  namespace: default
spec:
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector: 
    app.kubernetes.io/component: hedera-explorer
    app.kubernetes.io/name: hedera-explorer
    app.kubernetes.io/instance: mirror-1
  type: LoadBalancer
---
# Source: hedera-mirror-node/charts/hedera-json-rpc-relay/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: mirror-1-hedera-json-rpc-relay
  labels:
    helm.sh/chart: hedera-json-rpc-relay-0.12.0
    app.kubernetes.io/name: hedera-json-rpc-relay
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/version: "0.12.0-SNAPSHOT"
    app.kubernetes.io/managed-by: Helm  
spec:
  type: LoadBalancer
  ports:
    - port: 7546
      targetPort: jsonrpcrelay
      protocol: TCP
      name: jsonrpcrelay
  selector:
    app: hedera-json-rpc-relay
    app.kubernetes.io/name: hedera-json-rpc-relay
    app.kubernetes.io/instance: mirror-1
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/grpc/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  annotations: 
    traefik.ingress.kubernetes.io/service.serversscheme: h2c
  labels: 
    app.kubernetes.io/component: grpc
    app.kubernetes.io/name: grpc
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: hedera-mirror-node
    app.kubernetes.io/version: "0.68.0"
    helm.sh/chart: grpc-0.68.0
  name: mirror-1-grpc
  namespace: default
spec:
  ports:
    - port: 5600
      targetPort: grpc
      protocol: TCP
      name: grpc
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector: 
    app.kubernetes.io/component: grpc
    app.kubernetes.io/name: grpc
    app.kubernetes.io/instance: mirror-1
  type: ClusterIP
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/monitor/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  annotations: 
    {}
  labels: 
    app.kubernetes.io/component: monitor
    app.kubernetes.io/name: monitor
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: hedera-mirror-node
    app.kubernetes.io/version: "0.68.0"
    helm.sh/chart: monitor-0.68.0
  name: mirror-1-monitor
  namespace: default
spec:
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector: 
    app.kubernetes.io/component: monitor
    app.kubernetes.io/name: monitor
    app.kubernetes.io/instance: mirror-1
  type: ClusterIP
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/postgresql/templates/pgpool/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: mirror-1-postgres-pgpool
  namespace: "default"
  labels:
    app.kubernetes.io/name: postgres
    helm.sh/chart: postgresql-9.4.11
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: pgpool
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: "postgresql"
      port: 5432
      targetPort: postgresql
      protocol: TCP
      nodePort: null
  selector:
    app.kubernetes.io/name: postgres
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/component: pgpool
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/postgresql/templates/postgresql/service-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: mirror-1-postgres-postgresql-headless
  namespace: "default"
  labels:
    app.kubernetes.io/name: postgres
    helm.sh/chart: postgresql-9.4.11
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: "postgresql"
      port: 5432
      targetPort: postgresql
      protocol: TCP
  selector:
    app.kubernetes.io/name: postgres
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/component: postgresql
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/postgresql/templates/postgresql/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: mirror-1-postgres-postgresql
  namespace: "default"
  labels:
    app.kubernetes.io/name: postgres
    helm.sh/chart: postgresql-9.4.11
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: postgresql
spec:
  type: ClusterIP
  ports:
    - name: "postgresql"
      port: 5432
      targetPort: postgresql
      protocol: TCP
  selector:
    app.kubernetes.io/name: postgres
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/component: postgresql
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/redis/templates/headless-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: mirror-1-redis-headless
  namespace: "default"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-17.3.7
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
  annotations:
    
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - name: tcp-redis
      port: 6379
      targetPort: redis
    - name: tcp-sentinel
      port: 26379
      targetPort: redis-sentinel
  selector:
    app.kubernetes.io/name: redis
    app.kubernetes.io/instance: mirror-1
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/redis/templates/sentinel/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: mirror-1-redis
  namespace: "default"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-17.3.7
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: node
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: tcp-redis
      port: 6379
      targetPort: 6379
      nodePort: null
    - name: tcp-sentinel
      port: 26379
      targetPort: 26379
      nodePort: null
  selector:
    app.kubernetes.io/name: redis
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/component: node
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/rest/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  annotations: 
    {}
  labels: 
    app.kubernetes.io/component: rest
    app.kubernetes.io/name: rest
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: hedera-mirror-node
    app.kubernetes.io/version: "0.68.0"
    helm.sh/chart: rest-0.68.0
  name: mirror-node-rest
  namespace: default
spec:
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector: 
    app.kubernetes.io/component: rest
    app.kubernetes.io/name: rest
    app.kubernetes.io/instance: mirror-1
  type: ClusterIP
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/rosetta/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  annotations: 
    {}
  labels: 
    app.kubernetes.io/component: rosetta
    app.kubernetes.io/name: rosetta
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: hedera-mirror-node
    app.kubernetes.io/version: "0.68.0"
    helm.sh/chart: rosetta-0.68.0
  name: mirror-1-rosetta
  namespace: default
spec:
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector: 
    app.kubernetes.io/component: rosetta
    app.kubernetes.io/name: rosetta
    app.kubernetes.io/instance: mirror-1
  type: ClusterIP
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/web3/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  annotations: 
    {}
  labels: 
    app.kubernetes.io/component: web3
    app.kubernetes.io/name: web3
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: hedera-mirror-node
    app.kubernetes.io/version: "0.68.0"
    helm.sh/chart: web3-0.68.0
  name: mirror-1-web3
  namespace: default
spec:
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector: 
    app.kubernetes.io/component: web3
    app.kubernetes.io/name: web3
    app.kubernetes.io/instance: mirror-1
  type: ClusterIP
---
# Source: hedera-mirror-node/charts/network-node/templates/minio-service.yaml
apiVersion: v1
kind: Service
metadata:
  annotations:
    kompose.cmd: kompose -v convert
    kompose.version: 1.26.1 (a9d05d509)
  creationTimestamp: null
  labels:
    io.kompose.service: minio
  name: minio
spec:
  ports:
    - name: "9000"
      port: 9000
      targetPort: 9000
    - name: "9001"
      port: 9001
      targetPort: 9001
  selector:
    io.kompose.service: minio
status:
  loadBalancer: {}
---
# Source: hedera-mirror-node/charts/network-node/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: network-node
  labels:
    helm.sh/chart: network-node-0.1.0
    app.kubernetes.io/name: network-node
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: "50211"
      port: 50211
      targetPort: 50211
    - name: "50212"
      port: 50212
      targetPort: 50212


  selector:
    app.kubernetes.io/name: network-node
    app.kubernetes.io/instance: mirror-1
---
# Source: hedera-mirror-node/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: mirror-1-hedera-mirror-node
  labels:
    helm.sh/chart: hedera-mirror-node-0.1.0
    app.kubernetes.io/name: hedera-mirror-node
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: hedera-mirror-node
    app.kubernetes.io/instance: mirror-1
---
# Source: hedera-mirror-node/charts/hedera-explorer/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations: 
    {}
  labels: 
    app.kubernetes.io/component: hedera-explorer
    app.kubernetes.io/name: hedera-explorer
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: hedera-explorer
    app.kubernetes.io/version: "0.1.0-SNAPSHOT"
    helm.sh/chart: hedera-explorer-0.1.0
  name: mirror-1-hedera-explorer
  namespace: default
spec:
  replicas: 1
  revisionHistoryLimit: 3
  selector:
    matchLabels: 
      app.kubernetes.io/component: hedera-explorer
      app.kubernetes.io/name: hedera-explorer
      app.kubernetes.io/instance: mirror-1
  strategy: 
    rollingUpdate:
      maxSurge: 10%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      annotations:
        {}
      labels: 
        app.kubernetes.io/component: hedera-explorer
        app.kubernetes.io/name: hedera-explorer
        app.kubernetes.io/instance: mirror-1
    spec:
      affinity: 
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchLabels:
                  app.kubernetes.io/component: hedera-explorer
              topologyKey: kubernetes.io/hostname
            weight: 100
      containers:
        - name: hedera-explorer
          env:
            - name: VUE_APP_LOCAL_MIRROR_NODE_URL
              value: "https://mainnet-public.mirrornode.hedera.com"
          envFrom: 
            []
          image: "docker.io/cabob/hedera-explorer:latest"
          imagePullPolicy: Always
          livenessProbe:
            httpGet:
              path: /
              port: http
            initialDelaySeconds: 25
            timeoutSeconds: 2
          ports:
            - containerPort: 8080
              name: http
              protocol: TCP
          readinessProbe: 
            httpGet:
              path: /
              port: http
            initialDelaySeconds: 30
            timeoutSeconds: 2
          resources: 
            limits:
              cpu: 500m
              memory: 250Mi
            requests:
              cpu: 200m
              memory: 100Mi
          securityContext: 
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          volumeMounts:
            - name: cache
              mountPath: /var/cache/nginx
      imagePullSecrets: 
        []
      nodeSelector: 
        {}
      priorityClassName: 
      securityContext: 
        fsGroup: 1000
        runAsGroup: 1000
        runAsNonRoot: true
        runAsUser: 1000
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: mirror-1-hedera-explorer
      terminationGracePeriodSeconds: 30
      tolerations: 
        []
      volumes:
        - name: cache
          emptyDir: {}
---
# Source: hedera-mirror-node/charts/hedera-json-rpc-relay/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mirror-1-hedera-json-rpc-relay
  labels:
    app:  hedera-json-rpc-relay
    
    helm.sh/chart: hedera-json-rpc-relay-0.12.0
    app.kubernetes.io/name: hedera-json-rpc-relay
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/version: "0.12.0-SNAPSHOT"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 2
  selector:
    matchLabels:
      app.kubernetes.io/name: hedera-json-rpc-relay
      app.kubernetes.io/instance: mirror-1
  template:
    metadata:
      labels:
        app:  hedera-json-rpc-relay
        app.kubernetes.io/name: hedera-json-rpc-relay
        app.kubernetes.io/instance: mirror-1
    spec:
      serviceAccountName: mirror-1-hedera-json-rpc-relay
      securityContext:
        {}
      containers:
      - name: hedera-json-rpc-relay
        image: "cabob/hedera-json-rpc-relay:latest"
        imagePullPolicy: Always
        env:
          - name: CHAIN_ID
            valueFrom:
              configMapKeyRef:
                name: mirror-1-hedera-json-rpc-relay
                key: CHAIN_ID
                optional: true
          - name: HEDERA_NETWORK
            valueFrom:
              configMapKeyRef:
                name: mirror-1-hedera-json-rpc-relay
                key: HEDERA_NETWORK
                optional: false
          - name: OPERATOR_ID_ETH_SENDRAWTRANSACTION
            valueFrom:
              secretKeyRef:
                name: mirror-1-hedera-json-rpc-relay
                key: OPERATOR_ID_ETH_SENDRAWTRANSACTION
                optional: true
          - name: OPERATOR_KEY_ETH_SENDRAWTRANSACTION
            valueFrom:
              secretKeyRef:
                name: mirror-1-hedera-json-rpc-relay
                key: OPERATOR_KEY_ETH_SENDRAWTRANSACTION
                optional: true
          - name: MIRROR_NODE_URL
            valueFrom:
              configMapKeyRef:
                name: mirror-1-hedera-json-rpc-relay
                key: MIRROR_NODE_URL
                optional: false
          - name: LOCAL_NODE
            valueFrom:
              configMapKeyRef:
                name: mirror-1-hedera-json-rpc-relay
                key: LOCAL_NODE
                optional: false
          - name: SERVER_PORT
            valueFrom:
              configMapKeyRef:
                name: mirror-1-hedera-json-rpc-relay
                key: SERVER_PORT
                optional: false
          - name: OPERATOR_ID_MAIN
            valueFrom:
              secretKeyRef:
                name: mirror-1-hedera-json-rpc-relay
                key: OPERATOR_ID_MAIN
                optional: false
          - name: OPERATOR_KEY_MAIN
            valueFrom:
              secretKeyRef:
                name: mirror-1-hedera-json-rpc-relay
                key: OPERATOR_KEY_MAIN
                optional: false
          - name: DEFAULT_RATE_LIMIT
            valueFrom:
              configMapKeyRef:
                name: mirror-1-hedera-json-rpc-relay
                key: DEFAULT_RATE_LIMIT
                optional: true
          - name: TIER_1_RATE_LIMIT
            valueFrom:
              configMapKeyRef:
                name: mirror-1-hedera-json-rpc-relay
                key: TIER_1_RATE_LIMIT
                optional: true
          - name: TIER_2_RATE_LIMIT
            valueFrom:
              configMapKeyRef:
                name: mirror-1-hedera-json-rpc-relay
                key: TIER_2_RATE_LIMIT
                optional: true
          - name: TIER_3_RATE_LIMIT
            valueFrom:
              configMapKeyRef:
                name: mirror-1-hedera-json-rpc-relay
                key: TIER_3_RATE_LIMIT
                optional: true
          - name: LIMIT_DURATION
            valueFrom:
              configMapKeyRef:
                name: mirror-1-hedera-json-rpc-relay
                key: LIMIT_DURATION
                optional: true
          - name: HBAR_RATE_LIMIT_TINYBAR
            valueFrom:
              configMapKeyRef:
                name: mirror-1-hedera-json-rpc-relay
                key: HBAR_RATE_LIMIT_TINYBAR
                optional: true
          - name: HBAR_RATE_LIMIT_DURATION
            valueFrom:
              configMapKeyRef:
                name: mirror-1-hedera-json-rpc-relay
                key: HBAR_RATE_LIMIT_DURATION
                optional: true
          - name: ETH_GET_LOGS_BLOCK_RANGE_LIMIT
            valueFrom:
              configMapKeyRef:
                name: mirror-1-hedera-json-rpc-relay
                key: ETH_GET_LOGS_BLOCK_RANGE_LIMIT
                optional: true
          - name: RATE_LIMIT_DISABLED
            valueFrom:
              configMapKeyRef:
                name: mirror-1-hedera-json-rpc-relay
                key: RATE_LIMIT_DISABLED
                optional: true
          - name: DEV_MODE
            valueFrom:
              configMapKeyRef:
                name: mirror-1-hedera-json-rpc-relay
                key: DEV_MODE
                optional: true
        ports:
          - containerPort: 7546
            name: jsonrpcrelay
        livenessProbe:
          httpGet:
            path: /health/liveness
            port: jsonrpcrelay
        readinessProbe:
          httpGet:
            path: /health/readiness
            port: jsonrpcrelay
        resources:            {}
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/grpc/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations: 
    {}
  labels: 
    app.kubernetes.io/component: grpc
    app.kubernetes.io/name: grpc
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: hedera-mirror-node
    app.kubernetes.io/version: "0.68.0"
    helm.sh/chart: grpc-0.68.0
  name: mirror-1-grpc
  namespace: default
spec:
  revisionHistoryLimit: 3
  selector:
    matchLabels: 
      app.kubernetes.io/component: grpc
      app.kubernetes.io/name: grpc
      app.kubernetes.io/instance: mirror-1
  strategy: 
    rollingUpdate:
      maxSurge: 10%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      annotations:
        checksum/secret: ccd7e941032a341ed007863f4e35767627f7ddb94e6ea00979b2682e9e9dceb8
      labels: 
        app.kubernetes.io/component: grpc
        app.kubernetes.io/name: grpc
        app.kubernetes.io/instance: mirror-1
    spec:
      affinity: 
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchLabels:
                  app.kubernetes.io/component: grpc
              topologyKey: kubernetes.io/hostname
            weight: 100
      containers:
        - name: grpc
          env:
            - name: HEDERA_MIRROR_GRPC_DB_HOST
              valueFrom:
                secretKeyRef:
                  key: HEDERA_MIRROR_IMPORTER_DB_HOST
                  name: mirror-passwords
            - name: HEDERA_MIRROR_GRPC_DB_NAME
              valueFrom:
                secretKeyRef:
                  key: HEDERA_MIRROR_IMPORTER_DB_NAME
                  name: mirror-passwords
            - name: HEDERA_MIRROR_GRPC_DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: HEDERA_MIRROR_GRPC_DB_PASSWORD
                  name: mirror-passwords
            - name: HEDERA_MIRROR_GRPC_DB_USERNAME
              valueFrom:
                secretKeyRef:
                  key: HEDERA_MIRROR_GRPC_DB_USERNAME
                  name: mirror-passwords
            - name: SPRING_CLOUD_KUBERNETES_ENABLED
              value: "true"
            - name: SPRING_CONFIG_ADDITIONAL_LOCATION
              value: "file:/usr/etc/hedera/"
            - name: SPRING_REDIS_HOST
              valueFrom:
                secretKeyRef:
                  key: SPRING_REDIS_HOST
                  name: mirror-redis
            - name: SPRING_REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: SPRING_REDIS_PASSWORD
                  name: mirror-redis
          envFrom: 
            []
          image: "gcr.io/mirrornode/hedera-mirror-grpc:0.68.0"
          imagePullPolicy: IfNotPresent
          livenessProbe: 
            httpGet:
              path: /actuator/health/liveness
              port: http
            initialDelaySeconds: 50
            periodSeconds: 10
            timeoutSeconds: 2
          ports:
            - containerPort: 5600
              name: grpc
              protocol: TCP
            - containerPort: 8081
              name: http
              protocol: TCP
          readinessProbe: 
            httpGet:
              path: /actuator/health/readiness
              port: http
            initialDelaySeconds: 40
            timeoutSeconds: 2
          resources: 
            limits:
              cpu: 2
              memory: 2048Mi
            requests:
              cpu: 100m
              memory: 128Mi
          securityContext: 
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          volumeMounts:
            - name: config
              mountPath: /usr/etc/hedera
      imagePullSecrets: 
        []
      nodeSelector: 
        {}
      priorityClassName: 
      securityContext: 
        fsGroup: 1000
        runAsGroup: 1000
        runAsNonRoot: true
        runAsUser: 1000
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: mirror-1-grpc
      terminationGracePeriodSeconds: 60
      tolerations: 
        []
      volumes:
        - name: config
          secret:
            defaultMode: 420
            secretName: 'mirror-1-grpc'
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/importer/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations: 
    {}
  labels: 
    app.kubernetes.io/component: importer
    app.kubernetes.io/name: importer
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: hedera-mirror-node
    app.kubernetes.io/version: "0.68.0"
    helm.sh/chart: importer-0.68.0
  name: mirror-1-importer
  namespace: default
spec:
  replicas: 1
  revisionHistoryLimit: 3
  selector:
    matchLabels: 
      app.kubernetes.io/component: importer
      app.kubernetes.io/name: importer
      app.kubernetes.io/instance: mirror-1
  strategy: 
    type: Recreate
  template:
    metadata:
      annotations:
        checksum/secret: cb24bb09c388c25bb9d57fb1feb3c267f8f8dc64bcf964447f1bdf9da2a122ee
      labels: 
        app.kubernetes.io/component: importer
        app.kubernetes.io/name: importer
        app.kubernetes.io/instance: mirror-1
    spec:
      affinity: 
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchLabels:
                  app.kubernetes.io/component: importer
              topologyKey: kubernetes.io/hostname
            weight: 100
      containers:
        - name: importer
          env:
            - name: SPRING_CLOUD_KUBERNETES_ENABLED
              value: "true"
            - name: SPRING_CONFIG_ADDITIONAL_LOCATION
              value: "file:/usr/etc/hedera-mirror-importer/"
          envFrom: 
            - secretRef:
                name: mirror-passwords
            - secretRef:
                name: mirror-redis
          image: "gcr.io/mirrornode/hedera-mirror-importer:0.68.0"
          imagePullPolicy: Always
          livenessProbe: 
            httpGet:
              path: /actuator/health/liveness
              port: http
            initialDelaySeconds: 60
            periodSeconds: 30
            timeoutSeconds: 2
          ports:
            - containerPort: 8080
              name: http
              protocol: TCP
          readinessProbe: 
            httpGet:
              path: /actuator/health/readiness
              port: http
            initialDelaySeconds: 60
            timeoutSeconds: 2
          resources: 
            limits:
              cpu: 1
              memory: 4Gi
            requests:
              cpu: 200m
              memory: 512Mi
          securityContext: 
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          startupProbe: 
            failureThreshold: 8640
            httpGet:
              path: /actuator/health/startup
              port: http
            initialDelaySeconds: 60
            periodSeconds: 10
            timeoutSeconds: 2
          volumeMounts:
            - name: abook
              mountPath: /usr/etc/hedera-mirror-importer/local-dev-1-node.addressbook.f102.json.bin
            - name: config
              mountPath: /usr/etc/hedera
      imagePullSecrets: 
        []
      nodeSelector: 
        {}
      priorityClassName: 
      securityContext: 
        fsGroup: 1000
        runAsGroup: 1000
        runAsNonRoot: true
        runAsUser: 1000
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: mirror-1-importer
      terminationGracePeriodSeconds: 30
      tolerations: 
        []
      volumes:
        - name: abook
          secret:
            secretName: abook-config
        - name: config
          secret:
            defaultMode: 420
            secretName: app-config
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/monitor/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations: 
    {}
  labels: 
    app.kubernetes.io/component: monitor
    app.kubernetes.io/name: monitor
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: hedera-mirror-node
    app.kubernetes.io/version: "0.68.0"
    helm.sh/chart: monitor-0.68.0
  name: mirror-1-monitor
  namespace: default
spec:
  replicas: 1
  revisionHistoryLimit: 3
  selector:
    matchLabels: 
      app.kubernetes.io/component: monitor
      app.kubernetes.io/name: monitor
      app.kubernetes.io/instance: mirror-1
  strategy: 
    rollingUpdate:
      maxSurge: 10%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      annotations:
        checksum/secret: 4c71b7b9f291376dce7d146bac3a68caaf0a54e8af7339f310434486bbe5c20c
      labels: 
        app.kubernetes.io/component: monitor
        app.kubernetes.io/name: monitor
        app.kubernetes.io/instance: mirror-1
    spec:
      affinity: 
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchLabels:
                  app.kubernetes.io/component: monitor
              topologyKey: kubernetes.io/hostname
            weight: 100
      containers:
        - name: monitor
          env:
            - name: HEDERA_MIRROR_MONITOR_MIRROR_NODE_GRPC_HOST
              value: "mirror-1-grpc"
            - name: HEDERA_MIRROR_MONITOR_MIRROR_NODE_REST_HOST
              value: "mirror-1-rest"
            - name: HEDERA_MIRROR_MONITOR_MIRROR_NODE_REST_PORT
              value: "80"
            - name: SPRING_CLOUD_KUBERNETES_ENABLED
              value: "true"
            - name: SPRING_CONFIG_ADDITIONAL_LOCATION
              value: "file:/usr/etc/hedera/"
          envFrom: 
            []
          image: "gcr.io/mirrornode/hedera-mirror-monitor:0.68.0"
          imagePullPolicy: IfNotPresent
          livenessProbe: 
            failureThreshold: 5
            httpGet:
              path: /actuator/health/liveness
              port: http
            initialDelaySeconds: 90
            periodSeconds: 10
            timeoutSeconds: 2
          ports:
            - containerPort: 8082
              name: http
              protocol: TCP
          readinessProbe: 
            failureThreshold: 5
            httpGet:
              path: /actuator/health/readiness
              port: http
            initialDelaySeconds: 60
            timeoutSeconds: 2
          resources: 
            limits:
              cpu: 500m
              memory: 768Mi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext: 
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          volumeMounts:
            - name: config
              mountPath: /usr/etc/hedera
      imagePullSecrets: 
        []
      nodeSelector: 
        {}
      priorityClassName: 
      securityContext: 
        fsGroup: 1000
        runAsGroup: 1000
        runAsNonRoot: true
        runAsUser: 1000
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: mirror-1-monitor
      terminationGracePeriodSeconds: 60
      tolerations: 
        []
      volumes:
        - name: config
          secret:
            defaultMode: 420
            secretName: 'mirror-1-monitor'
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/postgresql/templates/pgpool/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mirror-1-postgres-pgpool
  namespace: "default"
  labels: 
    app.kubernetes.io/name: postgres
    helm.sh/chart: postgresql-9.4.11
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: pgpool
spec:
  replicas: 1
  selector:
    matchLabels: 
      app.kubernetes.io/name: postgres
      app.kubernetes.io/instance: mirror-1
      app.kubernetes.io/component: pgpool
  template:
    metadata:
      labels: 
        app.kubernetes.io/name: postgres
        helm.sh/chart: postgresql-9.4.11
        app.kubernetes.io/instance: mirror-1
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: pgpool
        role: db
    spec:
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: postgres
                    app.kubernetes.io/instance: mirror-1
                    app.kubernetes.io/component: pgpool
                namespaces:
                  - "default"
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      serviceAccountName: mirror-1-postgres
      initContainers:
      # Auxiliary vars to populate environment variables
      containers:
        - name: pgpool
          image: docker.io/bitnami/pgpool:4.3.3-debian-11-r22
          imagePullPolicy: "IfNotPresent"
          securityContext:
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            runAsUser: 1001
          env:
            - name: BITNAMI_DEBUG
              value: "true"
            - name: PGPOOL_BACKEND_NODES
              value: 0:mirror-1-postgres-postgresql-0.mirror-1-postgres-postgresql-headless:5432,
            - name: PGPOOL_SR_CHECK_USER
              value: "repmgr"
            - name: PGPOOL_SR_CHECK_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: mirror-passwords
                  key: repmgr-password
            - name: PGPOOL_SR_CHECK_DATABASE
              value: "postgres"
            - name: PGPOOL_ENABLE_LDAP
              value: "no"
            - name: PGPOOL_POSTGRES_USERNAME
              value: "postgres"
            - name: PGPOOL_POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: mirror-passwords
                  key: postgresql-password
            - name: PGPOOL_ADMIN_USERNAME
              value: "admin"
            - name: PGPOOL_ADMIN_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: mirror-passwords
                  key: admin-password
            - name: PGPOOL_AUTHENTICATION_METHOD
              value: "scram-sha-256"
            - name: PGPOOL_ENABLE_LOAD_BALANCING
              value: "yes"
            - name: PGPOOL_DISABLE_LOAD_BALANCE_ON_WRITE
              value: transaction
            - name: PGPOOL_ENABLE_LOG_CONNECTIONS
              value: "no"
            - name: PGPOOL_ENABLE_LOG_HOSTNAME
              value: "yes"
            - name: PGPOOL_ENABLE_LOG_PER_NODE_STATEMENT
              value: "no"
            - name: PGPOOL_RESERVED_CONNECTIONS
              value: '1'
            - name: PGPOOL_CHILD_LIFE_TIME
              value: ""
            - name: PGPOOL_ENABLE_TLS
              value: "no"
            - name: PGPOOL_POSTGRES_CUSTOM_PASSWORDS
              valueFrom:
                secretKeyRef:
                  key: PGPOOL_POSTGRES_CUSTOM_PASSWORDS
                  name: mirror-passwords
            - name: PGPOOL_POSTGRES_CUSTOM_USERS
              valueFrom:
                secretKeyRef:
                  key: PGPOOL_POSTGRES_CUSTOM_USERS
                  name: mirror-passwords
          envFrom:
          ports:
            - name: postgresql
              containerPort: 5432
              protocol: TCP
          livenessProbe:
            failureThreshold: 5
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - /opt/bitnami/scripts/pgpool/healthcheck.sh
          readinessProbe:
            failureThreshold: 5
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - bash
                - -ec
                - PGPASSWORD=${PGPOOL_POSTGRES_PASSWORD} psql -U "postgres" -d "postgres" -h /opt/bitnami/pgpool/tmp -tA -c "SELECT 1" >/dev/null
          resources:
            limits:
              cpu: 300m
              memory: 750Mi
            requests:
              cpu: 150m
              memory: 256Mi
          volumeMounts:
      volumes:
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/rest/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations: 
    {}
  labels: 
    app.kubernetes.io/component: rest
    app.kubernetes.io/name: rest
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: hedera-mirror-node
    app.kubernetes.io/version: "0.68.0"
    helm.sh/chart: rest-0.68.0
  name: mirror-node-rest
  namespace: default
spec:
  revisionHistoryLimit: 3
  selector:
    matchLabels: 
      app.kubernetes.io/component: rest
      app.kubernetes.io/name: rest
      app.kubernetes.io/instance: mirror-1
  strategy: 
    rollingUpdate:
      maxSurge: 10%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      annotations:
        checksum/secret: c6a41c2145da73a27e346289482ce8d0c38297a6259c9d70d7495be6b1d4dbc3
      labels: 
        app.kubernetes.io/component: rest
        app.kubernetes.io/name: rest
        app.kubernetes.io/instance: mirror-1
    spec:
      affinity: 
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchLabels:
                  app.kubernetes.io/component: rest
              topologyKey: kubernetes.io/hostname
            weight: 100
      containers:
        - name: rest
          env:
            - name: CONFIG_PATH
              value: "/usr/etc/hedera/"
            - name: HEDERA_MIRROR_REST_DB_HOST
              valueFrom:
                secretKeyRef:
                  key: HEDERA_MIRROR_IMPORTER_DB_HOST
                  name: mirror-passwords
            - name: HEDERA_MIRROR_REST_DB_NAME
              valueFrom:
                secretKeyRef:
                  key: HEDERA_MIRROR_IMPORTER_DB_NAME
                  name: mirror-passwords
            - name: HEDERA_MIRROR_REST_DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: HEDERA_MIRROR_IMPORTER_DB_RESTPASSWORD
                  name: mirror-passwords
            - name: HEDERA_MIRROR_REST_DB_USERNAME
              valueFrom:
                secretKeyRef:
                  key: HEDERA_MIRROR_IMPORTER_DB_RESTUSERNAME
                  name: mirror-passwords
          envFrom: 
            []
          image: "docker.io//cabob/hedera-mirror-rest:latest"
          imagePullPolicy: IfNotPresent
          livenessProbe:
            httpGet:
              path: /health/liveness
              port: http
            initialDelaySeconds: 25
            timeoutSeconds: 2
          ports:
            - containerPort: 5551
              name: http
              protocol: TCP
          readinessProbe: 
            httpGet:
              path: /health/readiness
              port: http
            initialDelaySeconds: 30
            timeoutSeconds: 2
          resources: 
            limits:
              cpu: 750m
              memory: 350Mi
            requests:
              cpu: 300m
              memory: 64Mi
          securityContext: 
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          volumeMounts:
            - name: config
              mountPath: /usr/etc/hedera
      imagePullSecrets: 
        []
      nodeSelector: 
        {}
      priorityClassName: 
      securityContext: 
        fsGroup: 1000
        runAsGroup: 1000
        runAsNonRoot: true
        runAsUser: 1000
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: mirror-node-rest
      terminationGracePeriodSeconds: 60
      tolerations: 
        []
      volumes:
        - name: config
          secret:
            defaultMode: 420
            secretName: 'mirror-node-rest'
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/rosetta/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations: 
    {}
  labels: 
    app.kubernetes.io/component: rosetta
    app.kubernetes.io/name: rosetta
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: hedera-mirror-node
    app.kubernetes.io/version: "0.68.0"
    helm.sh/chart: rosetta-0.68.0
  name: mirror-1-rosetta
  namespace: default
spec:
  revisionHistoryLimit: 3
  selector:
    matchLabels: 
      app.kubernetes.io/component: rosetta
      app.kubernetes.io/name: rosetta
      app.kubernetes.io/instance: mirror-1
  strategy: 
    rollingUpdate:
      maxSurge: 10%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      annotations:
        checksum/secret: 26446164d7af191eeea66d6dfb9c6c7544b56ba0b00fc63f3f123cb01e4a2d6a
      labels: 
        app.kubernetes.io/component: rosetta
        app.kubernetes.io/name: rosetta
        app.kubernetes.io/instance: mirror-1
    spec:
      affinity: 
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchLabels:
                  app.kubernetes.io/component: rosetta
              topologyKey: kubernetes.io/hostname
            weight: 100
      containers:
        - name: rosetta
          env:
            - name: CONFIG_PATH
              value: "/usr/etc/hedera/"
            - name: HEDERA_MIRROR_ROSETTA_DB_HOST
              valueFrom:
                secretKeyRef:
                  key: HEDERA_MIRROR_IMPORTER_DB_HOST
                  name: mirror-passwords
            - name: HEDERA_MIRROR_ROSETTA_DB_NAME
              valueFrom:
                secretKeyRef:
                  key: HEDERA_MIRROR_IMPORTER_DB_NAME
                  name: mirror-passwords
            - name: HEDERA_MIRROR_ROSETTA_DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: HEDERA_MIRROR_ROSETTA_DB_PASSWORD
                  name: mirror-passwords
            - name: HEDERA_MIRROR_ROSETTA_DB_USERNAME
              valueFrom:
                secretKeyRef:
                  key: HEDERA_MIRROR_ROSETTA_DB_USERNAME
                  name: mirror-passwords
          envFrom: 
            []
          image: "gcr.io/mirrornode/hedera-mirror-rosetta:0.68.0"
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /health/liveness
              port: http
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 2
          ports:
            - containerPort: 5700
              name: http
              protocol: TCP
          readinessProbe: 
            failureThreshold: 5
            httpGet:
              path: /health/readiness
              port: http
            initialDelaySeconds: 30
            timeoutSeconds: 2
          resources: 
            limits:
              cpu: 500m
              memory: 200Mi
            requests:
              cpu: 50m
              memory: 64Mi
          securityContext: 
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          volumeMounts:
            - name: config
              mountPath: /usr/etc/hedera
      imagePullSecrets: 
        []
      nodeSelector: 
        {}
      priorityClassName: 
      securityContext: 
        fsGroup: 1000
        runAsGroup: 1000
        runAsNonRoot: true
        runAsUser: 1000
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: mirror-1-rosetta
      terminationGracePeriodSeconds: 60
      tolerations: 
        []
      volumes:
        - name: config
          secret:
            defaultMode: 420
            secretName: 'mirror-1-rosetta'
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/web3/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations: 
    {}
  labels: 
    app.kubernetes.io/component: web3
    app.kubernetes.io/name: web3
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: hedera-mirror-node
    app.kubernetes.io/version: "0.68.0"
    helm.sh/chart: web3-0.68.0
  name: mirror-1-web3
  namespace: default
spec:
  revisionHistoryLimit: 3
  selector:
    matchLabels: 
      app.kubernetes.io/component: web3
      app.kubernetes.io/name: web3
      app.kubernetes.io/instance: mirror-1
  strategy: 
    rollingUpdate:
      maxSurge: 10%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      annotations:
        checksum/secret: b27896f5eed59aefd7c1c08b0db2794335917199920f876facb7de551e691c4d
      labels: 
        app.kubernetes.io/component: web3
        app.kubernetes.io/name: web3
        app.kubernetes.io/instance: mirror-1
    spec:
      affinity: 
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchLabels:
                  app.kubernetes.io/component: web3
              topologyKey: kubernetes.io/hostname
            weight: 100
      containers:
        - name: web3
          env:
            - name: HEDERA_MIRROR_WEB3_DB_HOST
              valueFrom:
                secretKeyRef:
                  key: HEDERA_MIRROR_IMPORTER_DB_HOST
                  name: mirror-passwords
            - name: HEDERA_MIRROR_WEB3_DB_NAME
              valueFrom:
                secretKeyRef:
                  key: HEDERA_MIRROR_IMPORTER_DB_NAME
                  name: mirror-passwords
            - name: HEDERA_MIRROR_WEB3_DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: HEDERA_MIRROR_WEB3_DB_PASSWORD
                  name: mirror-passwords
            - name: HEDERA_MIRROR_WEB3_DB_USERNAME
              valueFrom:
                secretKeyRef:
                  key: HEDERA_MIRROR_WEB3_DB_USERNAME
                  name: mirror-passwords
            - name: SPRING_CLOUD_KUBERNETES_ENABLED
              value: "true"
            - name: SPRING_CONFIG_ADDITIONAL_LOCATION
              value: "file:/usr/etc/hedera/"
          envFrom: 
            []
          image: "gcr.io/mirrornode/hedera-mirror-web3:0.68.0"
          imagePullPolicy: IfNotPresent
          livenessProbe: 
            httpGet:
              path: /actuator/health/liveness
              port: http
            initialDelaySeconds: 50
            periodSeconds: 10
            timeoutSeconds: 2
          ports:
            - containerPort: 8545
              name: http
              protocol: TCP
          readinessProbe: 
            httpGet:
              path: /actuator/health/readiness
              port: http
            initialDelaySeconds: 40
            timeoutSeconds: 2
          resources: 
            limits:
              cpu: 2
              memory: 2048Mi
            requests:
              cpu: 100m
              memory: 128Mi
          securityContext: 
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          volumeMounts:
            - name: config
              mountPath: /usr/etc/hedera
      imagePullSecrets: 
        []
      nodeSelector: 
        {}
      priorityClassName: 
      securityContext: 
        fsGroup: 1000
        runAsGroup: 1000
        runAsNonRoot: true
        runAsUser: 1000
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: mirror-1-web3
      terminationGracePeriodSeconds: 60
      tolerations: 
        []
      volumes:
        - name: config
          secret:
            defaultMode: 420
            secretName: 'mirror-1-web3'
---
# Source: hedera-mirror-node/charts/network-node/templates/deployment-hevged.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    kompose.cmd: kompose -v convert -c
    kompose.version: 1.26.1 (a9d05d509)
  creationTimestamp: null
  labels:
    io.kompose.service: haveged
  name: haveged
spec:
  replicas: 1
  selector:
    matchLabels:
      io.kompose.service: haveged
  strategy: {}
  template:
    metadata:
      annotations:
        kompose.cmd: kompose -v convert -c
        kompose.version: 1.26.1 (a9d05d509)
      creationTimestamp: null
      labels:
        io.kompose.service: haveged
    spec:
      containers:
        - image: gcr.io/hedera-registry/network-node-haveged:0.32.0-alpha.1
          name: haveged
          resources: {}
          securityContext:
            privileged: true
      restartPolicy: Always
status: {}
---
# Source: hedera-mirror-node/charts/network-node/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mirror-1-network-node
  labels:
    helm.sh/chart: network-node-0.1.0
    app.kubernetes.io/name: network-node
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: network-node
      app.kubernetes.io/instance: mirror-1
  template:
    metadata:
      labels:
        app.kubernetes.io/name: network-node
        app.kubernetes.io/instance: mirror-1
    spec:
      serviceAccountName: mirror-1-network-node
      securityContext:
        {}
      hostname: network-node
      containers:
        - name: network-node
          securityContext:
            allowPrivilegeEscalation: false
            runAsUser: 0
          env:
            - name: JAVA_HEAP_MAX
              value: "2g"
            - name: JAVA_HEAP_MIN
              value: "256m"
            - name: JAVA_OPTS
              value: "-XX:+UnlockExperimentalVMOptions -XX:+UseZGC -Xlog:gc*:gc.log"
          envFrom: 
            []
          image: "cabob/hedera-services:latest"
          command: ["/bin/sh", "-c", "cp -r config-mount/* .;  ./start-services.sh;hostname"]
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: 0
              mountPath: /opt/hgcapp/accountBalances
              name: network-node-logs-root-path
              subPath: accountBalances
            - name: 1
              mountPath: /opt/hgcapp/recordStreams
              name: network-node-logs-root-path
              subPath: recordStreams
            - name: 2
              mountPath: /opt/hedera/services/services/output
              name: network-node-logs-root-path
              subPath: logs
          # 
          # 
          #   - name: application-config-path
          #     mountPath:   /opt/hedera/services/config-mount/api-permission.properties
          #     subPath:  1202c1ef941ef443d566693d4f4fe66b232e78467b6c9c776c03790420d25b83
          # 
          # 
          #   - name: application-config-path
          #     mountPath:   /opt/hedera/services/config-mount/application.properties
          #     subPath:  8c4feac6bcb94afde681db33f96763e349685f781f3db91f7019bce3744f6e95
          # 
          # 
          #   - name: application-config-path
          #     mountPath:   /opt/hedera/services/config-mount/bootstrap.properties
          #     subPath:  9ac74c654a84e0c745f2df63ec2574fd43f116d5a7200683e287a4baa97c5de7
          # 
          
          
            - name: application-root-path
              mountPath:   /opt/hedera/services/config-mount/config.txt
              subPath:  f2cc712e966feb696a4f0e6d540754849f998d15210192ebb23e3da74ec479dd
          
          
            - name: application-root-path
              mountPath:   /opt/hedera/services/config-mount/data/config/api-permission.properties
              subPath:  fff42a8c2cb099f0ccc7e617bd13806177c34b16dd863656439ef8b318c77d09
          
          
            - name: application-root-path
              mountPath:   /opt/hedera/services/config-mount/data/config/application.properties
              subPath:  6d00b1694ca05c63de005f98ad3c11513f068198ac965eabdef601b59c2ba468
          
          
            - name: application-root-path
              mountPath:   /opt/hedera/services/config-mount/data/config/bootstrap.properties
              subPath:  afed3074a19539c939a88c6c8e0af1a497410b7d54cf3f80a9d1ed17e40c6569
          
          
            - name: application-root-path
              mountPath:   /opt/hedera/services/config-mount/data/keys/private-node0.pfx
              subPath:  0225d92109512382e4db92bc9ab1a7c52ae6f3c1b433ba58cd45b083a67733c5
          
          
            - name: application-root-path
              mountPath:   /opt/hedera/services/config-mount/data/keys/public.pfx
              subPath:  29d12e603d75d76c087739c53873fb50028166a8e846d40dc027719f7a809977
          
          
            - name: application-root-path
              mountPath:   /opt/hedera/services/config-mount/data/onboard/GenesisPrivKey.txt
              subPath:  8eb93144b5258363c7257da297fa50f0c9c52cef301274d61f1eff3dcc2dc0e8
          
          
            - name: application-root-path
              mountPath:   /opt/hedera/services/config-mount/data/onboard/GenesisPubKey.txt
              subPath:  3dce3e857c64de1041e1381126d72e253358bc068548babf913422a635d5f741
          
          
            - name: application-root-path
              mountPath:   /opt/hedera/services/config-mount/data/onboard/StartUpAccount.txt
              subPath:  80d384465a899703c90c247667ba88cf4faaa792eb0e019b1550d1c63a1de1da
          
          
            - name: application-root-path
              mountPath:   /opt/hedera/services/config-mount/data/onboard/addressBook.txt
              subPath:  a31d7b72ab811ab8682b550efdc57504a8aa413f85d9e278b56a9df3fb6f67b8
          
          
            - name: application-root-path
              mountPath:   /opt/hedera/services/config-mount/data/onboard/devGenesisKeypair.pem
              subPath:  9f408ac0f831a62adad92ae9fe1790de78fc9108eedc7a5f39d83a45c5f491f8
          
          
            - name: application-root-path
              mountPath:   /opt/hedera/services/config-mount/hedera.crt
              subPath:  51cdc88d75ce7fe2a721cf1dd701216852c6530bf2baa0e3072ebd261696fcee
          
          
            - name: application-root-path
              mountPath:   /opt/hedera/services/config-mount/hedera.key
              subPath:  db77ec4dce1ccb420185a2458b815dfeedef26e67a652ac9229d35b7693e0d37
          
          
            - name: application-root-path
              mountPath:   /opt/hedera/services/config-mount/log4j2.xml
              subPath:  2250858faf37c92e1f4533f266195ed6431d45c28071ec802b1bf7444db9e3a2
          
          
            - name: application-root-path
              mountPath:   /opt/hedera/services/config-mount/settings.txt
              subPath:  5944af072cc85da3a45532b4b00ab484595f8a042de5798427db26ade98a1338
          
          ports:
            - containerPort: 50211
            - containerPort: 50212


          livenessProbe:
             exec:
               command: ["grep", "ServicesMain - Now current platform status = ACTIVE", "/opt/hedera/services/output/hgcaa.log"]
             initialDelaySeconds: 60
          #     port: endpoint
          # # readinessProbe:
          #   httpGet:
          #     path: /
          #     port: endpoint
          resources:
            {}



        - name: record-streams-uploader
        
          args:
            - /usr/bin/env
            - python3.7
            - /usr/local/bin/mirror.py
            - --linux
            - --watch-directory
            - /records
            - --s3-endpoint
            - http://minio:9000
            - --debug
          env:
            - name: BUCKET_NAME
              value: hedera-streams
            - name: BUCKET_PATH
              value: recordstreams/record0.0.3
            - name: DEBUG
              value: "true"
            - name: GCS_ENABLE
              value: "false"
            - name: REAPER_ENABLE
              value: "true"
            - name: REAPER_INTERVAL
              value: "5"
            - name: REAPER_MIN_KEEP
              value: "1"
            - name: S3_ACCESS_KEY
              value: minioadmin
            - name: S3_ENABLE
              value: "true"
            - name: S3_SECRET_KEY
              value: minioadmin
            - name: SIG_EXTENSION
              value: rcd_sig
            - name: SIG_PRIORITIZE
              value: "true"
            - name: SIG_REQUIRE
              value: "true"
            - name: STREAM_EXTENSION
              value: rcd.gz
          image: gcr.io/hedera-registry/uploader-mirror:0.7.0
          resources: {}
          volumeMounts:
            - name: network-node-logs-root-path
              mountPath: /records
              subPath: recordStreams/record0.0.3
          
        - name: account-balances-uploader
          args:
            - /usr/bin/env
            - python3.7
            - /usr/local/bin/mirror.py
            - --linux
            - --watch-directory
            - /balances
            - --s3-endpoint
            - http://minio:9000
            - --debug
          env:
            - name: BUCKET_NAME
              value: hedera-streams
            - name: BUCKET_PATH
              value: accountBalances/balance0.0.3
            - name: DEBUG
              value: "true"
            - name: GCS_ENABLE
              value: "false"
            - name: REAPER_ENABLE
              value: "true"
            - name: REAPER_INTERVAL
              value: "5"
            - name: REAPER_MIN_KEEP
              value: "1"
            - name: S3_ACCESS_KEY
              value: minioadmin
            - name: S3_ENABLE
              value: "true"
            - name: S3_SECRET_KEY
              value: minioadmin
            - name: SIG_EXTENSION
              value: pb_sig
            - name: SIG_PRIORITIZE
              value: "true"
            - name: SIG_REQUIRE
              value: "true"
            - name: STREAM_EXTENSION
              value: pb
          image: gcr.io/hedera-registry/uploader-mirror:0.7.0
          resources: {}
          volumeMounts:
            - name: network-node-logs-root-path
              mountPath: /balances
              subPath: accountBalances/balance0.0.3
              

        - name: record-sidecar-uploader
          args:
            - /usr/bin/env
            - python3.7
            - /usr/local/bin/mirror.py
            - --linux
            - --watch-directory
            - /sidecar-files
            - --s3-endpoint
            - http://minio:9000
            - --debug
          env:
            - name: BUCKET_NAME
              value: hedera-streams
            - name: BUCKET_PATH
              value: recordstreams/record0.0.3/sidecar
            - name: DEBUG
              value: "true"
            - name: GCS_ENABLE
              value: "false"
            - name: REAPER_ENABLE
              value: "true"
            - name: REAPER_INTERVAL
              value: "5"
            - name: REAPER_MIN_KEEP
              value: "1"
            - name: S3_ACCESS_KEY
              value: minioadmin
            - name: S3_ENABLE
              value: "true"
            - name: S3_SECRET_KEY
              value: minioadmin
            - name: SIG_PRIORITIZE
              value: "false"
            - name: SIG_REQUIRE
              value: "false"
            - name: STREAM_EXTENSION
              value: rcd.gz
          image: gcr.io/hedera-registry/uploader-mirror:0.7.0

          resources: {}
          volumeMounts:
            - name: network-node-logs-root-path
              mountPath: /sidecar-files
              subPath: recordStreams/record0.0.3/sidecar
      volumes:
        - name: application-config-path
          configMap:
            name: application-config
        - name: application-root-path
          configMap:
            name: application-root
        - name: data-empty
          emptyDir: {}
        - name: network-node-logs-root-path
          persistentVolumeClaim:
            claimName: node-logs
---
# Source: hedera-mirror-node/charts/network-node/templates/minio-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    kompose.cmd: kompose -v convert
    kompose.version: 1.26.1 (a9d05d509)
  creationTimestamp: null
  labels:
    io.kompose.service: minio
  name: minio
spec:
  replicas: 1
  selector:
    matchLabels:
      io.kompose.service: minio
  strategy:
    type: Recreate
  template:
    metadata:
      annotations:
        kompose.cmd: kompose -v convert
        kompose.version: 1.26.1 (a9d05d509)
      creationTimestamp: null
      labels:
        io.kompose.network/test-cloud-storage: "true"
        io.kompose.service: minio
    spec:
      containers:
        - args:
            - -c
            - mkdir -p /data/hedera-streams && minio server /data --console-address ":9001"
          command:
            - sh
          env:
            - name: MINIO_ACCESS_KEY
              value: minioadmin
            - name: MINIO_ROOT_PASSWORD
              value: minioadmin
            - name: MINIO_ROOT_USER
              value: minioadmin
            - name: MINIO_SECRET_KEY
              value: minioadmin
          image: minio/minio
          name: minio
          ports:
            - containerPort: 9000
            - containerPort: 9001
          resources: {}
          volumeMounts:
            - mountPath: /data
              name: minio-data
      restartPolicy: Always
      volumes:
        - name: minio-data
          persistentVolumeClaim:
            claimName: minio-data
status: {}
---
# Source: hedera-mirror-node/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mirror-1-hedera-mirror-node
  labels:
    helm.sh/chart: hedera-mirror-node-0.1.0
    app.kubernetes.io/name: hedera-mirror-node
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: hedera-mirror-node
      app.kubernetes.io/instance: mirror-1
  template:
    metadata:
      labels:
        app.kubernetes.io/name: hedera-mirror-node
        app.kubernetes.io/instance: mirror-1
    spec:
      serviceAccountName: mirror-1-hedera-mirror-node
      securityContext:
        {}
      containers:
        - name: hedera-mirror-node
          securityContext:
            {}
          image: "nginx:1.16.0"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /
              port: http
          readinessProbe:
            httpGet:
              path: /
              port: http
          resources:
            {}
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/rest/templates/hpa.yaml
apiVersion: autoscaling/v2beta2
kind: HorizontalPodAutoscaler
metadata:
  labels: 
    app.kubernetes.io/component: rest
    app.kubernetes.io/name: rest
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: hedera-mirror-node
    app.kubernetes.io/version: "0.68.0"
    helm.sh/chart: rest-0.68.0
  name: mirror-node-rest
  namespace: default
spec:
  behavior: 
    {}
  maxReplicas: 15
  metrics: 
    - resource:
        name: cpu
        target:
          averageUtilization: 80
          type: Utilization
      type: Resource
  minReplicas: 1
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: mirror-node-rest
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/rosetta/templates/hpa.yaml
apiVersion: autoscaling/v2beta2
kind: HorizontalPodAutoscaler
metadata:
  labels: 
    app.kubernetes.io/component: rosetta
    app.kubernetes.io/name: rosetta
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: hedera-mirror-node
    app.kubernetes.io/version: "0.68.0"
    helm.sh/chart: rosetta-0.68.0
  name: mirror-1-rosetta
  namespace: default
spec:
  behavior: 
    {}
  maxReplicas: 10
  metrics: 
    - resource:
        name: cpu
        target:
          averageUtilization: 80
          type: Utilization
      type: Resource
  minReplicas: 1
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: mirror-1-rosetta
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/postgresql/templates/postgresql/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mirror-1-postgres-postgresql
  namespace: "default"
  labels:
    app.kubernetes.io/name: postgres
    helm.sh/chart: postgresql-9.4.11
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: postgresql
spec:
  replicas: 1
  podManagementPolicy: Parallel
  serviceName: mirror-1-postgres-postgresql-headless
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: postgres
      app.kubernetes.io/instance: mirror-1
      app.kubernetes.io/component: postgresql
  template:
    metadata:
      labels:
        app.kubernetes.io/name: postgres
        helm.sh/chart: postgresql-9.4.11
        app.kubernetes.io/instance: mirror-1
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: postgresql
    spec:
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: postgres
                    app.kubernetes.io/instance: mirror-1
                    app.kubernetes.io/component: postgresql
                namespaces:
                  - "default"
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      serviceAccountName: mirror-1-postgres
      hostNetwork: false
      hostIPC: false
      initContainers:
      containers:
        - name: postgresql
          image: docker.io/bitnami/postgresql-repmgr:14.5.0-debian-11-r31
          imagePullPolicy: "IfNotPresent"
          securityContext:
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            runAsUser: 1001
          lifecycle:
            preStop:
              exec:
                command:
                  - /pre-stop.sh
          # Auxiliary vars to populate environment variables
          env:
            - name: BITNAMI_DEBUG
              value: "true"
            # PostgreSQL configuration
            - name: POSTGRESQL_VOLUME_DIR
              value: "/bitnami/postgresql"
            - name: PGDATA
              value: "/bitnami/postgresql/data"
            - name: POSTGRES_USER
              value: "postgres"
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: mirror-passwords
                  key: postgresql-password
            - name: POSTGRES_DB
              value: "postgres"
            - name: POSTGRESQL_LOG_HOSTNAME
              value: "true"
            - name: POSTGRESQL_LOG_CONNECTIONS
              value: "false"
            - name: POSTGRESQL_LOG_DISCONNECTIONS
              value: "false"
            - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
              value: "off"
            - name: POSTGRESQL_CLIENT_MIN_MESSAGES
              value: "error"
            - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
              value: "pgaudit, repmgr"
            - name: POSTGRESQL_ENABLE_TLS
              value: "no"
            - name: POSTGRESQL_PORT_NUMBER
              value: "5432"
            # Repmgr configuration
            - name: REPMGR_PORT_NUMBER
              value: "5432"
            - name: REPMGR_PRIMARY_PORT
              value: "5432"
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: REPMGR_UPGRADE_EXTENSION
              value: "no"
            - name: REPMGR_PGHBA_TRUST_ALL
              value: "no"
            - name: REPMGR_MOUNTED_CONF_DIR
              value: "/bitnami/repmgr/conf"
            - name: REPMGR_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: REPMGR_PARTNER_NODES
              value: mirror-1-postgres-postgresql-0.mirror-1-postgres-postgresql-headless.$(REPMGR_NAMESPACE).svc.cluster.local,
            - name: REPMGR_PRIMARY_HOST
              value: "mirror-1-postgres-postgresql-0.mirror-1-postgres-postgresql-headless.$(REPMGR_NAMESPACE).svc.cluster.local"
            - name: REPMGR_NODE_NAME
              value: "$(MY_POD_NAME)"
            - name: REPMGR_NODE_NETWORK_NAME
              value: "$(MY_POD_NAME).mirror-1-postgres-postgresql-headless.$(REPMGR_NAMESPACE).svc.cluster.local"
            - name: REPMGR_LOG_LEVEL
              value: "DEBUG"
            - name: REPMGR_CONNECT_TIMEOUT
              value: "5"
            - name: REPMGR_RECONNECT_ATTEMPTS
              value: "2"
            - name: REPMGR_RECONNECT_INTERVAL
              value: "3"
            - name: REPMGR_USERNAME
              value: "repmgr"
            - name: REPMGR_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: mirror-passwords
                  key: repmgr-password
            - name: REPMGR_DATABASE
              value: "repmgr"
            - name: REPMGR_FENCE_OLD_PRIMARY
              value: "no"
            - name: REPMGR_CHILD_NODES_CHECK_INTERVAL
              value: "5"
            - name: REPMGR_CHILD_NODES_CONNECTED_MIN_COUNT
              value: "1"
            - name: REPMGR_CHILD_NODES_DISCONNECT_TIMEOUT
              value: "30"
          envFrom:
            - secretRef:
                name: mirror-passwords
          ports:
            - name: postgresql
              containerPort: 5432
              protocol: TCP
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - bash
                - -ec
                - 'PGPASSWORD=$POSTGRES_PASSWORD psql -w -U "postgres" -d "postgres" -h 127.0.0.1 -p 5432 -c "SELECT 1"'
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - bash
                - -ec
                - 'PGPASSWORD=$POSTGRES_PASSWORD psql -w -U "postgres" -d "postgres" -h 127.0.0.1 -p 5432 -c "SELECT 1"'
          resources:
            limits:
              cpu: 1500m
              memory: 1000Mi
            requests:
              cpu: 250m
              memory: 500Mi
          volumeMounts:
            - name: custom-init-scripts-secret
              mountPath: /docker-entrypoint-initdb.d/secret
            - name: data
              mountPath: /bitnami/postgresql
            - name: hooks-scripts
              mountPath: /pre-stop.sh
              subPath: pre-stop.sh
      volumes:
        - name: hooks-scripts
          configMap:
            name: mirror-1-postgres-postgresql-hooks-scripts
            defaultMode: 0755
        - name: custom-init-scripts-secret
          secret:
            secretName: mirror-1-init
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "500Gi"
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/redis/templates/sentinel/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mirror-1-redis-node
  namespace: "default"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-17.3.7
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: node
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: redis
      app.kubernetes.io/instance: mirror-1
      app.kubernetes.io/component: node
  serviceName: mirror-1-redis-headless
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: redis
        helm.sh/chart: redis-17.3.7
        app.kubernetes.io/instance: mirror-1
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: node
      annotations:
        checksum/configmap: 1ddc45aae70356f46ffc0793129298a9b6f292a37e29af4f68b513431350a5d5
        checksum/health: ec84e2d65669b6ad8c39afac04e693193f5c03ed50c1d462465081ad978d8fe3
        checksum/scripts: 3766dd8983834289864d3d30e58e68b10451b799068ae06eeec23defdb55fd70
        checksum/secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
    spec:
      
      securityContext:
        fsGroup: 1001
        runAsGroup: 1001
        runAsUser: 1001
      serviceAccountName: mirror-1-redis
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: redis
                    app.kubernetes.io/instance: mirror-1
                    app.kubernetes.io/component: node
                namespaces:
                  - "default"
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      terminationGracePeriodSeconds: 30
      containers:
        - name: redis
          image: docker.io/bitnami/redis:7.0.5-debian-11-r7
          imagePullPolicy: "IfNotPresent"
          lifecycle:
            preStop:
              exec:
                command:
                  - /bin/bash
                  - -c
                  - /opt/bitnami/scripts/start-scripts/prestop-redis.sh
          securityContext:
            runAsUser: 1001
          command:
            - /bin/bash
          args:
            - -c
            - /opt/bitnami/scripts/start-scripts/start-node.sh
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: REDIS_MASTER_PORT_NUMBER
              value: "6379"
            - name: ALLOW_EMPTY_PASSWORD
              value: "no"
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: mirror-redis
                  key: SPRING_REDIS_PASSWORD
            - name: REDIS_MASTER_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: mirror-redis
                  key: SPRING_REDIS_PASSWORD
            - name: REDIS_TLS_ENABLED
              value: "no"
            - name: REDIS_PORT
              value: "6379"
            - name: REDIS_DATA_DIR
              value: /data
          ports:
            - name: redis
              containerPort: 6379
          startupProbe:
            failureThreshold: 22
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_liveness_local.sh 5
          livenessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_liveness_local.sh 5
          readinessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            timeoutSeconds: 1
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_readiness_local.sh 1
          resources:
            limits:
              cpu: 1500m
              memory: 1000Mi
            requests:
              cpu: 250m
              memory: 500Mi
          volumeMounts:
            - name: start-scripts
              mountPath: /opt/bitnami/scripts/start-scripts
            - name: health
              mountPath: /health
            - name: sentinel-data
              mountPath: /opt/bitnami/redis-sentinel/etc
            - name: redis-data
              mountPath: /data
              subPath: 
            - name: config
              mountPath: /opt/bitnami/redis/mounted-etc
            - name: redis-tmp-conf
              mountPath: /opt/bitnami/redis/etc
            - name: tmp
              mountPath: /tmp
        - name: sentinel
          image: docker.io/bitnami/redis-sentinel:7.0.5-debian-11-r6
          imagePullPolicy: "IfNotPresent"
          lifecycle:
            preStop:
              exec:
                command:
                  - /bin/bash
                  - -c
                  - /opt/bitnami/scripts/start-scripts/prestop-sentinel.sh
          securityContext:
            runAsUser: 1001
          command:
            - /bin/bash
          args:
            - -c
            - /opt/bitnami/scripts/start-scripts/start-sentinel.sh
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: mirror-redis
                  key: SPRING_REDIS_PASSWORD
            - name: REDIS_SENTINEL_TLS_ENABLED
              value: "no"
            - name: REDIS_SENTINEL_PORT
              value: "26379"
          ports:
            - name: redis-sentinel
              containerPort: 26379
          startupProbe:
            failureThreshold: 22
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_sentinel.sh 5
          livenessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_sentinel.sh 5
          readinessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            timeoutSeconds: 1
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_sentinel.sh 1
          resources:
            limits:
              cpu: 150m
              memory: 256Mi
            requests:
              cpu: 75m
              memory: 75Mi
          volumeMounts:
            - name: start-scripts
              mountPath: /opt/bitnami/scripts/start-scripts
            - name: health
              mountPath: /health
            - name: sentinel-data
              mountPath: /opt/bitnami/redis-sentinel/etc
            - name: redis-data
              mountPath: /data
              subPath: 
            - name: config
              mountPath: /opt/bitnami/redis-sentinel/mounted-etc
      volumes:
        - name: start-scripts
          configMap:
            name: mirror-1-redis-scripts
            defaultMode: 0755
        - name: health
          configMap:
            name: mirror-1-redis-health
            defaultMode: 0755
        - name: config
          configMap:
            name: mirror-1-redis-configuration
        - name: redis-tmp-conf
          emptyDir: {}
        - name: tmp
          emptyDir: {}
  volumeClaimTemplates:
    - metadata:
        name: redis-data
        labels:
          app.kubernetes.io/name: redis
          app.kubernetes.io/instance: mirror-1
          app.kubernetes.io/component: node
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
        
    - metadata:
        name: sentinel-data
        labels:
          app.kubernetes.io/name: redis
          app.kubernetes.io/instance: mirror-1
          app.kubernetes.io/component: node
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "100Mi"
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/grpc/templates/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    traefik.ingress.kubernetes.io/router.middlewares: "default-mirror-1-grpc@kubernetescrd"
  labels: 
    app.kubernetes.io/component: grpc
    app.kubernetes.io/name: grpc
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: hedera-mirror-node
    app.kubernetes.io/version: "0.68.0"
    helm.sh/chart: grpc-0.68.0
  name: mirror-1-grpc
  namespace: default
spec:
  rules:
    - host: ""
      http:
        paths:
          - path: /com.hedera.mirror.api.proto.ConsensusService
            pathType: ImplementationSpecific
            backend:
              service:
                name: mirror-1-grpc
                port:
                  number: 5600
          - path: /com.hedera.mirror.api.proto.NetworkService
            pathType: ImplementationSpecific
            backend:
              service:
                name: mirror-1-grpc
                port:
                  number: 5600
          - path: /grpc.reflection.v1alpha.ServerReflection
            pathType: ImplementationSpecific
            backend:
              service:
                name: mirror-1-grpc
                port:
                  number: 5600
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/monitor/templates/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  labels: 
    app.kubernetes.io/component: monitor
    app.kubernetes.io/name: monitor
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: hedera-mirror-node
    app.kubernetes.io/version: "0.68.0"
    helm.sh/chart: monitor-0.68.0
  name: mirror-1-monitor
  namespace: default
spec:
  rules:
    - host: ""
      http:
        paths:
          - path: /actuator/health/cluster
            pathType: ImplementationSpecific
            backend:
              service:
                name: mirror-1-monitor
                port:
                  number: 80
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/rest/templates/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    traefik.ingress.kubernetes.io/router.middlewares: "default-mirror-node-rest@kubernetescrd"
  labels: 
    app.kubernetes.io/component: rest
    app.kubernetes.io/name: rest
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: hedera-mirror-node
    app.kubernetes.io/version: "0.68.0"
    helm.sh/chart: rest-0.68.0
  name: mirror-node-rest
  namespace: default
spec:
  rules:
    - host: ""
      http:
        paths:
          - path: /api/v1
            pathType: ImplementationSpecific
            backend:
              service:
                name: mirror-node-rest
                port:
                  number: 80
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/rosetta/templates/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    traefik.ingress.kubernetes.io/router.middlewares: "default-mirror-1-rosetta@kubernetescrd"
  labels: 
    app.kubernetes.io/component: rosetta
    app.kubernetes.io/name: rosetta
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: hedera-mirror-node
    app.kubernetes.io/version: "0.68.0"
    helm.sh/chart: rosetta-0.68.0
  name: mirror-1-rosetta
  namespace: default
spec:
  rules:
    - host: ""
      http:
        paths:
          - path: /rosetta/account
            pathType: ImplementationSpecific
            backend:
              service:
                name: mirror-1-rosetta
                port:
                  number: 80
          - path: /rosetta/block
            pathType: ImplementationSpecific
            backend:
              service:
                name: mirror-1-rosetta
                port:
                  number: 80
          - path: /rosetta/call
            pathType: ImplementationSpecific
            backend:
              service:
                name: mirror-1-rosetta
                port:
                  number: 80
          - path: /rosetta/construction
            pathType: ImplementationSpecific
            backend:
              service:
                name: mirror-1-rosetta
                port:
                  number: 80
          - path: /rosetta/events
            pathType: ImplementationSpecific
            backend:
              service:
                name: mirror-1-rosetta
                port:
                  number: 80
          - path: /rosetta/mempool
            pathType: ImplementationSpecific
            backend:
              service:
                name: mirror-1-rosetta
                port:
                  number: 80
          - path: /rosetta/network
            pathType: ImplementationSpecific
            backend:
              service:
                name: mirror-1-rosetta
                port:
                  number: 80
          - path: /rosetta/search
            pathType: ImplementationSpecific
            backend:
              service:
                name: mirror-1-rosetta
                port:
                  number: 80
---
# Source: hedera-mirror-node/charts/hedera-mirror/charts/web3/templates/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    traefik.ingress.kubernetes.io/router.middlewares: "default-mirror-1-web3@kubernetescrd"
  labels: 
    app.kubernetes.io/component: web3
    app.kubernetes.io/name: web3
    app.kubernetes.io/instance: mirror-1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: hedera-mirror-node
    app.kubernetes.io/version: "0.68.0"
    helm.sh/chart: web3-0.68.0
  name: mirror-1-web3
  namespace: default
spec:
  rules:
    - host: ""
      http:
        paths:
          - path: /web3
            pathType: ImplementationSpecific
            backend:
              service:
                name: mirror-1-web3
                port:
                  number: 80

NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace default -l "app.kubernetes.io/name=hedera-mirror-node,app.kubernetes.io/instance=mirror-1" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT
